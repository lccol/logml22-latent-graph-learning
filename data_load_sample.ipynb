{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LOGML_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fde7c0872e7a4881ba3941a84c7f5171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80a7b6a0f76040ffa4ca3e36df5f2090",
              "IPY_MODEL_2b7fd18b903941a19c5fb2bb2880149a",
              "IPY_MODEL_2e4d36c0b4454f40b052de02bb25ca85"
            ],
            "layout": "IPY_MODEL_3d93167d4190427d8f70ecb8486a4454"
          }
        },
        "80a7b6a0f76040ffa4ca3e36df5f2090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40c404090cd3408698e6425f8e98f0dd",
            "placeholder": "​",
            "style": "IPY_MODEL_26ce064b9fa14579adcf1f8426d62419",
            "value": "100%"
          }
        },
        "2b7fd18b903941a19c5fb2bb2880149a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5896f3f2adb4a469abbaf915ae1b496",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e98daeba5d6459b94e37771eb509295",
            "value": 3
          }
        },
        "2e4d36c0b4454f40b052de02bb25ca85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36bc58c1df9f4a43b6ec6200aaefb0a3",
            "placeholder": "​",
            "style": "IPY_MODEL_770df2eaf36b4489ae30912310d247ec",
            "value": " 3/3 [00:00&lt;00:00, 27.09it/s]"
          }
        },
        "3d93167d4190427d8f70ecb8486a4454": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40c404090cd3408698e6425f8e98f0dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26ce064b9fa14579adcf1f8426d62419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5896f3f2adb4a469abbaf915ae1b496": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e98daeba5d6459b94e37771eb509295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36bc58c1df9f4a43b6ec6200aaefb0a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "770df2eaf36b4489ae30912310d247ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy.ndimage import zoom\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "try:\n",
        "    import pickle5 as pickle\n",
        "except ImportError as e:\n",
        "    import pickle\n",
        "from skimage import transform\n",
        "import multiprocessing as mp\n"
      ],
      "metadata": {
        "id": "FJLcsyEJ9Xf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "channel_stoi_default = {\"i\": 0, \"ii\": 1, \"v1\":2, \"v2\":3, \"v3\":4, \"v4\":5, \"v5\":6, \"v6\":7, \"iii\":8, \"avr\":9, \"avl\":10, \"avf\":11, \"vx\":12, \"vy\":13, \"vz\":14}\n"
      ],
      "metadata": {
        "id": "S56ayoMC9V3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stratify(data, classes, ratios, samples_per_group=None):\n",
        "    \"\"\"Stratifying procedure. Modified from https://vict0rs.ch/2018/05/24/sample-multilabel-dataset/ (based on Sechidis 2011)\n",
        "    data is a list of lists: a list of labels, for each sample (possibly containing duplicates not multi-hot encoded).\n",
        "    \n",
        "    classes is the list of classes each label can take\n",
        "    ratios is a list, summing to 1, of how the dataset should be split\n",
        "    samples_per_group: list with number of samples per patient/group\n",
        "    \"\"\"\n",
        "    np.random.seed(0) # fix the random seed\n",
        "\n",
        "    # data is now always a list of lists; len(data) is the number of patients; data[i] is the list of all labels for patient i (possibly multiple identical entries)\n",
        "\n",
        "    if(samples_per_group is None):\n",
        "        samples_per_group = np.ones(len(data))\n",
        "        \n",
        "    #size is the number of ecgs\n",
        "    size = np.sum(samples_per_group)\n",
        "\n",
        "    # Organize data per label: for each label l, per_label_data[l] contains the list of patients\n",
        "    # in data which have this label (potentially multiple identical entries)\n",
        "    per_label_data = {c: [] for c in classes}\n",
        "    for i, d in enumerate(data):\n",
        "        for l in d:\n",
        "            per_label_data[l].append(i)\n",
        "\n",
        "    # In order not to compute lengths each time, they are tracked here.\n",
        "    subset_sizes = [r * size for r in ratios] #list of subset_sizes in terms of ecgs\n",
        "    per_label_subset_sizes = { c: [r * len(per_label_data[c]) for r in ratios] for c in classes } #dictionary with label: list of subset sizes in terms of patients\n",
        "\n",
        "    # For each subset we want, the set of sample-ids which should end up in it\n",
        "    stratified_data_ids = [set() for _ in range(len(ratios))] #initialize empty\n",
        "\n",
        "    # For each sample in the data set\n",
        "    print(\"Starting fold distribution...\")\n",
        "    size_prev=size+1 #just for output\n",
        "    while size > 0:\n",
        "        if(int(size_prev/1000) > int(size/1000)):\n",
        "            print(\"Remaining entries to distribute:\",size,\"non-empty labels:\", np.sum([1 for l, label_data in per_label_data.items() if len(label_data)>0]))\n",
        "        size_prev=size\n",
        "        # Compute |Di| \n",
        "        lengths = {\n",
        "            l: len(label_data)\n",
        "            for l, label_data in per_label_data.items()\n",
        "        } #dictionary label: number of ecgs with this label that have not been assigned to a fold yet\n",
        "        try:\n",
        "            # Find label of smallest |Di|\n",
        "            label = min({k: v for k, v in lengths.items() if v > 0}, key=lengths.get)\n",
        "        except ValueError:\n",
        "            # If the dictionary in `min` is empty we get a Value Error. \n",
        "            # This can happen if there are unlabeled samples.\n",
        "            # In this case, `size` would be > 0 but only samples without label would remain.\n",
        "            # \"No label\" could be a class in itself: it's up to you to format your data accordingly.\n",
        "            break\n",
        "        # For each patient with label `label` get patient and corresponding counts\n",
        "        unique_samples, unique_counts = np.unique(per_label_data[label],return_counts=True)\n",
        "        idxs_sorted = np.argsort(unique_counts, kind='stable')[::-1]\n",
        "        unique_samples = unique_samples[idxs_sorted] # this is a list of all patient ids with this label sort by size descending\n",
        "        unique_counts =  unique_counts[idxs_sorted] # these are the corresponding counts\n",
        "        \n",
        "        # loop through all patient ids with this label\n",
        "        for current_id, current_count in zip(unique_samples,unique_counts):\n",
        "            \n",
        "            subset_sizes_for_label = per_label_subset_sizes[label] #current subset sizes for the chosen label\n",
        "\n",
        "            # Find argmax clj i.e. subset in greatest need of the current label\n",
        "            largest_subsets = np.argwhere(subset_sizes_for_label == np.amax(subset_sizes_for_label)).flatten()\n",
        "            \n",
        "            # if there is a single best choice: assign it\n",
        "            if len(largest_subsets) == 1:\n",
        "                subset = largest_subsets[0]\n",
        "            # If there is more than one such subset, find the one in greatest need of any label\n",
        "            else:\n",
        "                largest_subsets2 = np.argwhere(np.array(subset_sizes)[largest_subsets] == np.amax(np.array(subset_sizes)[largest_subsets])).flatten()\n",
        "                subset = largest_subsets[np.random.choice(largest_subsets2)]\n",
        "\n",
        "            # Store the sample's id in the selected subset\n",
        "            stratified_data_ids[subset].add(current_id)\n",
        "\n",
        "            # There is current_count fewer samples to distribute\n",
        "            size -= samples_per_group[current_id]\n",
        "            # The selected subset needs current_count fewer samples\n",
        "            subset_sizes[subset] -= samples_per_group[current_id]\n",
        "\n",
        "            # In the selected subset, there is one more example for each label\n",
        "            # the current sample has\n",
        "            for l in data[current_id]:\n",
        "                per_label_subset_sizes[l][subset] -= 1\n",
        "               \n",
        "            # Remove the sample from the dataset, meaning from all per_label dataset created\n",
        "            for x in per_label_data.keys():\n",
        "                per_label_data[x] = [y for y in per_label_data[x] if y!=current_id]\n",
        "              \n",
        "    # Create the stratified dataset as a list of subsets, each containing the orginal labels\n",
        "    stratified_data_ids = [sorted(strat) for strat in stratified_data_ids]\n",
        "    #stratified_data = [\n",
        "    #    [data[i] for i in strat] for strat in stratified_data_ids\n",
        "    #]\n",
        "\n",
        "    # Return both the stratified indexes, to be used to sample the `features` associated with your labels\n",
        "    # And the stratified labels dataset\n",
        "\n",
        "    #return stratified_data_ids, stratified_data\n",
        "    return stratified_data_ids"
      ],
      "metadata": {
        "id": "Hj4s-y3E-bDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_data(sigbufs, channel_labels, fs, target_fs, channels=8, channel_stoi=None,skimage_transform=True,interpolation_order=3):\n",
        "    channel_labels = [c.lower() for c in channel_labels]\n",
        "    #https://github.com/scipy/scipy/issues/7324 zoom issues\n",
        "    factor = target_fs/fs\n",
        "    timesteps_new = int(len(sigbufs)*factor)\n",
        "    if(channel_stoi is not None):\n",
        "        data = np.zeros((timesteps_new, channels), dtype=np.float32)\n",
        "        for i,cl in enumerate(channel_labels):\n",
        "            if(cl in channel_stoi.keys() and channel_stoi[cl]<channels):\n",
        "                if(skimage_transform):\n",
        "                    data[:,channel_stoi[cl]]=transform.resize(sigbufs[:,i],(timesteps_new,),order=interpolation_order).astype(np.float32)\n",
        "                else:\n",
        "                    data[:,channel_stoi[cl]]=zoom(sigbufs[:,i],timesteps_new/len(sigbufs),order=interpolation_order).astype(np.float32)\n",
        "    else:\n",
        "        if(skimage_transform):\n",
        "            data=transform.resize(sigbufs,(timesteps_new,channels),order=interpolation_order).astype(np.float32)\n",
        "        else:\n",
        "            data=zoom(sigbufs,(timesteps_new/len(sigbufs),1),order=interpolation_order).astype(np.float32)\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "2S48PKyD-m8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(target_root,filename_postfix=\"\",df_mapped=True):\n",
        "    target_root = Path(target_root)\n",
        "    # if(df_mapped):\n",
        "    #     df = pd.read_pickle(target_root/(\"df_memmap\"+filename_postfix+\".pkl\"))\n",
        "    # else:\n",
        "    #     df = pd.read_pickle(target_root/(\"df\"+filename_postfix+\".pkl\")\n",
        "    \n",
        "    ### due to pickle 5 protocol error\n",
        "\n",
        "    if(df_mapped):\n",
        "        df = pickle.load(open(target_root/(\"df_memmap\"+filename_postfix+\".pkl\"), \"rb\"))\n",
        "    else:\n",
        "        df = pickle.load(open(target_root/(\"df\"+filename_postfix+\".pkl\"), \"rb\"))\n",
        "\n",
        "\n",
        "    if((target_root/(\"lbl_itos\"+filename_postfix+\".pkl\")).exists()):#dict as pickle\n",
        "        infile = open(target_root/(\"lbl_itos\"+filename_postfix+\".pkl\"), \"rb\")\n",
        "        lbl_itos=pickle.load(infile)\n",
        "        infile.close()\n",
        "    else:#array\n",
        "        lbl_itos = np.load(target_root/(\"lbl_itos\"+filename_postfix+\".npy\"))\n",
        "\n",
        "\n",
        "    mean = np.load(target_root/(\"mean\"+filename_postfix+\".npy\"))\n",
        "    std = np.load(target_root/(\"std\"+filename_postfix+\".npy\"))\n",
        "    return df, lbl_itos, mean, std\n",
        "\n",
        "def save_dataset(df,lbl_itos,mean,std,target_root,filename_postfix=\"\",protocol=4):\n",
        "    target_root = Path(target_root)\n",
        "    df.to_pickle(target_root/(\"df\"+filename_postfix+\".pkl\"), protocol=protocol)\n",
        "\n",
        "\n",
        "def dataset_get_stats(df, col=\"data\", simple=True):\n",
        "    '''creates (weighted) means and stds from mean, std and length cols of the df'''\n",
        "    if(simple):\n",
        "        return df[col+\"_mean\"].mean(), df[col+\"_std\"].mean()\n",
        "    else:\n",
        "        #https://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html\n",
        "        #or https://gist.github.com/thomasbrandon/ad5b1218fc573c10ea4e1f0c63658469\n",
        "        def combine_two_means_vars(x1,x2):\n",
        "            (mean1,var1,n1) = x1\n",
        "            (mean2,var2,n2) = x2\n",
        "            mean = mean1*n1/(n1+n2)+ mean2*n2/(n1+n2)\n",
        "            var = var1*n1/(n1+n2)+ var2*n2/(n1+n2)+n1*n2/(n1+n2)/(n1+n2)*np.power(mean1-mean2,2)\n",
        "            return (mean, var, (n1+n2))\n",
        "\n",
        "        def combine_all_means_vars(means,vars,lengths):\n",
        "            inputs = list(zip(means,vars,lengths))\n",
        "            result = inputs[0]\n",
        "\n",
        "            for inputs2 in inputs[1:]:\n",
        "                result= combine_two_means_vars(result,inputs2)\n",
        "            return result\n",
        "\n",
        "        means = list(df[col+\"_mean\"])\n",
        "        vars = np.power(list(df[col+\"_std\"]),2)\n",
        "        lengths = list(df[col+\"_length\"])\n",
        "        mean,var,length = combine_all_means_vars(means,vars,lengths)\n",
        "        return mean, np.sqrt(var)\n",
        "\n",
        "def dataset_add_std_col(df, col=\"data\", axis=(0), data_folder=None):\n",
        "    '''adds a column with mean'''\n",
        "    df[col+\"_std\"]=df[col].apply(lambda x: np.std(np.load(x if data_folder is None else data_folder/x, allow_pickle=True),axis=axis))\n",
        "\n",
        "\n",
        "def dataset_add_mean_col(df, col=\"data\", axis=(0), data_folder=None):\n",
        "    '''adds a column with mean'''\n",
        "    df[col+\"_mean\"]=df[col].apply(lambda x: np.mean(np.load(x if data_folder is None else data_folder/x, allow_pickle=True),axis=axis))\n",
        "\n",
        "def dataset_add_length_col(df, col=\"data\", data_folder=None):\n",
        "    '''add a length column to the dataset df'''\n",
        "    df[col+\"_length\"]=df[col].apply(lambda x: len(np.load(x if data_folder is None else data_folder/x, allow_pickle=True)))"
      ],
      "metadata": {
        "id": "GB0zNK7Q-cqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def npys_to_memmap(npys, target_filename, max_len=0, delete_npys=True):\n",
        "    memmap = None\n",
        "    start = []#start_idx in current memmap file\n",
        "    length = []#length of segment\n",
        "    filenames= []#memmap files\n",
        "    file_idx=[]#corresponding memmap file for sample\n",
        "    shape=[]\n",
        "\n",
        "    for idx,npy in tqdm(list(enumerate(npys))):\n",
        "        data = np.load(npy, allow_pickle=True)\n",
        "        if(memmap is None or (max_len>0 and start[-1]+length[-1]>max_len)):\n",
        "            if(max_len>0):\n",
        "                filenames.append(target_filename.parent/(target_filename.stem+\"_\"+str(len(filenames)+\".npy\")))\n",
        "            else:\n",
        "                filenames.append(target_filename)\n",
        "\n",
        "            if(memmap is not None):#an existing memmap exceeded max_len\n",
        "                shape.append([start[-1]+length[-1]]+[l for l in data.shape[1:]])\n",
        "                del memmap\n",
        "            #create new memmap\n",
        "            start.append(0)\n",
        "            length.append(data.shape[0])\n",
        "            memmap = np.memmap(filenames[-1], dtype=data.dtype, mode='w+', shape=data.shape)\n",
        "        else:\n",
        "            #append to existing memmap\n",
        "            start.append(start[-1]+length[-1])\n",
        "            length.append(data.shape[0])\n",
        "            memmap = np.memmap(filenames[-1], dtype=data.dtype, mode='r+', shape=tuple([start[-1]+length[-1]]+[l for l in data.shape[1:]]))\n",
        "\n",
        "        #store mapping memmap_id to memmap_file_id\n",
        "        file_idx.append(len(filenames)-1)\n",
        "        #insert the actual data\n",
        "        memmap[start[-1]:start[-1]+length[-1]]=data[:]\n",
        "        memmap.flush()\n",
        "        if(delete_npys is True):\n",
        "            npy.unlink()\n",
        "    del memmap\n",
        "\n",
        "    #append final shape if necessary\n",
        "    if(len(shape)<len(filenames)):\n",
        "        shape.append([start[-1]+length[-1]]+[l for l in data.shape[1:]])\n",
        "    #convert everything to relative paths\n",
        "    filenames= [f.name for f in filenames]\n",
        "    #save metadata\n",
        "    np.savez(target_filename.parent/(target_filename.stem+\"_meta.npz\"),start=start,length=length,shape=shape,file_idx=file_idx,dtype=data.dtype,filenames=filenames)"
      ],
      "metadata": {
        "id": "DniCk5RdIBsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def npys_to_memmap_batched(npys, target_filename, max_len=0, delete_npys=True, batch_length=900000):\n",
        "    memmap = None\n",
        "    start = np.array([0])#start_idx in current memmap file (always already the next start- delete last token in the end)\n",
        "    length = []#length of segment\n",
        "    filenames= []#memmap files\n",
        "    file_idx=[]#corresponding memmap file for sample\n",
        "    shape=[]#shapes of all memmap files\n",
        "\n",
        "    data = []\n",
        "    data_lengths=[]\n",
        "    dtype = None\n",
        "\n",
        "    for idx,npy in tqdm(list(enumerate(npys))):\n",
        "\n",
        "        data.append(np.load(npy, allow_pickle=True))\n",
        "        data_lengths.append(len(data[-1]))\n",
        "\n",
        "        if(idx==len(npys)-1 or np.sum(data_lengths)>batch_length):#flush\n",
        "            data = np.concatenate(data)\n",
        "            if(memmap is None or (max_len>0 and start[-1]>max_len)):#new memmap file has to be created\n",
        "                if(max_len>0):\n",
        "                    filenames.append(target_filename.parent/(target_filename.stem+\"_\"+str(len(filenames))+\".npy\"))\n",
        "                else:\n",
        "                    filenames.append(target_filename)\n",
        "\n",
        "                shape.append([np.sum(data_lengths)]+[l for l in data.shape[1:]])#insert present shape\n",
        "\n",
        "                if(memmap is not None):#an existing memmap exceeded max_len\n",
        "                    del memmap\n",
        "                #create new memmap\n",
        "                start[-1] = 0\n",
        "                start = np.concatenate([start,np.cumsum(data_lengths)])\n",
        "                length = np.concatenate([length,data_lengths])\n",
        "\n",
        "                memmap = np.memmap(filenames[-1], dtype=data.dtype, mode='w+', shape=data.shape)\n",
        "            else:\n",
        "                #append to existing memmap\n",
        "                start = np.concatenate([start,start[-1]+np.cumsum(data_lengths)])\n",
        "                length = np.concatenate([length,data_lengths])\n",
        "                shape[-1] = [start[-1]]+[l for l in data.shape[1:]]\n",
        "                memmap = np.memmap(filenames[-1], dtype=data.dtype, mode='r+', shape=tuple(shape[-1]))\n",
        "\n",
        "            #store mapping memmap_id to memmap_file_id\n",
        "            file_idx=np.concatenate([file_idx,[(len(filenames)-1)]*len(data_lengths)])\n",
        "            #insert the actual data\n",
        "            memmap[start[-len(data_lengths)-1]:start[-len(data_lengths)-1]+len(data)]=data[:]\n",
        "            memmap.flush()\n",
        "            dtype = data.dtype\n",
        "            data = []#reset data storage\n",
        "            data_lengths = []\n",
        "\n",
        "    start= start[:-1]#remove the last element\n",
        "    #cleanup\n",
        "    for npy in npys:\n",
        "        if(delete_npys is True):\n",
        "            npy.unlink()\n",
        "    del memmap\n",
        "\n",
        "    #convert everything to relative paths\n",
        "    filenames= [f.name for f in filenames]\n",
        "    #save metadata\n",
        "    np.savez(target_filename.parent/(target_filename.stem+\"_meta.npz\"),start=start,length=length,shape=shape,file_idx=file_idx,dtype=dtype,filenames=filenames)\n",
        "    \n",
        "def reformat_as_memmap(df, target_filename, data_folder=None, annotation=False, max_len=0, delete_npys=True,col_data=\"data\",col_label=\"label\", batch_length=0):\n",
        "    npys_data = []\n",
        "    npys_label = []\n",
        "\n",
        "    for id,row in df.iterrows():\n",
        "        npys_data.append(data_folder/row[col_data] if data_folder is not None else row[col_data])\n",
        "        if(annotation):\n",
        "            npys_label.append(data_folder/row[col_label] if data_folder is not None else row[col_label])\n",
        "    if(batch_length==0):\n",
        "        npys_to_memmap(npys_data, target_filename, max_len=max_len, delete_npys=delete_npys)\n",
        "    else:\n",
        "        npys_to_memmap_batched(npys_data, target_filename, max_len=max_len, delete_npys=delete_npys,batch_length=batch_length)\n",
        "    if(annotation):\n",
        "        if(batch_length==0):\n",
        "            npys_to_memmap(npys_label, target_filename.parent/(target_filename.stem+\"_label.npy\"), max_len=max_len, delete_npys=delete_npys)\n",
        "        else:\n",
        "            npys_to_memmap_batched(npys_label, target_filename.parent/(target_filename.stem+\"_label.npy\"), max_len=max_len, delete_npys=delete_npys, batch_length=batch_length)\n",
        "\n",
        "    #replace data(filename) by integer\n",
        "    df_mapped = df.copy()\n",
        "    df_mapped[\"data_original\"]=df_mapped.data\n",
        "    df_mapped[\"data\"]=np.arange(len(df_mapped))\n",
        "\n",
        "    df_mapped.to_pickle(target_filename.parent/(\"df_\"+target_filename.stem+\".pkl\"))\n",
        "    return df_mapped"
      ],
      "metadata": {
        "id": "TDefNgemHMOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV5NxiWVPmxq"
      },
      "outputs": [],
      "source": [
        "def prepare_data(data_path, denoised=False, target_fs=100, strat_folds=10, channels=8, channel_stoi=channel_stoi_default, target_folder=None, skimage_transform=True, recreate_data=True):\n",
        "    '''prepares the Zheng et al 2020 dataset'''\n",
        "    target_root = Path(\".\") if target_folder is None else target_folder\n",
        "    target_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if(recreate_data is True):\n",
        "        #df_attributes = pd.read_excel(\"./AttributesDictionary.xlsx\")\n",
        "        #df_conditions = pd.read_excel(\"./ConditionNames.xlsx\")\n",
        "        #df_rhythm = pd.read_excel(\"./RhythmNames.xlsx\")\n",
        "        df = pd.read_excel(data_path/\"Diagnostics.xlsx\")\n",
        "        df[\"id\"]=df.FileName\n",
        "        df[\"data\"]=df.FileName.apply(lambda x: x+\".npy\")\n",
        "        df[\"label_condition_txt\"]=df.Beat.apply(lambda x: [y for y in x.split(\" \") if x!=\"NONE\"])\n",
        "        df[\"label_rhythm_txt\"]=df.Rhythm.apply(lambda x: x.split(\" \"))\n",
        "        df[\"label_txt\"]=df.apply(lambda row: row[\"label_condition_txt\"]+row[\"label_rhythm_txt\"],axis=1)\n",
        "        df[\"sex\"]=df.Gender.apply(lambda x:x.lower())\n",
        "        df[\"age\"]=df.PatientAge\n",
        "        df.drop([\"Gender\",\"PatientAge\",\"Rhythm\",\"Beat\",\"FileName\"],inplace=True,axis=1)\n",
        "\n",
        "        #map to numerical indices\n",
        "        lbl_itos={}\n",
        "        lbl_stoi={}\n",
        "        lbl_itos[\"all\"] = np.unique([item for sublist in list(df.label_txt) for item in sublist])\n",
        "        lbl_stoi[\"all\"] = {s:i for i,s in enumerate(lbl_itos[\"all\"])}\n",
        "        df[\"label\"] = df[\"label_txt\"].apply(lambda x: [lbl_stoi[\"all\"][y] for y in x])\n",
        "        lbl_itos[\"condition\"] = np.unique([item for sublist in list(df.label_condition_txt) for item in sublist])\n",
        "        lbl_stoi[\"condition\"] = {s:i for i,s in enumerate(lbl_itos[\"condition\"])}\n",
        "        df[\"label_condition\"] = df[\"label_condition_txt\"].apply(lambda x: [lbl_stoi[\"condition\"][y] for y in x])\n",
        "        lbl_itos[\"rhythm\"] = np.unique([item for sublist in list(df.label_rhythm_txt) for item in sublist])\n",
        "        lbl_stoi[\"rhythm\"] = {s:i for i,s in enumerate(lbl_itos[\"rhythm\"])}\n",
        "        df[\"label_rhythm\"] = df[\"label_rhythm_txt\"].apply(lambda x: [lbl_stoi[\"rhythm\"][y] for y in x])\n",
        "        df[\"dataset\"]=\"Zheng2020\"\n",
        "\n",
        "        for id,row in tqdm(list(df.iterrows())):\n",
        "            fs = 500.\n",
        "\n",
        "            df_tmp = pd.read_csv(data_path/(\"ECGDataDenoised\")/(row[\"id\"]+\".csv\"))\n",
        "            channel_labels = list(df_tmp.columns)\n",
        "            sigbufs = np.array(df_tmp)*0.001 #assuming data is given in muV\n",
        "\n",
        "            data = resample_data(sigbufs=sigbufs,channel_stoi=channel_stoi,channel_labels=channel_labels,fs=fs,target_fs=target_fs,channels=channels,skimage_transform=skimage_transform)\n",
        "            assert(target_fs<=fs)\n",
        "            np.save(target_root/(row[\"id\"]+\".npy\"),data)\n",
        "\n",
        "        stratified_ids = stratify(list(df[\"label_txt\"]), lbl_itos[\"all\"], [1./strat_folds]*strat_folds)\n",
        "        df[\"strat_fold\"]=-1\n",
        "        idxs = np.array(df.index.values)\n",
        "        for i,split in enumerate(stratified_ids):\n",
        "            df.loc[idxs[split],\"strat_fold\"]=i\n",
        "\n",
        "        #add means and std\n",
        "        dataset_add_mean_col(df,data_folder=target_root)\n",
        "        dataset_add_std_col(df,data_folder=target_root)\n",
        "        dataset_add_length_col(df,data_folder=target_root)\n",
        "\n",
        "        #save means and stds\n",
        "        mean, std = dataset_get_stats(df)\n",
        "\n",
        "        #save\n",
        "        save_dataset(df, lbl_itos, mean, std, target_root)\n",
        "    else:\n",
        "        df, lbl_itos, mean, std = load_dataset(target_root,df_mapped=False)\n",
        "    return df, lbl_itos, mean, std"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_fs=100\n",
        "data_root=Path(\"/content/ECG_Data/\")\n",
        "target_root=Path(\"./ecg_data_processed\")\n"
      ],
      "metadata": {
        "id": "wbMi88W__Dwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_folder_zheng = data_root/\"zheng2020/\"\n",
        "target_folder_zheng = target_root/(\"zheng_fs\"+str(target_fs))"
      ],
      "metadata": {
        "id": "wzt_DLuQ_WEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_zheng, lbl_itos_zheng,  mean_zheng, std_zheng = prepare_data(data_folder_zheng, denoised=False, target_fs=target_fs, channels=12, channel_stoi=channel_stoi_default, target_folder=target_folder_zheng)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "fde7c0872e7a4881ba3941a84c7f5171",
            "80a7b6a0f76040ffa4ca3e36df5f2090",
            "2b7fd18b903941a19c5fb2bb2880149a",
            "2e4d36c0b4454f40b052de02bb25ca85",
            "3d93167d4190427d8f70ecb8486a4454",
            "40c404090cd3408698e6425f8e98f0dd",
            "26ce064b9fa14579adcf1f8426d62419",
            "a5896f3f2adb4a469abbaf915ae1b496",
            "9e98daeba5d6459b94e37771eb509295",
            "36bc58c1df9f4a43b6ec6200aaefb0a3",
            "770df2eaf36b4489ae30912310d247ec"
          ]
        },
        "id": "y4pp2AmeBL8M",
        "outputId": "f0ce3055-e89f-44ff-b3b3-fbf0b3bc4f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fde7c0872e7a4881ba3941a84c7f5171"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fold distribution...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reformat_as_memmap(df_zheng, target_folder_zheng/(\"memmap.npy\"),data_folder=target_folder_zheng,delete_npys=True)\n"
      ],
      "metadata": {
        "id": "r-TPjfS8G4B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "15Fv4k02rKvB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}