{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from models.gat_transformer import GATConvTransformer, GATConvLSTM\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "from torch_geometric.data import Batch\n",
    "from models.gcn import GCNConvLSTM\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_index(node_num=12):\n",
    "    src_nodes = [idx for idx in range(node_num) for _ in range(node_num)]\n",
    "    tgt_nodes = [idx for _ in range(node_num) for idx in range(node_num)]\n",
    "    edge_index = torch.tensor(np.array([src_nodes, tgt_nodes]), dtype=torch.long)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_df = pd.read_excel('data/Diagnostics.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(diagnostics_df.Rhythm.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AF', 'AFIB', 'AT', 'AVNRT', 'AVRT', 'SA', 'SAAWR', 'SB', 'SR',\n",
       "       'ST', 'SVT'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_df['label'] = le.transform(diagnostics_df.Rhythm.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         7\n",
       "2         5\n",
       "3         7\n",
       "4         0\n",
       "         ..\n",
       "10641    10\n",
       "10642    10\n",
       "10643    10\n",
       "10644    10\n",
       "10645    10\n",
       "Name: label, Length: 10646, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnostics_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7     3889\n",
       "8     1826\n",
       "1     1780\n",
       "9     1568\n",
       "10     587\n",
       "0      445\n",
       "5      399\n",
       "2      121\n",
       "3       16\n",
       "4        8\n",
       "6        7\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnostics_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "589it [00:05, 112.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1117it [00:10, 111.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1504it [00:15, 33.91it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2092it [00:33, 37.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2709it [00:50, 39.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2858it [00:54, 45.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3082it [00:59, 40.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3606it [01:13, 40.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5052it [01:50, 39.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5153it [01:52, 43.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5743it [02:06, 44.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ECGDataDenoised/MUSE_20180113_124215_52000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5836it [02:09, 42.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6453it [02:23, 40.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7936it [02:56, 43.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8520it [03:09, 49.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8797it [03:15, 52.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10377it [03:49, 37.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10421it [03:50, 46.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10646it [03:55, 45.19it/s]\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "data_path = Path('data', 'ECGDataDenoised')\n",
    "for file_name in tqdm(data_path.iterdir()):\n",
    "    if file_name.suffix == '.csv':\n",
    "        sub_name = file_name.stem\n",
    "        y = torch.tensor(diagnostics_df.loc[diagnostics_df.FileName == sub_name].label.values[0])\n",
    "        df = pd.read_csv(file_name, header=None)\n",
    "        if len(df) != 5000:\n",
    "            print(file_name)\n",
    "            continue\n",
    "        x = torch.Tensor(np.array([df[col].values for col in df.columns]))\n",
    "        if x.isnan().any():\n",
    "            print('found NaN! skipping file...')\n",
    "            continue\n",
    "        edge_index = get_edge_index(node_num=12)\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data_list[:8000], batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(data_list[8000:], batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[96, 5000], edge_index=[2, 1152], y=[8], batch=[96], ptr=[9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = torch_geometric.utils.to_networkx(instance, to_undirected=True)\n",
    "#nx.draw(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channels = 250\n",
    "device = 'cpu' if not torch.cuda.is_available() else 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(-1, 250)\n",
      "  (bn1): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): GCNConv(250, 250)\n",
      "  (bn2): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): GCNConv(250, 250)\n",
      "  (lin): Linear(in_features=250, out_features=11, bias=True)\n",
      ")\n",
      "GAT(\n",
      "  (conv1): GATConv(-1, 250, heads=1)\n",
      "  (bn1): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): GATConv(250, 250, heads=1)\n",
      "  (bn2): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): GATConv(250, 250, heads=1)\n",
      "  (lin): Linear(in_features=250, out_features=11, bias=True)\n",
      ")\n",
      "GNN(\n",
      "  (conv1): GraphConv(-1, 250)\n",
      "  (conv2): GraphConv(250, 250)\n",
      "  (conv3): GraphConv(250, 250)\n",
      "  (lin): Linear(in_features=250, out_features=11, bias=True)\n",
      ")\n",
      "GCNLSTM(\n",
      "  (conv1): GCNConvLSTM(1, 250)\n",
      "  (bn1): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): GCNConv(250, 250)\n",
      "  (bn2): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): GCNConv(250, 250)\n",
      "  (lin): Linear(in_features=250, out_features=11, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Training Graph Neural Networks for Graph Classification\n",
    "# Embed each node by performing multiple rounds of message passing\n",
    "# Aggregate node embeddings into a unified graph embedding (readout layer)\n",
    "# Train a final classifier on the graph embedding\n",
    " \n",
    "# There exists multiple readout layers in literature, but the most common one is to simply take the average of node embeddings\n",
    "\n",
    "# For ex: In GCNConv we use the  ReLU(ùë•)=max(ùë•,0)  activation for obtaining localized node embeddings,\n",
    "# before we apply our final classifier on top of a graph readout layer.\n",
    "\n",
    "# PyTorch Geometric provides this functionality via torch_geometric.nn.global_mean_pool, \n",
    "# which takes in the node embeddings of all nodes in the mini-batch and the assignment vector batch \n",
    "# to compute a graph embedding of size [batch_size, hidden_channels] for each graph in the batch.\n",
    "\n",
    "# The final architecture for applying GNNs to the task of graph classification then looks as follows and allows for complete end-to-end training:\n",
    "\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import GraphConv\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GraphConv(-1, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, len(le.classes_))\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(-1, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, len(le.classes_))\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GAT, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(-1, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, len(le.classes_))\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class GCNLSTM(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNLSTM, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConvLSTM(1, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, len(le.classes_))\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "       \n",
    "# Define the models\n",
    "model1 = GCN(hidden_channels=hidden_channels).to(device)\n",
    "model2= GAT(hidden_channels=hidden_channels).to(device)\n",
    "model3 = GNN(hidden_channels=hidden_channels).to(device)\n",
    "model4 = GCNLSTM(hidden_channels=hidden_channels).to(device)\n",
    "\n",
    "# Print them \n",
    "print(model1)\n",
    "print(model2)\n",
    "print(model3)\n",
    "print(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model paramters and model type\n",
    "def set_model_parameters(model_type, lr=0.01):\n",
    "    model = model_type\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "# Train the model\n",
    "def train(model, optimizer,criterion):\n",
    "    model.train()\n",
    "\n",
    "    for idx, data in enumerate(train_loader):  # Iterate in batches over the training dataset.\n",
    "        data.x = data.x.to(device)\n",
    "        data.y = data.y.to(device)\n",
    "        data.edge_index = data.edge_index.to(device)\n",
    "        data.batch = data.batch.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        print(f'output: {out}')\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        print(f'loss: {loss}')\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        \n",
    "        if idx % 500 == 0:\n",
    "            print(f'{idx}tr) loss: {loss}')\n",
    "    return\n",
    "\n",
    "# Test the model \n",
    "def test(loader, model):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        data.x = data.x.to(device)\n",
    "        data.y = data.y.to(device)\n",
    "        data.edge_index = data.edge_index.to(device)\n",
    "        data.batch = data.batch.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "# Training and Testing Pipeline \n",
    "def running_epochs(model,optimizer,criterion):\n",
    "    for epoch in range(1, 10):\n",
    "        train(model,optimizer,criterion)\n",
    "        train_acc = test(train_loader, model)\n",
    "        test_acc = test(test_loader, model)\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2114612/1196594096.py:3: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0tr) loss: 1.2389345169067383\n",
      "500tr) loss: 2.163346767425537\n",
      "Epoch: 001, Train Acc: 0.4736, Test Acc: 0.4627\n",
      "0tr) loss: 1.745064377784729\n",
      "500tr) loss: 2.2241251468658447\n",
      "Epoch: 002, Train Acc: 0.5139, Test Acc: 0.4886\n",
      "0tr) loss: 1.5111541748046875\n",
      "500tr) loss: 1.2976725101470947\n",
      "Epoch: 003, Train Acc: 0.5221, Test Acc: 0.4810\n",
      "0tr) loss: 1.2629868984222412\n",
      "500tr) loss: 1.319043755531311\n",
      "Epoch: 004, Train Acc: 0.5679, Test Acc: 0.5183\n",
      "0tr) loss: 1.5851362943649292\n",
      "500tr) loss: 1.4444528818130493\n",
      "Epoch: 005, Train Acc: 0.5198, Test Acc: 0.4817\n",
      "0tr) loss: 1.2885963916778564\n",
      "500tr) loss: 1.0512919425964355\n",
      "Epoch: 006, Train Acc: 0.5939, Test Acc: 0.5209\n",
      "0tr) loss: 2.2602334022521973\n",
      "500tr) loss: 1.5723509788513184\n",
      "Epoch: 007, Train Acc: 0.5821, Test Acc: 0.5065\n",
      "0tr) loss: 1.2063523530960083\n",
      "500tr) loss: 1.6647557020187378\n",
      "Epoch: 008, Train Acc: 0.5885, Test Acc: 0.4973\n",
      "0tr) loss: 0.8360915780067444\n",
      "500tr) loss: 1.4548461437225342\n",
      "Epoch: 009, Train Acc: 0.5831, Test Acc: 0.5068\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: GCN Baseline \n",
    "model, optimizer, criterion = set_model_parameters(model1, lr=0.001)\n",
    "# with torch.autograd.detect_anomaly(): # used for debugging\n",
    "running_epochs(model,optimizer,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2114612/497437324.py:3: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly(): # used for debugging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[ 0.1708, -0.4179, -0.6674,  0.2599, -0.2027,  0.7780,  0.3570, -0.2831,\n",
      "          0.0384,  0.1920,  0.1012],\n",
      "        [-0.3191, -0.0261,  0.5773, -0.0172, -0.1099, -0.2526, -0.1489,  0.3435,\n",
      "          0.6470, -0.3529, -0.3762],\n",
      "        [-0.0315,  0.7417,  0.2363,  0.0673,  0.1925, -1.5089, -1.1498,  0.3531,\n",
      "          0.6929,  0.0982, -0.4312],\n",
      "        [-0.8694, -0.1196,  0.0140, -0.1988, -0.2810,  0.1909,  0.1903,  0.1838,\n",
      "          0.4722, -0.5285, -0.2669],\n",
      "        [-0.7587,  0.3654,  0.3210,  0.7196,  0.0293,  0.1648,  0.1539, -0.7173,\n",
      "          0.8345,  0.0129,  0.2239],\n",
      "        [ 0.4746, -0.0060, -0.1980, -0.0292,  0.1588,  0.2983,  1.3400,  0.7774,\n",
      "          0.1064, -1.3597, -0.2671],\n",
      "        [ 0.7806, -0.1176,  0.3090, -0.3527,  0.4529,  0.5664,  0.7344,  1.3921,\n",
      "          1.2538, -1.0327,  0.3247],\n",
      "        [ 0.3681,  1.1594,  0.1511, -0.0604,  0.4586,  0.5777,  0.7259, -0.7239,\n",
      "          0.4492, -1.7603, -0.0696]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.630655527114868\n",
      "0tr) loss: 2.630655527114868\n",
      "output: tensor([[ 1.1864, -0.8879, -1.1718, -0.4805, -1.1088, -1.2909, -0.2356,  5.1971,\n",
      "          1.1964,  1.1478, -2.0329],\n",
      "        [-0.1393,  0.3681, -0.6674, -0.1038, -0.9306, -1.1600, -0.9459,  3.4378,\n",
      "          2.0192,  0.6422, -1.4339],\n",
      "        [-1.8785, -0.7894, -1.0749, -0.9465, -0.9747, -0.4489, -1.2324,  4.5099,\n",
      "          1.5670,  0.0613, -1.9036],\n",
      "        [ 1.4801, -0.8205, -0.3079, -1.1745, -1.6714, -1.8638, -0.7841,  3.3743,\n",
      "          0.4608,  1.8997, -1.9764],\n",
      "        [-0.1059,  0.4917, -1.2034, -0.8434, -0.3404, -0.0918, -0.4006,  3.3513,\n",
      "          0.2161,  0.2009, -1.1005],\n",
      "        [-1.1014, -2.1091, -3.0250, -0.9355, -4.7451, -2.8854, -2.6753,  7.0892,\n",
      "          4.9792, -2.2612, -0.7107],\n",
      "        [ 1.0435,  0.2696, -0.8607, -2.3049, -2.1067, -1.4346,  0.1844,  5.3693,\n",
      "          0.4722,  0.3293, -2.1410],\n",
      "        [ 0.5315,  1.5111, -2.0939, -3.2822, -2.5786, -0.5448, -0.6380,  6.1979,\n",
      "          0.9611,  0.7616, -2.0831]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.1933207511901855\n",
      "output: tensor([[-3.2722e-01,  3.8671e+00, -1.8372e+00, -3.5029e-01, -5.2376e-01,\n",
      "         -2.5683e+00, -1.4205e+00,  2.4730e+00,  1.3045e+00, -3.0609e-01,\n",
      "         -2.6005e+00],\n",
      "        [-9.2943e-01,  2.0420e+00, -3.2693e+00, -8.2515e-01, -1.6406e+00,\n",
      "         -1.7660e+00, -1.4712e+00,  3.2657e+00, -6.5217e-01,  2.1621e+00,\n",
      "         -2.6528e+00],\n",
      "        [-7.9498e-01,  3.2742e+00, -6.5792e-01, -2.2383e+00, -2.1838e+00,\n",
      "         -3.4675e+00, -1.6839e+00,  1.6375e+00, -2.4321e-01, -2.9366e-01,\n",
      "         -3.6916e+00],\n",
      "        [-3.4992e+00,  3.3843e+00, -7.1302e+00, -6.1350e+00, -8.4663e+00,\n",
      "         -4.6956e+00, -4.8150e+00,  1.1935e+01,  7.9913e+00,  1.9000e-01,\n",
      "         -2.7438e+00],\n",
      "        [-2.9921e-01, -1.2596e+00, -2.6001e+00, -2.1778e+00, -3.1408e+00,\n",
      "         -8.5542e-01, -2.5633e+00,  5.4649e+00,  1.5699e+00,  1.5186e+00,\n",
      "         -2.3864e+00],\n",
      "        [-1.6706e+00,  2.5939e+00, -1.4948e+00, -2.3417e+00, -3.0479e+00,\n",
      "         -1.5318e+00, -2.2511e+00,  6.0707e+00,  1.3335e+00,  1.3656e+00,\n",
      "         -2.3848e+00],\n",
      "        [-8.1412e-01, -1.0121e-01, -8.0671e-01, -1.5587e+00, -2.8221e+00,\n",
      "         -1.3556e+00, -2.2835e+00,  2.5928e+00,  1.9340e+00,  4.2640e-01,\n",
      "         -1.3533e+00],\n",
      "        [-1.1864e-02,  2.4807e+00, -1.4059e+00, -1.1843e+00, -1.0061e+00,\n",
      "         -1.7560e+00, -4.2017e-01,  1.3941e+00, -2.1693e-01,  5.8237e-01,\n",
      "         -1.9689e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 0.8269354104995728\n",
      "output: tensor([[-1.2201,  1.1220, -2.4228, -2.2193, -2.6656, -1.4780, -1.7742,  4.4733,\n",
      "          4.1555,  2.2452, -2.3503],\n",
      "        [-5.4242,  0.1693, -7.2619, -0.8666, -8.5871, -4.3932, -5.5921,  9.1747,\n",
      "         11.3136,  2.0983, -7.0911],\n",
      "        [-0.4156,  5.8436, -0.5565, -0.7969, -1.4843, -2.4459, -1.0304,  0.7780,\n",
      "          2.5261,  1.2821, -2.5461],\n",
      "        [-2.6006,  1.3154, -1.8610, -1.5569, -3.1576, -1.4207, -1.5397,  5.1111,\n",
      "          2.5195,  1.0218, -2.5174],\n",
      "        [ 0.2486,  4.7073, -1.0732,  0.3132, -2.8314, -2.3282, -2.2439, -0.8853,\n",
      "          3.8239,  0.1025, -2.1587],\n",
      "        [-3.5570,  2.5425, -3.9450, -2.3546, -4.8331, -3.3416, -4.4754,  5.8527,\n",
      "          4.2798,  1.7016, -4.1327],\n",
      "        [-2.4968,  2.2073, -3.3664, -1.5348, -4.1716, -2.3247, -4.0732,  6.5919,\n",
      "          5.4821,  1.3275, -3.5130],\n",
      "        [-1.2945,  6.4024, -2.9987, -3.0804, -2.2334, -1.4827, -1.5585,  0.7856,\n",
      "          3.4507,  0.6361, -3.1421]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.850243091583252\n",
      "output: tensor([[-4.0897,  3.6658, -3.9356, -4.2641, -5.4462, -4.0326, -3.8917,  5.3116,\n",
      "          2.5008,  2.8345, -2.0866],\n",
      "        [-3.0013,  4.8166, -3.0040, -2.2657, -4.1813, -3.7802, -3.9511, -1.2885,\n",
      "          8.0093,  3.6710, -1.2515],\n",
      "        [-1.2732,  2.5569, -1.4177, -1.5857, -2.7718, -1.3590, -3.2342, -2.2685,\n",
      "          4.7496,  1.5115, -1.1740],\n",
      "        [-4.1289,  2.3535, -3.2685, -3.4524, -3.8065, -2.6958, -4.1424,  3.5480,\n",
      "          5.1080,  1.5358, -1.9769],\n",
      "        [-3.9431,  1.2262, -4.9856, -5.5691, -5.6879, -4.9296, -6.2600,  5.7149,\n",
      "          8.4228, -1.2369, -1.9675],\n",
      "        [-2.2546,  1.1342, -1.4603, -0.9224, -2.4934, -1.4977, -1.5556,  1.5756,\n",
      "          5.9101,  3.0134,  1.0867],\n",
      "        [-1.5854,  1.2993, -1.4397, -1.2961, -2.7862, -2.3356, -2.2745, -1.1317,\n",
      "          4.4817,  1.6526, -1.2378],\n",
      "        [-1.1172,  1.1022, -0.8361, -0.3200, -2.4079, -1.6170, -2.2287,  0.2664,\n",
      "          5.9905,  1.4665,  1.1118]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.81780481338501\n",
      "output: tensor([[-2.5156,  0.3014, -1.4530, -0.6364, -2.2504, -1.5365, -1.6938, -1.9859,\n",
      "          3.0197,  3.5681,  0.4206],\n",
      "        [-0.9974,  2.0815, -0.7360, -0.0858, -2.7318, -1.3569, -4.3474,  7.6254,\n",
      "          5.9646,  4.0046, 11.0919],\n",
      "        [-1.5556, -0.6715, -1.1347, -0.7672, -3.1646, -2.7151, -2.5285, -0.8688,\n",
      "          4.3116,  2.9233,  1.1222],\n",
      "        [-6.2399, -1.2914, -2.8646, -3.2464, -7.5629, -5.0035, -5.5887,  4.4735,\n",
      "          5.2037,  6.7746,  6.9768],\n",
      "        [-2.9219, -0.3168, -1.4173, -1.3582, -2.4858, -1.6605, -2.1658, -2.0257,\n",
      "          3.7723,  3.9840,  0.6100],\n",
      "        [-2.0577,  0.3584, -2.1212, -1.4997, -2.7781, -2.1927, -2.7850,  0.0418,\n",
      "          3.6326,  0.7461,  0.6173],\n",
      "        [-3.9609,  0.7019,  0.5440, -1.6977, -5.2041, -1.9784, -1.9061,  7.6258,\n",
      "          2.0054,  6.0385,  5.7839],\n",
      "        [-4.1273, -4.8392, -1.6595, -1.5719, -3.6377, -2.5660, -2.0767,  0.6614,\n",
      "          3.8077,  3.4176, -0.3730]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.8530867099761963\n",
      "output: tensor([[-1.0721e+00, -8.3561e-01, -7.3938e-01,  9.9452e-02, -2.0486e+00,\n",
      "         -8.8678e-01, -7.1489e-01,  1.8811e-03,  1.5616e+00,  2.6390e+00,\n",
      "          1.1411e+00],\n",
      "        [-8.8190e-01, -2.5920e+00, -4.4695e+00, -3.6243e+00, -5.1905e+00,\n",
      "         -3.2494e+00, -3.1229e+00,  1.3759e-01,  3.1573e+00,  6.7310e+00,\n",
      "          2.6933e+00],\n",
      "        [-8.9202e+00,  1.0631e+00, -1.1660e+00, -2.2158e+00, -6.3278e+00,\n",
      "         -2.3330e+00, -5.7738e+00,  1.2176e+01, -5.5660e+00,  6.1940e+00,\n",
      "          3.7736e+00],\n",
      "        [-3.3488e+00,  5.9689e-01, -2.4070e+00, -4.5174e+00, -4.6111e+00,\n",
      "         -1.5052e+00, -4.2804e+00,  7.5708e+00, -1.6056e+00,  7.7955e+00,\n",
      "          2.0650e+00],\n",
      "        [-3.6384e+00,  1.3756e+00, -1.3773e+00, -1.5065e+00, -3.5610e+00,\n",
      "         -5.6006e-01, -2.4584e+00,  4.0034e+00, -1.0027e+00,  1.6083e+00,\n",
      "          1.5364e+00],\n",
      "        [-5.4151e+00,  1.2476e+00, -4.6942e+00, -4.9388e+00, -7.8918e+00,\n",
      "         -3.1849e+00, -4.9160e+00,  8.9260e+00, -1.6900e+00,  6.8287e-01,\n",
      "          1.6155e+00],\n",
      "        [-5.9373e+00, -5.3888e+00, -4.8356e+00, -1.8493e+00, -5.4728e+00,\n",
      "         -3.7380e+00, -7.3828e+00,  2.7808e+00,  1.6925e+00,  3.1205e+00,\n",
      "          2.6533e-01],\n",
      "        [-2.1875e+00,  1.0964e+00, -1.1218e+00,  4.0964e-01, -1.4519e+00,\n",
      "         -9.1646e-01, -5.4478e-01,  7.1227e-03,  1.0419e+00,  2.6984e+00,\n",
      "          1.8321e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 1.1936063766479492\n",
      "output: tensor([[-2.5333e+00,  2.5263e-01, -2.7184e+00, -2.2379e+00, -3.4258e+00,\n",
      "         -1.0946e+00, -1.4382e+00,  3.3109e+00,  5.5977e-01,  1.8071e+00,\n",
      "          4.2356e+00],\n",
      "        [-2.4949e+00, -1.5258e+00, -2.0877e+00, -1.3934e+00, -3.4008e+00,\n",
      "         -1.3166e+00, -3.1600e+00,  1.8796e+00, -2.4884e-02,  3.2378e+00,\n",
      "          3.9085e-03],\n",
      "        [-1.1365e+01,  3.4975e+00, -6.8885e+00, -6.4398e+00, -9.5928e+00,\n",
      "         -3.9033e+00, -5.3726e+00,  1.0257e+01, -7.2635e+00,  5.4997e+00,\n",
      "          2.8801e+00],\n",
      "        [-4.8342e+00, -1.6662e+00, -3.7545e+00, -2.2857e+00, -4.9157e+00,\n",
      "         -3.0528e+00, -4.3285e+00,  3.8782e+00, -1.5797e+00,  4.5507e+00,\n",
      "         -3.6137e-01],\n",
      "        [-4.2954e+00, -1.4539e-01, -2.2779e+00, -2.7290e+00, -2.6146e+00,\n",
      "         -1.0480e+00, -2.8316e+00,  3.7606e+00, -3.5395e-01,  1.1989e+00,\n",
      "          4.6704e-01],\n",
      "        [-1.5491e+01,  4.1910e+00, -1.1615e+01, -1.2376e+01, -1.2016e+01,\n",
      "         -5.9281e+00, -9.4687e+00,  1.0952e+01, -1.2775e+01,  1.1360e+01,\n",
      "          4.2448e+00],\n",
      "        [-6.3097e+00, -6.4825e+00, -4.9196e+00, -2.8974e+00, -6.1456e+00,\n",
      "         -5.5625e+00, -6.1651e+00,  4.9784e+00, -2.6509e-01,  6.1647e+00,\n",
      "          1.6690e-01],\n",
      "        [-2.1470e+00, -1.3297e+00, -2.1292e+00, -2.3789e+00, -4.2282e+00,\n",
      "         -2.9130e+00, -2.8374e+00,  1.2134e+00, -1.0764e+00,  1.1604e+00,\n",
      "          7.4550e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 4.595273971557617\n",
      "output: tensor([[-4.2180,  3.4956, -2.3216, -2.4478, -4.0996, -2.1262, -2.4016,  2.9766,\n",
      "         -2.6211,  2.3097,  2.2970],\n",
      "        [-1.5260, -3.1184, -0.1413, -0.4240, -1.4869, -0.1342, -2.9120,  2.5922,\n",
      "         -0.9810,  2.6757,  1.8290],\n",
      "        [-6.9960,  7.8723, -5.3460, -4.7586, -8.8471, -4.8852, -7.2317, 12.3863,\n",
      "         -8.6929,  3.5224,  5.1063],\n",
      "        [-9.0006,  4.8256, -1.9629, -6.7446, -6.0758, -5.4433, -6.0293, 10.6536,\n",
      "         -8.5995,  1.4781,  4.2797],\n",
      "        [-6.5423, -2.8688, -4.2301, -4.4279, -6.6662, -2.2632, -4.9319,  5.8922,\n",
      "         -2.2548,  4.0895,  1.4973],\n",
      "        [-3.0423, -1.9662, -1.5081, -1.7162, -2.4716, -2.0169, -1.4116,  2.5379,\n",
      "         -2.9035,  4.3196,  1.2062],\n",
      "        [-6.8401,  2.6886, -5.5535, -6.0876, -8.9073, -5.2752, -4.5371,  8.6299,\n",
      "         -9.5048,  8.7825,  3.8718],\n",
      "        [-3.0031,  1.5401, -2.8303, -2.6810, -2.9726, -0.2345, -2.6571,  2.3409,\n",
      "         -0.5956,  2.5560,  0.9254]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.536675214767456\n",
      "output: tensor([[ -4.9224,  -4.4768,  -4.6232,  -4.9351,  -6.3655,  -0.2267,  -5.3734,\n",
      "           5.3866,  -2.3536,   3.7786,  -0.9426],\n",
      "        [ -3.2544,   2.3450,  -2.9323,  -4.8283,  -6.7220,  -1.4252,  -5.0489,\n",
      "           4.5578,  -5.2505,   1.3060,   1.8436],\n",
      "        [ -2.3252,   3.8351,  -2.9746,  -2.2817,  -2.8718,   0.4725,  -0.9322,\n",
      "           2.4817,  -5.4548,   2.1717,   3.9460],\n",
      "        [-10.2199,   8.8269,  -6.0221,  -7.4545,  -6.0972,  -7.6818,  -7.8887,\n",
      "          10.9473,  -8.8276,  -4.1539,   4.9152],\n",
      "        [ -1.3024,  -2.5994,   1.0751,  -1.7140,  -0.7707,   1.1661,  -0.8563,\n",
      "           3.8151,  -3.9809,   1.4670,   0.9062],\n",
      "        [ -7.1506,   9.4373,  -8.7967, -11.7406, -13.7965,  -2.7154,  -7.6820,\n",
      "          12.9185,  -8.4319,  -0.9599,   6.7609],\n",
      "        [ -4.4809,  -0.5938,  -4.6919,  -3.6978,  -5.7111,  -3.0570,  -5.6966,\n",
      "           4.8917,  -3.3334,   2.3991,   1.6859],\n",
      "        [ -1.7852,  -2.1466,  -2.8817,  -2.8961,  -2.8080,   0.6837,  -3.1720,\n",
      "           1.0060,  -2.0564,   3.3885,   1.1312]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.627277135848999\n",
      "output: tensor([[-3.0586e+00,  4.4085e+00, -3.7879e+00, -3.9123e+00, -5.4093e+00,\n",
      "         -8.8605e-01, -4.7516e+00,  5.2103e+00, -2.3920e+00, -2.5001e+00,\n",
      "          3.7814e-01],\n",
      "        [-4.5395e+00,  6.8740e+00, -7.9948e+00, -8.7562e+00, -7.2399e+00,\n",
      "         -3.2062e+00, -5.0445e+00,  5.3437e+00, -6.1872e+00, -1.7198e+00,\n",
      "          5.2507e+00],\n",
      "        [-1.2624e+00, -2.4969e+00, -3.0202e+00, -3.6970e+00, -4.7118e+00,\n",
      "          2.9880e+00, -4.0949e+00, -3.1638e-01, -6.2953e+00,  5.7482e+00,\n",
      "         -5.8690e-01],\n",
      "        [-4.9831e+00,  3.8455e+00, -4.5826e+00, -7.3279e+00, -6.5799e+00,\n",
      "         -2.7891e+00, -5.9790e+00,  7.5801e+00, -2.3390e+00, -4.7701e+00,\n",
      "         -8.1958e-01],\n",
      "        [-2.4802e+00, -3.6193e+00, -2.5705e+00, -2.4510e+00, -4.4201e+00,\n",
      "          2.7080e+00, -8.0028e+00,  5.3108e+00, -5.0704e+00, -1.9168e+00,\n",
      "          2.9200e+00],\n",
      "        [-4.7337e+00,  1.0782e+01, -4.1061e+00, -6.6682e+00, -7.1886e+00,\n",
      "          4.5496e-03, -6.7564e+00,  5.2153e+00, -8.7267e+00,  1.6144e+00,\n",
      "          7.1714e+00],\n",
      "        [-6.8645e-01,  1.7826e+00, -2.2367e+00, -2.7219e+00, -2.5165e+00,\n",
      "         -1.1747e+00, -1.5895e+00,  1.8842e+00, -1.6318e+00,  5.1963e-02,\n",
      "          1.6898e+00],\n",
      "        [-1.8541e+00,  2.2620e+00, -1.1005e+00, -2.2957e+00, -2.3037e+00,\n",
      "         -1.0709e+00, -8.4912e-01,  1.8504e+00, -3.4804e+00,  2.0477e-01,\n",
      "          1.9669e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 4.563239097595215\n",
      "output: tensor([[  1.2517,   7.3936,  -1.3702,  -4.0257,  -4.2567,   3.4006,  -2.7298,\n",
      "           6.1703,  -7.9134,  -0.1771,   6.1509],\n",
      "        [  0.2840,   2.0762,  -2.1091,  -2.9820,  -2.3925,   0.1559,  -0.9363,\n",
      "           0.2550,  -1.8656,   1.3855,   1.8648],\n",
      "        [  0.1176,   0.4717,  -2.7052,  -0.0787,  -3.1289,   1.4475,  -3.7465,\n",
      "           4.7046,  -6.0649,  -2.5145,   3.8668],\n",
      "        [ -0.9482,  -1.7393,  -2.0419,  -3.6061,  -4.0812,   2.9186,  -2.3559,\n",
      "           2.4082,  -6.7180,   1.4909,   2.9507],\n",
      "        [  0.4299,  -2.3688,  -3.4558,  -5.0929,  -6.0933,   2.4191,  -4.2669,\n",
      "           5.9166,  -6.3352,  -0.9731,   1.1579],\n",
      "        [ -7.6470,   9.8912,  -7.8331,  -5.7249, -12.0586,  -0.0610,  -8.1728,\n",
      "          12.1553,  -5.0109,  -5.7547,   0.9528],\n",
      "        [ -4.6945,   6.3966,  -8.0730, -14.2786,  -9.6175,  -3.6312,  -7.2952,\n",
      "          10.3786,  -4.4645,  -4.0214,   0.4345],\n",
      "        [ -2.6929,   3.2484,  -4.5574,  -5.1865,  -6.5247,  -3.1843,  -3.9026,\n",
      "           4.8432,  -1.9354,  -0.9419,  -0.0360]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.660097122192383\n",
      "output: tensor([[  3.4164,  -2.5699,  -2.4041,  -3.6332,  -2.2398,   3.1335,  -2.7704,\n",
      "          -2.0694,  -6.7476,  -0.1341,   3.2834],\n",
      "        [ -2.3767,   2.8389,  -4.3166,  -3.6916,  -2.6161,   0.7872,  -1.8348,\n",
      "           1.8109,  -1.8130,  -1.6904,   1.3764],\n",
      "        [ -8.2823,   2.0036, -10.3079, -10.4970, -11.0943,  -5.3478, -12.3503,\n",
      "          12.2447,  -3.5151,  -8.9112,  -1.1442],\n",
      "        [  2.7992,  -8.6300,  -9.4322,  -7.8906,  -8.0749,   4.9765,  -8.6820,\n",
      "           7.7499,  -4.4946,  -7.3882,   2.8436],\n",
      "        [ -3.4591,   4.4021,  -4.0612,  -3.6500,  -5.0147,  -1.5964,  -5.8418,\n",
      "           3.4622,  -1.6367,  -2.4669,  -0.7021],\n",
      "        [ -5.5742,   1.9419,  -6.9384, -11.3673, -10.4311,  -4.6998,  -8.4319,\n",
      "           9.3635,  -2.4412,  -6.0687,   2.5775],\n",
      "        [  0.5822,  -1.4428,  -4.0576,  -2.9482,  -3.0898,  -0.3620,  -2.6871,\n",
      "           0.3370,  -0.7380,  -1.7830,   0.6534],\n",
      "        [  0.7829,   0.1281,  -2.2083,  -2.0225,  -2.5234,   1.3066,  -2.7242,\n",
      "          -0.0985,  -1.3133,   0.5853,  -0.3299]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.533653736114502\n",
      "output: tensor([[ 1.7285,  0.5722, -0.1805, -2.0257, -2.5274,  2.1162, -1.6317,  0.0592,\n",
      "         -0.9283, -1.3342,  1.8545],\n",
      "        [-0.4228,  1.4692, -4.0624, -3.5799, -5.3916,  0.6786, -4.0190,  2.6021,\n",
      "         -0.9824, -4.4001,  0.6613],\n",
      "        [-3.2783,  2.0245, -3.7324, -4.3753, -6.6939, -2.1146, -5.7077,  9.5582,\n",
      "         -1.1402, -7.4565,  0.8035],\n",
      "        [ 2.5137,  1.1029, -6.6601, -2.7290, -5.6000, -0.5490, -6.8470, -0.4286,\n",
      "         -3.0552,  0.1154, -1.2139],\n",
      "        [-2.7249,  0.0317, -2.6048, -5.0362, -4.4067, -2.7130, -5.5650,  5.9754,\n",
      "         -0.2880, -5.3854,  3.2290],\n",
      "        [ 0.8647,  4.2313, -2.9038, -4.6916, -5.8839,  2.2160, -8.4409,  6.0408,\n",
      "          0.2147, -3.4380, -0.8901],\n",
      "        [-2.2596,  0.6700, -5.5752, -6.6191, -8.7081, -0.8698, -8.3663,  9.8676,\n",
      "          3.2141, -7.3807,  1.5448],\n",
      "        [ 4.7866, -4.1745, -0.9869, -2.8544, -0.6447,  9.0092, -1.0326, -0.7911,\n",
      "         -6.6978, -1.9674,  3.6799]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 5.603572845458984\n",
      "output: tensor([[-2.2228,  0.5154, -4.0719, -2.8129, -3.6234, -0.4978, -2.3966, -0.5996,\n",
      "          0.8201, -1.5983,  1.4267],\n",
      "        [-1.0982,  0.1306, -3.1265, -1.3375, -1.5202, -0.2746, -2.1340, -0.8497,\n",
      "          1.7264, -1.5508,  0.8492],\n",
      "        [-1.9969, -5.5652, -6.5569, -7.6084, -8.6193,  1.7116, -8.9682,  9.4313,\n",
      "          3.9118, -7.6221,  4.0161],\n",
      "        [-2.1555,  0.6800, -4.7789, -3.7824, -4.1617, -1.3101, -5.3640,  2.1771,\n",
      "          1.8490, -1.1253,  1.3163],\n",
      "        [-0.7581, -1.4365, -8.5609, -8.5892, -7.1639, -1.4936, -8.8494,  3.7279,\n",
      "          0.2180, -5.2340, -1.6008],\n",
      "        [ 4.4855, -4.8186, -6.0579, -4.3298, -2.6049,  5.1708, -4.7934, -1.3845,\n",
      "         -5.8499, -0.5324,  1.9577],\n",
      "        [ 1.2257, -1.1388, -3.8310, -3.8657, -2.2663,  2.0727, -2.0758,  0.5143,\n",
      "         -2.4720, -1.5875,  1.5427],\n",
      "        [-1.0130,  1.0172, -4.5464, -3.6445, -3.3782, -0.4446, -3.4618,  0.7470,\n",
      "          2.8258, -0.5208,  0.1580]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.531862735748291\n",
      "output: tensor([[ 9.9979e-01, -9.8113e+00, -6.5543e+00, -5.4740e+00, -6.3907e+00,\n",
      "         -5.5790e-01, -7.6552e+00,  6.7967e+00,  4.9563e+00, -9.8151e+00,\n",
      "          3.4302e+00],\n",
      "        [-2.0333e+00, -3.1932e-01, -5.2321e+00, -5.8594e+00, -5.2264e+00,\n",
      "         -6.2228e-01, -5.7289e+00,  1.0427e+00,  3.6374e+00, -3.5993e+00,\n",
      "         -1.3380e+00],\n",
      "        [-1.2342e+00,  3.7798e+00, -3.4931e+00, -3.5194e+00, -2.6640e+00,\n",
      "         -3.3512e+00, -4.3971e+00,  2.4558e+00,  8.5748e+00, -3.9482e+00,\n",
      "         -2.9604e-01],\n",
      "        [-6.5841e-01, -1.2367e+00, -2.9233e+00, -2.4069e+00, -2.4799e+00,\n",
      "         -7.3187e-01, -2.2409e+00, -2.7533e-01,  1.3407e+00, -2.7074e-01,\n",
      "          5.7233e-01],\n",
      "        [-3.8760e+00, -1.7947e+00, -8.9102e+00, -3.6430e+00, -3.0018e+00,\n",
      "         -4.0563e+00, -3.3127e+00, -2.5133e+00, -2.9171e+00,  6.9911e+00,\n",
      "          4.1515e+00],\n",
      "        [-1.3918e+00, -6.4825e-01, -3.7226e+00, -5.9682e+00, -4.1764e+00,\n",
      "         -3.5036e+00, -4.8002e+00, -1.1047e+00, -8.6302e-01,  2.7857e+00,\n",
      "          1.1940e+00],\n",
      "        [-1.6327e+00,  1.9698e-01, -5.5735e+00, -4.0995e+00, -6.0934e+00,\n",
      "         -1.5864e+00, -6.2311e+00, -1.0633e+00,  1.2080e-01, -1.2421e-01,\n",
      "         -4.5864e-03],\n",
      "        [-3.9365e+00,  3.7025e+00, -6.7252e+00, -7.4518e+00, -7.8303e+00,\n",
      "         -2.0167e+00, -7.0592e+00,  6.6731e-01,  3.9564e+00, -3.9127e+00,\n",
      "         -2.2402e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 2.0125889778137207\n",
      "output: tensor([[ -2.3317,  -0.9072,  -7.9011,  -6.1253,  -5.2075,  -2.5493,  -5.7023,\n",
      "           2.1108,   6.4534,  -3.6335,  -0.5203],\n",
      "        [ -3.1388,   2.0651,  -8.3321,  -7.7976,  -6.9398,  -2.6031,  -7.1629,\n",
      "          -0.4131,   5.9059,  -2.3990,  -1.3789],\n",
      "        [ -0.2412,  -1.0621,  -3.9363,  -5.2068,  -2.9462,  -1.8765,  -3.6929,\n",
      "          -1.1288,   1.9954,   2.2988,  -0.2263],\n",
      "        [ -0.4205,  -1.3567,  -2.6396,  -3.6668,  -3.4322,  -1.4903,  -3.5727,\n",
      "           0.1477,   3.0416,   1.1421,  -0.4513],\n",
      "        [ -0.3269,  -1.3941,  -9.4496,  -6.6027,  -7.8269,  -2.2257,  -8.4403,\n",
      "           3.7837,   7.9883,  -7.3403,  -0.1111],\n",
      "        [ -4.1466,  -0.5685,  -9.1050,  -8.6254, -10.3298,  -2.7844,  -8.2255,\n",
      "           0.0644,   5.6494,  -2.9857,  -4.2320],\n",
      "        [ -4.0398,   1.7865,  -7.7034,  -8.0686,  -6.9236,  -2.1388,  -6.9727,\n",
      "           0.7688,   3.5257,  -2.6450,  -2.2266],\n",
      "        [-13.1557,   1.0706, -19.2586, -15.7093, -17.4183,  -9.0407, -12.3283,\n",
      "          -1.6046,   1.2876,   1.0619,   3.5513]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.322761535644531\n",
      "output: tensor([[ -1.0099,  -3.4728, -10.0068, -11.7405, -13.0107,  -4.4843, -12.9518,\n",
      "           3.9890,  10.1609,  -0.6666,  -4.0045],\n",
      "        [ -2.7581,  -2.9814,  -9.1832,  -8.2124,  -8.3662,  -2.0095,  -5.8685,\n",
      "           2.9673,   8.3339,  -1.5789,  -1.9946],\n",
      "        [ -5.6615,   1.4565,  -7.6077,  -5.9519,  -7.1610,  -6.5327,  -5.9000,\n",
      "          -0.1929,   1.7582,   0.3324,  -1.9704],\n",
      "        [ -9.4597,  -1.0033, -16.7322, -16.9736, -15.4248, -11.2623, -13.4459,\n",
      "          -0.7994,   3.3205,   3.8801,  -1.1541],\n",
      "        [ -4.1792,  -0.2289,  -9.4726,  -8.8282,  -8.7261,  -4.8850, -11.6182,\n",
      "          -0.2186,   5.5034,  -1.8358,  -4.1336],\n",
      "        [ -6.5200,   1.7316,  -9.5842,  -7.9912,  -9.2019,  -4.3720,  -8.2862,\n",
      "          -0.0394,   4.4555,  -1.2678,  -3.4200],\n",
      "        [ -2.4727,   0.4722,  -6.8757,  -6.4317,  -6.5290,  -4.7134,  -5.0869,\n",
      "          -1.0907,   2.1004,   1.2064,  -2.5455],\n",
      "        [ -4.1770,   0.2829,  -8.4505,  -7.3456,  -7.8458,  -1.2740,  -7.5218,\n",
      "           1.5708,   6.3487,  -3.3638,  -2.3428]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 5.397958278656006\n",
      "output: tensor([[  0.1488,  -2.4256,  -3.4137,  -2.8882,  -3.2895,  -1.0330,  -2.9034,\n",
      "           1.2175,   2.4934,  -1.8542,  -1.3463],\n",
      "        [ -0.8597,  -1.0005,  -8.2311,  -9.6141,  -9.9512,  -6.4508,  -7.3758,\n",
      "          -3.1948,   1.8168,   2.7895,  -4.4365],\n",
      "        [ -2.9768,  -0.7521,  -8.7753,  -5.5620,  -8.8127,  -2.4266,  -8.4106,\n",
      "           2.0123,   4.6800,  -0.0484,  -0.3747],\n",
      "        [ -8.0068,   8.3265, -20.2973, -20.2330, -21.9602, -11.9787, -19.2233,\n",
      "          -0.8651,   0.9114,  -2.1223,  -5.2907],\n",
      "        [ -1.4877,  -6.5897, -12.1785, -11.3449, -10.9566,  -4.2626, -10.5114,\n",
      "           6.1458,   6.9813,  -2.1581,  -1.9518],\n",
      "        [ -3.4272,   3.6364,  -7.9912,  -6.2906,  -7.5097,  -4.2853,  -8.6847,\n",
      "          -1.3136,   3.2627,  -1.4023,  -2.8571],\n",
      "        [ -2.7161,  -0.5021,  -6.4329,  -5.0642,  -5.5989,  -0.8760,  -5.4331,\n",
      "           1.0971,   4.0551,  -1.1686,  -1.8066],\n",
      "        [ -5.9558,   1.8798, -11.4181, -10.6071, -12.3895,  -6.3440, -11.2947,\n",
      "           0.7743,   3.5497,   1.9791,  -4.6460]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 5.298150539398193\n",
      "output: tensor([[ -1.5594,  -0.5868,  -5.5938,  -4.4335,  -5.7262,  -1.8280,  -5.7183,\n",
      "           0.6099,   1.8157,   1.9060,  -1.8314],\n",
      "        [ -5.5159,   2.7025, -12.1920, -12.4693, -11.1658,  -6.6597, -10.5904,\n",
      "           2.1245,   3.8483,  -2.5607,  -5.5946],\n",
      "        [ -2.4114,   2.0756,  -3.7033,  -3.9200,  -3.7359,  -2.2921,  -3.1898,\n",
      "           0.1203,   1.1953,   0.4714,  -1.7267],\n",
      "        [ -1.0741,  -0.6621,  -4.3268,  -3.2813,  -3.8810,  -1.6616,  -3.2057,\n",
      "           0.8375,   0.8570,  -0.1900,  -1.5739],\n",
      "        [ -7.0221,   2.5106, -27.3351, -33.1897, -26.6511, -22.5300, -27.2646,\n",
      "           5.7755,   0.9982,   7.6079, -12.5721],\n",
      "        [ -0.6817,   1.1861,  -3.4728,  -2.8189,  -4.3581,  -0.2322,  -2.4549,\n",
      "           0.6674,   1.2516,  -0.5307,  -1.3479],\n",
      "        [ -3.3810,  -1.1463,  -8.6150,  -5.5495,  -7.9617,  -6.2248,  -4.6339,\n",
      "           1.3507,   5.2143,   0.1834,  -2.0778],\n",
      "        [ -0.2144,   1.4196,  -9.3739,  -6.6520,  -9.3009,  -8.8547,  -8.0269,\n",
      "           1.9845,   2.8998,  -0.0679,  -3.9402]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.488680839538574\n",
      "output: tensor([[-3.8092e+00, -3.0193e+00, -1.0617e+01, -8.9106e+00, -1.3003e+01,\n",
      "         -7.0212e+00, -4.2469e+00,  4.1098e+00,  6.3208e-02, -1.7971e-01,\n",
      "         -2.7022e+00],\n",
      "        [-2.2996e+00,  1.3920e+00, -4.6404e+00, -3.9613e+00, -3.8892e+00,\n",
      "         -2.5748e+00, -2.8216e+00,  9.2880e-01,  4.4536e-01, -6.7426e-01,\n",
      "         -1.6750e+00],\n",
      "        [-1.3648e+01,  7.2485e+00, -2.2689e+01, -2.0312e+01, -2.5635e+01,\n",
      "         -1.9137e+01, -1.9752e+01,  1.4782e+01, -4.0332e-01, -5.6267e+00,\n",
      "         -6.7114e+00],\n",
      "        [-3.9270e+00, -4.2981e+00, -1.1914e+01, -8.6612e+00, -1.0678e+01,\n",
      "         -8.3696e+00, -6.5504e+00,  6.6566e+00, -3.3551e-01,  2.4344e+00,\n",
      "         -1.5290e+00],\n",
      "        [-4.4265e+00,  5.5545e+00, -1.1558e+01, -8.5957e+00, -1.2529e+01,\n",
      "         -5.1379e+00, -9.0773e+00,  3.6735e+00,  1.1622e+00, -9.0827e-01,\n",
      "         -4.5091e+00],\n",
      "        [-1.6398e+00, -4.4242e-02, -6.4424e+00, -5.6907e+00, -6.2057e+00,\n",
      "         -3.6590e+00, -5.0270e+00,  7.1206e-01,  1.3639e+00, -1.3394e-01,\n",
      "         -2.9474e+00],\n",
      "        [-4.8533e+00,  3.4143e+00, -9.9195e+00, -8.1023e+00, -7.8358e+00,\n",
      "         -5.6746e+00, -6.1925e+00,  1.2206e+00,  9.1903e-01, -7.7022e-02,\n",
      "         -3.5714e+00],\n",
      "        [-3.0327e+00,  4.0557e-01, -6.4544e+00, -3.9633e+00, -5.6505e+00,\n",
      "         -4.0736e+00, -3.5327e+00,  2.0036e+00,  1.0084e+00, -1.1490e-02,\n",
      "         -1.6399e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 3.826840400695801\n",
      "output: tensor([[ -4.1931,   0.4888,  -8.2786,  -7.2311,  -9.6382,  -3.8496,  -5.2779,\n",
      "           3.9869,  -2.4109,   0.6566,  -5.6954],\n",
      "        [ -5.6790,   3.4043, -10.1718, -10.1770, -10.4373,  -7.5668, -10.0835,\n",
      "           4.3215,  -0.5348,  -3.1106,  -7.2047],\n",
      "        [ -2.9516,  -1.0997,  -6.7518,  -7.1447,  -7.4747,  -3.0395,  -2.8398,\n",
      "           3.5775,  -0.3158,   0.9070,  -3.3986],\n",
      "        [ -0.9805,  -0.9577, -10.7675,  -8.2146, -10.1461,  -7.6673,  -4.7608,\n",
      "           1.2726,  -1.3697,   1.4243,  -3.9103],\n",
      "        [ -8.3225,  -0.1457, -13.6123, -14.2421, -14.8089,  -7.5465, -11.7691,\n",
      "           7.9710,  -1.6248,  -1.6409,  -9.4886],\n",
      "        [ -3.1638,   0.1911, -12.4087, -10.2263, -10.3884, -10.6032,  -7.7716,\n",
      "           2.7141,  -1.8157,   2.4859,  -3.8403],\n",
      "        [ -3.3368,  -3.1079, -11.8951, -10.0063, -12.2766,  -5.1554,  -1.5627,\n",
      "           3.7536,   2.4964,   3.4204,  -7.1340],\n",
      "        [ -3.8822,  -0.7689, -17.6388, -14.5251, -16.3804, -19.5609, -13.6796,\n",
      "           4.6682,   0.7099,  -1.2058,  -6.9815]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.351739883422852\n",
      "output: tensor([[ -2.1004,   3.2558, -10.4705,  -9.1315, -10.5806,  -6.3409,  -8.4608,\n",
      "           3.4949,  -0.4020,   0.1254,  -6.0717],\n",
      "        [ -3.5123,   1.2298,  -8.3708,  -6.6366,  -7.8528,  -2.4801,  -5.1983,\n",
      "           4.8442,  -1.8922,   0.2491,  -4.6024],\n",
      "        [ -3.2283,   0.2474,  -5.1977,  -4.5653,  -5.2292,  -1.0913,  -2.0664,\n",
      "           2.6051,  -1.2007,  -0.3817,  -2.7137],\n",
      "        [-12.2576,   1.2231, -32.8273, -30.7918, -33.7708, -30.1365, -25.9294,\n",
      "          12.2298,  -1.0263,  -4.2610, -12.9726],\n",
      "        [ -1.2803,   0.8919,  -6.7544,  -6.6458,  -6.6990,  -6.7476,  -3.9950,\n",
      "           2.9022,  -1.3223,   3.5597,  -5.0530],\n",
      "        [ -2.3970,  -0.0765,  -5.7365,  -5.5605,  -4.8399,  -3.0300,  -3.3799,\n",
      "           3.2806,  -1.8355,   0.5383,  -2.8977],\n",
      "        [ -2.9263,   1.6167,  -5.4513,  -4.9879,  -4.6483,  -3.1296,  -3.4978,\n",
      "           2.2622,  -1.5130,   0.9426,  -2.1367],\n",
      "        [ -7.0076,   0.8792, -17.5288, -12.8917, -14.9021,  -5.5067,  -6.8342,\n",
      "           5.1114,  -4.5666,   1.9309,  -1.7268]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.5801126956939697\n",
      "output: tensor([[ -4.1215,   0.9812,  -8.7489,  -8.4837,  -8.9189,  -4.1056,  -5.8314,\n",
      "           4.2866,  -0.4385,  -0.0872,  -2.9863],\n",
      "        [ -3.5524,   0.2807,  -7.9637,  -7.2952,  -7.5289,  -4.4379,  -5.7902,\n",
      "           3.3229,  -1.4540,  -0.2647,  -3.0928],\n",
      "        [ -4.7681,  -3.6035, -15.5248, -13.0114, -12.9338, -13.1748, -14.2790,\n",
      "           5.3312,  -3.6399,  -3.8958,  -5.8836],\n",
      "        [ -5.1466,   1.9525,  -7.1013,  -7.3903,  -7.6230,  -2.5594,  -5.6232,\n",
      "           3.4624,  -2.6398,   0.5912,  -4.8990],\n",
      "        [ -1.4349,   2.2910,  -4.0781,  -3.4725,  -3.6746,  -1.7946,  -1.3835,\n",
      "           0.7800,  -1.0487,   2.3998,  -0.7221],\n",
      "        [ -3.4625,   1.7505,  -6.6180,  -4.9201,  -6.1862,  -2.2886,  -2.8463,\n",
      "           2.5744,  -1.5569,   0.7511,  -2.5756],\n",
      "        [ -5.8290,   3.1969, -22.9541, -17.4971, -19.3475, -18.5015,  -4.1489,\n",
      "           6.9760, -13.3879,   2.9509, -11.9314],\n",
      "        [ -2.3782,   0.0481,  -5.1209,  -3.9164,  -4.3625,  -2.3731,  -3.0263,\n",
      "           2.5667,  -2.7556,   1.3118,  -1.2775]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.552053451538086\n",
      "output: tensor([[ -4.5872,  -1.1616,  -8.2619,  -5.3181,  -6.4486,  -3.3172,  -4.3713,\n",
      "           5.7572,  -2.7611,  -2.0819,  -2.7642],\n",
      "        [  1.6947,   0.2453, -24.0859, -18.9423, -23.3554, -23.2992, -10.4829,\n",
      "           9.6570,  -2.6876,  -3.2682,  -5.4112],\n",
      "        [ -1.4942,   2.3500,  -7.9715,  -7.3154,  -7.1461,  -4.6872,  -4.0906,\n",
      "           0.2125,  -1.9576,   2.7283,  -1.3740],\n",
      "        [ -3.3725,   0.8413, -11.4679,  -8.9382,  -9.7905,  -4.8432,  -2.2299,\n",
      "           7.3020,  -6.6065,   5.3842,   0.3787],\n",
      "        [ -2.1193,   0.8997,  -6.0212,  -5.3582,  -5.8319,  -3.6441,  -1.6394,\n",
      "           2.5242,  -4.2482,   2.1193,  -0.8286],\n",
      "        [ -4.7012,   3.0662,  -9.9902,  -8.6346, -10.6761,  -7.4587,  -7.7494,\n",
      "           2.1036,  -1.3851,   3.1500,  -6.7341],\n",
      "        [ -2.5906,   1.4614,  -8.2129,  -8.2348,  -8.3448,  -3.3957,  -5.6177,\n",
      "           3.5348,  -0.9955,   0.1189,  -3.3832],\n",
      "        [ -4.7319,   0.7874,  -8.3733,  -8.2938,  -6.3368,  -3.1253,  -3.0631,\n",
      "           4.6987,  -3.1970,   2.3583,  -0.5651]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 5.216226577758789\n",
      "output: tensor([[ -2.4692,   1.9944, -20.4512, -19.3610, -22.5312, -17.2449, -14.6182,\n",
      "           1.8292,  -1.6645,   1.3956,  -9.5861],\n",
      "        [ -5.3336,  -1.8471,  -9.4393,  -7.3618,  -8.1027,  -1.5813,  -6.0957,\n",
      "           3.6618,  -4.7851,   1.3901,  -2.3076],\n",
      "        [ -2.6041,   1.1597, -18.1806, -14.1630, -15.1628, -15.6956,  -8.5138,\n",
      "           1.0035,   1.0292,   0.5390,  -3.9561],\n",
      "        [ -2.2878,   0.7115,  -8.1973,  -6.7370,  -6.5813,  -4.5172,  -3.0915,\n",
      "           1.8514,  -5.0164,   6.1055,  -2.9748],\n",
      "        [ -2.6974,   1.2169, -10.6845,  -6.3687, -10.9126,  -4.1611,  -3.0096,\n",
      "           0.4904,  -5.5732,   6.6177,  -4.1521],\n",
      "        [ -4.2415,   0.1001,  -8.4263,  -6.2727,  -8.3779,  -3.0602,  -4.6471,\n",
      "           2.1601,  -0.9570,   3.4335,  -4.5516],\n",
      "        [ -4.1845,   3.4428, -11.0642, -11.7068, -13.1555,  -3.8728, -10.0276,\n",
      "           1.8362,  -2.8516,   3.8800,  -9.1711],\n",
      "        [ -1.2493,   2.7826,  -7.8303,  -5.5512,  -4.4797,  -3.7903,  -0.3803,\n",
      "           1.3178,  -6.2765,   6.0263,   0.6530]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 6.1672444343566895\n",
      "output: tensor([[ -2.5745,   1.5851,  -7.5216,  -6.3422,  -7.3067,  -3.5402,  -2.8373,\n",
      "           2.2682,  -1.3511,  -0.1832,  -1.7169],\n",
      "        [ -2.0469,  -2.0432,  -7.5092,  -7.1640,  -7.2777,  -4.1518,  -4.1188,\n",
      "           1.3270,  -5.9911,   5.2348,  -4.0921],\n",
      "        [ -0.9252,   4.4334, -14.3913,  -9.7894, -14.9593,  -7.4117,  -4.9452,\n",
      "          -6.5077,  -6.8263,   8.4198,  -2.0637],\n",
      "        [  4.4205,   2.6147, -14.2703,  -7.3176, -12.4853,  -8.5257,  -4.4797,\n",
      "          -3.4331,  -2.2366,   1.9195,  -1.6877],\n",
      "        [ -0.6391,   0.7780,  -6.1350,  -6.1373,  -9.4057,  -1.9689,  -4.8888,\n",
      "           2.1250,  -3.9557,   0.0666,  -6.3144],\n",
      "        [ -2.4910,  -0.0349,  -7.9543,  -7.3256,  -8.0320,  -1.9119,  -6.2696,\n",
      "           2.5937,  -1.4692,   1.7535,  -3.6385],\n",
      "        [ -2.0670,   3.6726, -10.9360,  -9.0574,  -9.1931,  -7.2033,  -6.2291,\n",
      "          -1.2706,  -0.8618,   3.1504,  -2.2286],\n",
      "        [ -0.6560,   3.3760, -12.2144,  -8.9927, -13.0836,  -8.2792,  -6.1808,\n",
      "          -2.8315,  -3.6427,   7.2156,  -5.0218]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.8588802814483643\n",
      "output: tensor([[-1.9112e-01,  1.0546e+00, -7.3236e+00, -2.4172e+00, -6.7309e+00,\n",
      "         -1.4125e-01, -6.4627e-01,  7.9858e-01, -7.2185e+00,  3.0070e+00,\n",
      "         -2.2304e-01],\n",
      "        [ 4.1522e+00,  2.1365e+00, -1.3800e+01, -9.5036e+00, -1.3166e+01,\n",
      "         -4.1393e+00, -7.8811e+00, -1.7333e+00, -1.2154e+00,  1.3612e+00,\n",
      "         -9.4026e-01],\n",
      "        [ 1.0764e-01,  3.3767e-01, -5.0534e+00, -4.5874e+00, -5.1343e+00,\n",
      "         -2.3890e-01, -3.7428e+00, -3.2708e-01, -1.6827e+00,  2.3928e+00,\n",
      "         -2.1609e+00],\n",
      "        [-1.8288e+00,  8.4611e+00, -1.6076e+01, -1.3099e+01, -1.6195e+01,\n",
      "         -1.0532e+01, -5.0008e+00, -4.1285e+00,  2.7006e-01,  2.1387e+00,\n",
      "          4.2003e+00],\n",
      "        [-1.8250e+00,  8.1765e-01, -7.3218e+00, -5.6979e+00, -8.2094e+00,\n",
      "         -5.4618e+00, -2.5882e+00,  1.1535e-02, -2.1870e+00,  3.9766e+00,\n",
      "          3.0357e-01],\n",
      "        [-1.9644e+00,  2.0479e-01, -6.7633e+00, -4.9330e+00, -7.8432e+00,\n",
      "         -1.0835e+00, -1.8684e+00,  9.4070e-01, -2.2542e+00,  8.8118e-01,\n",
      "         -2.5412e+00],\n",
      "        [-1.6536e+00,  8.0100e-02, -5.3650e+00, -4.4332e+00, -4.9073e+00,\n",
      "         -1.8193e+00, -2.9743e+00,  7.3357e-01, -2.5868e+00,  1.6858e+00,\n",
      "         -3.7449e+00],\n",
      "        [-1.6340e-02, -1.2149e+00, -7.3350e+00, -5.7922e+00, -6.9888e+00,\n",
      "         -3.2583e+00, -4.5968e+00,  1.0752e+00, -1.8529e+00,  8.7484e-01,\n",
      "         -3.8281e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 5.27177095413208\n",
      "output: tensor([[ -0.2553,   3.4578, -10.9851,  -6.3506,  -9.1903,  -6.2856,  -1.4840,\n",
      "          -3.9417,  -0.8197,   4.9589,   4.7538],\n",
      "        [  1.0757,   3.4888,  -5.5029,  -4.1033,  -6.0865,  -1.9788,  -1.3717,\n",
      "          -1.4478,  -3.2101,   3.7174,   0.4958],\n",
      "        [  2.0879,  -2.0634,  -4.5824,  -3.6464,  -7.3162,  -0.3523,  -3.7105,\n",
      "           0.2357,  -3.5832,  -0.2167,  -2.7402],\n",
      "        [ -1.6946,  -0.3738,  -2.9919,  -1.8971,  -4.2783,   0.7364,  -1.1590,\n",
      "           1.4328,  -2.7781,  -0.2023,   0.6059],\n",
      "        [ -1.0915,  -0.2359, -13.4366, -10.9117, -14.7288,  -2.2760,  -6.5541,\n",
      "          -4.7152,  -2.3129,   3.5991,  -3.4604],\n",
      "        [  0.6855,   3.2293,  -6.1250,  -6.5296,  -7.2974,  -3.7857,   3.0136,\n",
      "          -1.1027,  -5.5645,   6.9365,   1.6868],\n",
      "        [ -1.6504,   0.0532,  -8.4572,  -8.4677,  -8.9689,  -1.9298,  -8.5994,\n",
      "           0.0247,  -0.1462,  -1.3006,  -2.5925],\n",
      "        [  0.3062,   0.9705,  -2.2871,  -1.7455,  -1.7261,   0.3478,   1.0180,\n",
      "           0.1300,  -1.8804,   0.7123,  -0.5824]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.307300090789795\n",
      "output: tensor([[ -3.6712,   3.7556, -13.4206,  -9.2422, -12.3199,  -0.5753,  -3.9658,\n",
      "          -2.8250,  -3.3286,   0.0505,   2.9359],\n",
      "        [  2.2949,   0.4424,  -2.9789,  -3.0729,  -2.7684,  -0.3536,   3.6248,\n",
      "          -0.8734,  -3.6756,   1.1289,   3.7937],\n",
      "        [ -2.0174,   0.8876,  -6.2330,  -5.0748,  -6.3993,  -1.4218,   0.0465,\n",
      "          -0.0950,  -3.7723,   3.1035,   2.6336],\n",
      "        [  0.2087,  -0.7744,  -4.5330,  -7.2235,  -8.2716,  -0.1033,  -5.9121,\n",
      "          -1.4864,  -1.6363,  -0.2752,  -5.8437],\n",
      "        [ -3.2337,   0.3864, -12.7639,  -9.6862, -11.8585,  -1.7117,  -7.6661,\n",
      "          -4.6617,   1.1234,   2.0677,  -3.0498],\n",
      "        [  0.5234,   1.3976,  -4.2666,  -4.4288,  -8.0385,   3.3076,  -3.0365,\n",
      "           1.1036,  -1.7134,  -1.0770,  -3.8353],\n",
      "        [ -0.6663,  -0.8281,  -3.5119,  -3.4815,  -4.8524,   0.9939,  -2.0074,\n",
      "           0.3791,  -2.4123,  -1.3717,  -2.3129],\n",
      "        [  1.8068,   1.0695,  -4.2726,  -0.9116,  -4.7391,   1.5285,   0.6129,\n",
      "          -2.2095,  -1.9314,   0.1806,   0.4134]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.940491199493408\n",
      "output: tensor([[  0.2020,  -1.7407,  -2.1614,  -3.0579,  -4.0603,   1.7146,  -2.9972,\n",
      "           0.3129,  -1.8062,   0.3250,  -0.6619],\n",
      "        [  0.2225,  -0.4729,  -1.8542,  -0.6491,  -1.8624,   2.1206,   1.3779,\n",
      "           0.7336,  -1.4891,  -0.3426,   3.1374],\n",
      "        [  0.5027,  -0.5277,  -2.7564,  -1.7082,  -3.2357,   1.8359,  -3.6819,\n",
      "           0.7714,  -0.6660,  -0.9974,  -1.4100],\n",
      "        [  0.8854,   0.5970,  -3.0744,  -1.8839,  -2.5248,  -0.0727,   0.7148,\n",
      "          -0.5418,  -4.3087,  -2.5268,   4.4612],\n",
      "        [ -6.5690,  -4.0565, -17.7591, -16.2393, -19.6455,   1.2256,  -6.8613,\n",
      "           2.2045,   1.2191,  -3.3675,  -2.3645],\n",
      "        [ -5.0479,   0.5957,  -9.1615, -10.7955,  -9.7104,  -3.0207,  -3.0202,\n",
      "          -2.2658,   0.2316,  -0.3156,   2.8173],\n",
      "        [  0.7361,   0.1389,  -2.4934,  -3.2517,  -4.0309,   1.5345,  -2.4331,\n",
      "           2.0166,  -0.9471,  -1.9099,  -1.2652],\n",
      "        [ -2.0947,  -1.8642,  -5.7677,  -5.2497,  -8.2813,   3.7447,  -3.9391,\n",
      "           2.8818,  -0.6571,  -1.6838,  -1.4901]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.16062331199646\n",
      "output: tensor([[-4.3501e-01, -1.2240e-01, -2.1352e+00, -1.8449e+00, -8.3611e-01,\n",
      "          1.5039e+00,  8.0765e-01,  5.6719e-01, -1.0244e+00, -6.2601e-01,\n",
      "          1.6495e+00],\n",
      "        [-2.1792e+00, -2.2336e+00, -7.1682e+00, -6.8247e+00, -9.7563e+00,\n",
      "          5.5554e-01, -5.1012e+00,  4.8338e-01,  9.1935e-01, -2.3379e+00,\n",
      "         -2.1420e+00],\n",
      "        [-1.2032e+00, -2.6884e+00, -9.0281e-01, -1.6406e+00, -3.5286e+00,\n",
      "          2.8036e+00, -1.1852e+00,  1.0588e+00, -2.8308e+00, -1.6513e+00,\n",
      "         -2.7820e-01],\n",
      "        [ 2.6867e-01, -9.9771e-01, -2.2748e+00, -2.5710e+00, -2.0757e+00,\n",
      "          2.2450e+00, -1.9302e+00,  2.0435e+00, -2.4591e+00, -2.7637e+00,\n",
      "          1.2413e+00],\n",
      "        [-3.8043e+00, -2.5670e+00, -9.4436e+00, -8.8986e+00, -1.2311e+01,\n",
      "          2.6074e+00, -8.9144e+00, -3.8301e-01,  2.3856e+00, -6.5658e+00,\n",
      "         -6.8748e-01],\n",
      "        [-5.6581e-02, -1.9622e+00, -1.3485e+00, -2.0137e+00, -3.7291e+00,\n",
      "          1.6299e+00, -1.8442e+00,  1.5847e+00, -4.3560e-04, -1.6883e+00,\n",
      "         -2.5351e-01],\n",
      "        [-4.2629e+00, -1.9460e+00, -3.6172e+00, -5.1919e+00, -8.9317e+00,\n",
      "          2.2287e+00,  4.5943e-01, -2.8129e+00,  3.9482e+00, -1.9999e+00,\n",
      "          7.5895e+00],\n",
      "        [-4.9754e+00, -2.1272e+00, -5.8711e+00, -6.1620e+00, -6.1630e+00,\n",
      "          3.1567e+00, -5.4289e+00,  4.1450e+00, -6.1391e-01, -2.6580e+00,\n",
      "         -1.0942e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 3.2955968379974365\n",
      "output: tensor([[ -0.9363,  -0.2803,  -8.7814, -10.7719,  -9.1872,   0.1807,  -5.1295,\n",
      "          -0.5171,   4.0350,  -0.9722,  -4.1354],\n",
      "        [ -1.6310,  -3.4116,  -6.0745,  -6.0904,  -6.6467,   0.7611,  -7.0331,\n",
      "           2.3822,   0.9399,  -2.8209,  -1.7318],\n",
      "        [  2.0709,  -1.5210,   2.6001,  -3.1675,  -2.7049,   1.8836,   1.5291,\n",
      "           1.2016,  -0.3321,  -6.7592,   4.3779],\n",
      "        [ -0.2954,  -0.7043,  -1.5553,  -3.4775,  -3.0944,   1.3700,  -3.3776,\n",
      "           2.8380,  -1.4683,  -3.6375,  -1.4179],\n",
      "        [-13.3010,  -1.6670, -13.2310, -15.1052, -16.8311,  -4.0293, -14.0012,\n",
      "          -3.5038,   3.5723,  -5.5374,  -0.2066],\n",
      "        [  0.8964,  -0.3976,  -0.3261,  -1.0277,  -2.2752,   2.5433,  -2.3061,\n",
      "           0.9355,  -0.1112,  -1.8811,   0.7768],\n",
      "        [ -2.0616,  -2.6513,  -3.0203,  -5.2596,  -5.4667,   1.2888,  -4.3249,\n",
      "           3.8298,  -1.0560,  -3.9250,  -1.0651],\n",
      "        [ -1.3218,  -3.0713,  -3.2074,  -3.5741,  -4.0507,   0.8895,  -3.5458,\n",
      "           2.0490,   1.3546,  -2.5262,  -0.4449]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.825314521789551\n",
      "output: tensor([[-3.0905e-01, -5.4773e-01, -4.8001e+00, -7.3013e+00, -7.9807e+00,\n",
      "         -2.7831e+00, -6.4407e+00,  4.2736e+00, -1.4959e-01, -5.8468e+00,\n",
      "         -4.9749e-01],\n",
      "        [-8.4125e-01,  6.2593e-01, -1.8633e+00, -3.7544e+00, -3.7858e+00,\n",
      "         -1.7063e+00, -3.7469e+00, -2.1648e-01,  4.1012e+00, -3.9123e+00,\n",
      "          8.9859e-01],\n",
      "        [ 1.2333e+00, -2.1117e+00,  2.3947e+00, -1.0660e-01, -2.1910e+00,\n",
      "          4.8384e+00, -2.7252e-01,  6.9429e-01,  3.1856e+00, -3.4906e+00,\n",
      "          1.2265e-01],\n",
      "        [-5.3870e-01, -1.2436e+00, -1.2796e+00, -5.7760e+00, -4.8069e+00,\n",
      "         -7.2806e-01, -8.1008e+00,  5.3587e+00, -3.4380e-03, -4.7263e+00,\n",
      "         -1.5223e+00],\n",
      "        [ 3.9762e-01, -4.1619e+00, -1.9034e+00, -5.1106e+00, -6.6134e+00,\n",
      "          1.8259e+00, -6.5568e+00,  2.2328e+00, -5.8445e-02, -4.3393e+00,\n",
      "          3.8481e-01],\n",
      "        [-4.8667e+00, -3.2066e+00, -9.7739e+00, -7.5459e+00, -1.0993e+01,\n",
      "          1.4208e+00, -5.3843e+00,  7.5290e-01,  4.4496e+00, -6.4273e+00,\n",
      "          4.6489e+00],\n",
      "        [-5.3963e-01, -3.1132e-01, -3.1370e+00, -5.7247e+00, -5.8928e+00,\n",
      "          2.3018e+00, -5.6555e+00,  3.2933e+00,  1.8246e-01, -4.9888e+00,\n",
      "         -1.0661e+00],\n",
      "        [-1.0705e+01, -4.7384e+00, -7.3636e+00, -1.1112e+01, -1.2077e+01,\n",
      "         -8.2840e-02, -9.7186e+00, -7.7725e-02,  5.4183e+00, -1.8812e+00,\n",
      "          4.2051e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 5.377538681030273\n",
      "output: tensor([[ -0.4330,   1.3575,  -4.4380,  -5.6821,  -5.2636,  -2.8330,  -3.9967,\n",
      "           0.6377,   0.9208,  -1.2277,  -0.6266],\n",
      "        [ -1.8485,  -0.4451,  -1.4023,  -2.8913,  -4.1422,  -0.6743,  -4.1225,\n",
      "           3.7217,   1.9071,  -4.4610,  -1.2254],\n",
      "        [ -2.1758,  -0.4199,  -7.3508, -13.0074, -11.1146,  -1.9880,  -9.6616,\n",
      "           3.1815,   5.2703,  -6.3600,  -2.2728],\n",
      "        [ -1.1904,  -1.6927,  -4.0008,  -5.3491,  -6.3477,  -0.7879,  -6.7795,\n",
      "           1.6340,   1.1231,  -1.7891,  -1.1913],\n",
      "        [ -6.8905,  -4.1437, -18.6447, -21.3232, -24.7164,   4.0961, -18.5740,\n",
      "           2.2673,   8.3932,  -4.1907,  -2.0076],\n",
      "        [  0.3473,  -0.3076,  -0.7397,  -2.6117,  -4.7016,  -1.2683,  -3.4124,\n",
      "           1.6525,   0.8124,  -4.3534,  -2.7192],\n",
      "        [ -2.4273,  -0.8875,  -5.7723,  -6.4449,  -8.1450,  -2.7642,  -7.9802,\n",
      "           2.0612,   0.5907,  -1.8340,  -0.7256],\n",
      "        [  2.2569,   1.2690,  -1.1782,  -4.2022,  -3.1484,  -0.3846,  -2.0774,\n",
      "           0.3965,   0.8728,  -2.8086,  -0.8309]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.897860050201416\n",
      "output: tensor([[ -1.8598,   0.4523,  -5.1800,  -6.9187,  -7.2774,  -4.3484,  -7.5529,\n",
      "           1.3309,  -0.2100,  -1.3752,  -1.5549],\n",
      "        [  0.8498,  -1.3953,  -0.7601,  -3.0194,  -1.6474,  -1.8652,  -2.9126,\n",
      "          -0.3148,   2.9893,  -2.3171,  -0.6601],\n",
      "        [ -0.4313,   0.2782,  -2.4014,  -3.6757,  -3.5411,  -1.7941,  -3.1616,\n",
      "           1.3298,   0.8551,  -2.4975,  -0.5811],\n",
      "        [ -0.5784,   0.6271,  -1.6443,  -2.9299,  -1.8279,  -1.1207,  -3.5372,\n",
      "           0.2145,   0.6568,  -0.9300,  -0.9840],\n",
      "        [ -0.2106,   1.1744,  -1.2020,  -3.1240,  -2.5682,  -0.5671,  -2.2156,\n",
      "           0.4769,   0.5601,  -1.2882,  -0.7177],\n",
      "        [ -3.3620,  -3.7615,  -4.0376,  -6.2746,  -7.3730,  -1.6928,  -6.3774,\n",
      "           4.6424,   1.2752,  -4.6608,  -2.3398],\n",
      "        [-14.5050,  -4.1603, -19.5270, -23.3542, -31.0512,  -2.1852, -17.2976,\n",
      "           4.4356,   4.7863,  -3.4526,  -4.5553],\n",
      "        [ -1.0751,  -0.5922,  -0.8803,  -8.2571,  -6.7620,  -1.0626,  -7.0960,\n",
      "           4.1671,   4.9358,  -3.5021,  -4.3150]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 1.2734102010726929\n",
      "output: tensor([[-1.6954e+00,  2.0301e-01, -2.6731e+00, -4.7388e+00, -5.4498e+00,\n",
      "         -4.4432e+00, -5.5801e+00,  1.1507e+00,  2.2763e+00, -2.4740e+00,\n",
      "         -1.1130e+00],\n",
      "        [-2.0683e+00,  1.2221e+00, -6.2630e-01, -8.2015e+00, -5.3465e+00,\n",
      "         -1.7667e+00, -1.1826e+01,  6.7927e+00,  2.8136e+00, -5.2138e+00,\n",
      "         -2.9747e+00],\n",
      "        [-3.0352e-02,  2.3700e-02,  1.0138e+00, -4.1353e+00, -3.6362e+00,\n",
      "         -2.9871e+00, -4.4130e+00,  3.0007e+00,  2.2767e+00, -2.7517e+00,\n",
      "         -1.7468e+00],\n",
      "        [-6.9450e+00,  2.2970e-01, -1.6149e+01, -2.8669e+01, -2.5880e+01,\n",
      "          2.4474e+00, -2.4712e+01,  6.4671e+00, -1.5005e+00, -5.6469e+00,\n",
      "         -4.4166e+00],\n",
      "        [-1.4426e+00, -2.2637e+00, -4.3285e+00, -7.0393e+00, -7.3531e+00,\n",
      "         -3.9926e+00, -6.8277e+00,  3.4503e+00,  1.1269e+00, -2.3317e+00,\n",
      "         -1.4780e+00],\n",
      "        [-9.2684e-02, -6.2571e-01, -6.7714e-01, -4.7925e+00, -4.7613e+00,\n",
      "         -4.1337e+00, -5.6618e+00,  3.2325e+00,  2.7899e+00, -3.7019e+00,\n",
      "         -2.1583e+00],\n",
      "        [-9.2946e-01,  1.1666e+00, -4.9972e+00, -5.0587e+00, -5.8098e+00,\n",
      "         -3.5256e+00, -3.8509e+00,  2.2459e-02,  8.4446e-01, -1.4276e+00,\n",
      "         -1.1340e+00],\n",
      "        [-1.2303e+00, -1.4666e-01, -3.0773e+00, -5.1209e+00, -5.3817e+00,\n",
      "         -2.9696e+00, -4.9315e+00,  1.1879e+00,  7.1862e-01, -1.8009e+00,\n",
      "         -8.5184e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 2.3836710453033447\n",
      "output: tensor([[ -2.4379,  -0.1821,  -4.2826,  -7.5407,  -9.6265,  -6.8439,  -7.6063,\n",
      "           3.4745,   1.9526,  -3.0706,  -4.0531],\n",
      "        [ -8.8363,  -0.7792,  -1.3119, -12.8940, -11.4118,   1.0329,  -9.6887,\n",
      "           3.6048,   0.5118,   0.1095,  -2.7502],\n",
      "        [ -2.5237,  -0.3493,  -1.2935,  -5.7090,  -6.5028,  -6.7121,  -6.6423,\n",
      "           2.7359,   1.0930,  -3.6346,  -1.7850],\n",
      "        [ -2.1370,   0.5856,  -4.3635, -10.1011,  -8.6392,  -4.8980,  -9.2316,\n",
      "           3.7407,   2.1220,  -4.4343,  -0.5562],\n",
      "        [ -5.8987,  -0.3173,  -3.6958,  -9.1375, -10.3648,  -2.4471, -11.4171,\n",
      "           5.5764,   3.8802,  -8.0444,  -6.5005],\n",
      "        [ -5.2451,  -1.8960,  -6.2234, -12.9562, -13.8164,   0.5321,  -9.2161,\n",
      "           2.6609,   0.1727,  -1.0811,   0.0891],\n",
      "        [ -2.1797,   1.3537,  -4.7647,  -5.8161,  -5.8379,  -1.3977,  -4.2838,\n",
      "           2.4756,   0.5706,  -1.5669,  -0.9222],\n",
      "        [ -2.3572,  -2.5134,  -1.9840,  -5.2384,  -6.7117,  -1.3176,  -7.3974,\n",
      "           5.2603,   1.4428,  -3.8073,  -3.0485]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.390674114227295\n",
      "output: tensor([[-2.4435e+00,  1.3639e+00, -3.9809e+00, -1.2889e+01, -1.2091e+01,\n",
      "         -1.5096e+00, -9.7247e+00,  3.8278e+00,  1.8701e+00,  2.2449e-01,\n",
      "         -8.5568e+00],\n",
      "        [-7.4901e+00,  1.9042e+00, -1.0897e+01, -1.6875e+01, -1.7625e+01,\n",
      "         -1.2581e+01, -1.5558e+01,  4.1198e+00,  1.3210e+00, -3.5187e+00,\n",
      "         -2.4089e+00],\n",
      "        [-2.4730e+00,  7.3635e-01, -1.0482e+00, -7.7255e+00, -8.6671e+00,\n",
      "         -7.1781e+00, -7.9995e+00,  2.5958e+00,  1.0408e+00, -3.1704e+00,\n",
      "         -3.3706e+00],\n",
      "        [-4.4418e+00,  1.3910e+00,  2.3847e+00, -6.6263e+00, -6.3059e+00,\n",
      "         -3.8450e+00, -7.5373e+00,  2.1403e+00, -8.1899e-02, -3.4445e+00,\n",
      "         -1.1591e+00],\n",
      "        [-5.8548e+00, -1.6214e+00, -3.6583e+00, -1.0552e+01, -9.8119e+00,\n",
      "         -3.5241e+00, -1.0035e+01,  2.8447e+00,  1.6684e+00, -1.6420e+00,\n",
      "         -2.4129e-01],\n",
      "        [-2.6551e+00,  1.7882e-01, -8.2171e-02, -3.4921e+00, -4.2336e+00,\n",
      "         -1.6761e+00, -3.6216e+00,  3.7192e+00,  7.7122e-01, -1.7066e+00,\n",
      "         -2.9735e+00],\n",
      "        [-2.7822e+00,  2.1339e+00, -2.3709e+00, -5.2104e+00, -6.1634e+00,\n",
      "         -2.5538e+00, -4.5492e+00,  1.5381e+00,  8.7435e-03, -1.5942e+00,\n",
      "         -2.1903e+00],\n",
      "        [-1.0488e+00,  2.0360e+00, -3.0462e+00, -5.7975e+00, -5.0506e+00,\n",
      "         -3.1027e+00, -5.8632e+00,  1.0339e+00,  2.7349e-01, -2.1216e+00,\n",
      "         -1.7178e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 2.0626418590545654\n",
      "output: tensor([[ -4.3809,   0.9983,  -6.0845, -10.1013, -10.4656,  -8.6461, -10.3050,\n",
      "           1.2876,   1.6822,  -1.1427,  -1.0156],\n",
      "        [ -4.2832,   1.1091,  -0.0444,  -4.9228,  -3.6934,  -3.6599,  -6.7261,\n",
      "           4.9005,  -0.0803,  -2.0535,  -4.9579],\n",
      "        [ -2.1801,   2.0033,  -3.9979,  -7.2755,  -8.9958,  -6.4755,  -6.2850,\n",
      "           0.7062,   1.3035,  -1.2634,  -1.8094],\n",
      "        [ -1.8184,   0.5921,  -3.0332,  -7.2566,  -8.1497,  -4.4148,  -7.8102,\n",
      "           4.1721,   0.4368,  -3.7317,  -5.2607],\n",
      "        [ -1.6769,   1.9035,   0.4658,  -7.0183,  -5.9773,  -3.2924,  -7.7309,\n",
      "           2.9930,   1.7139,  -2.8035,  -4.5345],\n",
      "        [ -6.9518,   3.2711,  -0.7519,  -7.4791,  -9.2820,   0.6653,  -8.4484,\n",
      "           3.7774,  -2.0468,   3.5487,  -4.8180],\n",
      "        [ -5.4127,   2.8534, -13.9742, -20.3178, -18.4834,  -4.4023, -21.6230,\n",
      "           1.1330,   0.5538,   2.8634,  -5.6826],\n",
      "        [ -3.9598,   3.0445,  -0.5909,  -8.0819,  -6.2122,  -5.3128,  -7.8845,\n",
      "          -0.1338,   1.5083,  -1.8048,  -1.7940]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.217262029647827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: Error detected in CudnnRnnBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/traitlets/config/application.py\", line 976, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2936, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3135, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2114612/497437324.py\", line 4, in <cell line: 3>\n",
      "    running_epochs(model,optimizer,criterion)\n",
      "  File \"/tmp/ipykernel_2114612/2446513367.py\", line 47, in running_epochs\n",
      "    train(model,optimizer,criterion)\n",
      "  File \"/tmp/ipykernel_2114612/2446513367.py\", line 17, in train\n",
      "    out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tmp/ipykernel_2114612/2564363434.py\", line 119, in forward\n",
      "    x = self.conv1(x, edge_index)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/dauin_user/lcolomba/projects/logml22-latent-graph-learning/models/gcn.py\", line 135, in forward\n",
      "    _, (x, _) = self.lin(x)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/torch/nn/modules/rnn.py\", line 769, in forward\n",
      "    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " (Triggered internally at  ../torch/csrc/autograd/python_anomaly_mode.cpp:102.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'CudnnRnnBackward0' returned nan values in its 1th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model, optimizer, criterion \u001b[38;5;241m=\u001b[39m set_model_parameters(model4, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mdetect_anomaly(): \u001b[38;5;66;03m# used for debugging\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mrunning_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mrunning_epochs\u001b[0;34m(model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrunning_epochs\u001b[39m(model,optimizer,criterion):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 47\u001b[0m         \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m         train_acc \u001b[38;5;241m=\u001b[39m test(train_loader, model)\n\u001b[1;32m     49\u001b[0m         test_acc \u001b[38;5;241m=\u001b[39m test(test_loader, model)\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)  \u001b[38;5;66;03m# Compute the loss.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Derive gradients.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update parameters based on gradients.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pyg2/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pyg2/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'CudnnRnnBackward0' returned nan values in its 1th output."
     ]
    }
   ],
   "source": [
    "# Experiment 4: GCN Baseline \n",
    "model, optimizer, criterion = set_model_parameters(model4, lr=0.001)\n",
    "with torch.autograd.detect_anomaly(): # used for debugging\n",
    "    running_epochs(model,optimizer,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchGeometric",
   "language": "python",
   "name": "pyg2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8cb2cf009b56e4f45028220f93cf800117e77713dd4472a875537ab4fe70ac3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
