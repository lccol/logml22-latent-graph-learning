{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from models.gat_transformer import GATConvTransformer, GATConvLSTM\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pathlib import Path\n",
    "from torch_geometric.data import Batch\n",
    "from models.gcn import GCNConvLSTM\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_index(node_num=12):\n",
    "    src_nodes = [idx for idx in range(node_num) for _ in range(node_num)]\n",
    "    tgt_nodes = [idx for _ in range(node_num) for idx in range(node_num)]\n",
    "    edge_index = torch.tensor(np.array([src_nodes, tgt_nodes]), dtype=torch.long)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_df = pd.read_excel('data/Diagnostics.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(diagnostics_df.Rhythm.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AF', 'AFIB', 'AT', 'AVNRT', 'AVRT', 'SA', 'SAAWR', 'SB', 'SR',\n",
       "       'ST', 'SVT'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics_df['label'] = le.transform(diagnostics_df.Rhythm.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         7\n",
       "2         5\n",
       "3         7\n",
       "4         0\n",
       "         ..\n",
       "10641    10\n",
       "10642    10\n",
       "10643    10\n",
       "10644    10\n",
       "10645    10\n",
       "Name: label, Length: 10646, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnostics_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7     3889\n",
       "8     1826\n",
       "1     1780\n",
       "9     1568\n",
       "10     587\n",
       "0      445\n",
       "5      399\n",
       "2      121\n",
       "3       16\n",
       "4        8\n",
       "6        7\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnostics_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "575it [00:18, 29.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1108it [00:36, 29.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [00:48, 34.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2093it [01:05, 32.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2710it [01:22, 36.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2855it [01:26, 40.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3083it [01:32, 41.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3608it [01:46, 41.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5055it [02:25, 39.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5151it [02:27, 36.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5743it [02:41, 41.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ECGDataDenoised/MUSE_20180113_124215_52000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5833it [02:44, 40.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6458it [02:58, 40.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7930it [03:32, 32.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8522it [03:45, 49.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8794it [03:51, 51.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10374it [04:26, 36.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10419it [04:27, 46.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found NaN! skipping file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10646it [04:32, 39.09it/s]\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "data_path = Path('data', 'ECGDataDenoised')\n",
    "for file_name in tqdm(data_path.iterdir()):\n",
    "    if file_name.suffix == '.csv':\n",
    "        sub_name = file_name.stem\n",
    "        y = torch.tensor(diagnostics_df.loc[diagnostics_df.FileName == sub_name].label.values[0])\n",
    "        df = pd.read_csv(file_name, header=None)\n",
    "        if len(df) != 5000:\n",
    "            print(file_name)\n",
    "            continue\n",
    "        x = torch.Tensor(np.array([df[col].values for col in df.columns]))\n",
    "        if x.isnan().any():\n",
    "            print('found NaN! skipping file...')\n",
    "            continue\n",
    "        edge_index = get_edge_index(node_num=12)\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data_list[:8000], batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(data_list[8000:], batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[96, 5000], edge_index=[2, 1152], y=[8], batch=[96], ptr=[9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = torch_geometric.utils.to_networkx(instance, to_undirected=True)\n",
    "#nx.draw(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channels = 250\n",
    "device = 'cpu' if not torch.cuda.is_available() else 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(-1, 250)\n",
      "  (bn1): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): GCNConv(250, 250)\n",
      "  (bn2): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): GCNConv(250, 250)\n",
      "  (lin): Linear(in_features=250, out_features=11, bias=True)\n",
      ")\n",
      "GAT(\n",
      "  (conv1): GATConv(-1, 250, heads=1)\n",
      "  (bn1): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): GATConv(250, 250, heads=1)\n",
      "  (bn2): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): GATConv(250, 250, heads=1)\n",
      "  (lin): Linear(in_features=250, out_features=11, bias=True)\n",
      ")\n",
      "GNN(\n",
      "  (conv1): GraphConv(-1, 250)\n",
      "  (conv2): GraphConv(250, 250)\n",
      "  (conv3): GraphConv(250, 250)\n",
      "  (lin): Linear(in_features=250, out_features=11, bias=True)\n",
      ")\n",
      "GCNLSTM(\n",
      "  (conv1): GCNConvLSTM(1, 250)\n",
      "  (bn1): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): GCNConv(250, 250)\n",
      "  (bn2): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): GCNConv(250, 250)\n",
      "  (lin): Linear(in_features=250, out_features=11, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Training Graph Neural Networks for Graph Classification\n",
    "# Embed each node by performing multiple rounds of message passing\n",
    "# Aggregate node embeddings into a unified graph embedding (readout layer)\n",
    "# Train a final classifier on the graph embedding\n",
    " \n",
    "# There exists multiple readout layers in literature, but the most common one is to simply take the average of node embeddings\n",
    "\n",
    "# For ex: In GCNConv we use the  ReLU(𝑥)=max(𝑥,0)  activation for obtaining localized node embeddings,\n",
    "# before we apply our final classifier on top of a graph readout layer.\n",
    "\n",
    "# PyTorch Geometric provides this functionality via torch_geometric.nn.global_mean_pool, \n",
    "# which takes in the node embeddings of all nodes in the mini-batch and the assignment vector batch \n",
    "# to compute a graph embedding of size [batch_size, hidden_channels] for each graph in the batch.\n",
    "\n",
    "# The final architecture for applying GNNs to the task of graph classification then looks as follows and allows for complete end-to-end training:\n",
    "\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import GraphConv\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GraphConv(-1, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, len(le.classes_))\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(-1, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, len(le.classes_))\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GAT, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(-1, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, len(le.classes_))\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class GCNLSTM(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNLSTM, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConvLSTM(1, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, len(le.classes_))\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "       \n",
    "# Define the models\n",
    "model1 = GCN(hidden_channels=hidden_channels).to(device)\n",
    "model2= GAT(hidden_channels=hidden_channels).to(device)\n",
    "model3 = GNN(hidden_channels=hidden_channels).to(device)\n",
    "model4 = GCNLSTM(hidden_channels=hidden_channels).to(device)\n",
    "\n",
    "# Print them \n",
    "print(model1)\n",
    "print(model2)\n",
    "print(model3)\n",
    "print(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model paramters and model type\n",
    "def set_model_parameters(model_type, lr=0.01):\n",
    "    model = model_type\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "# Train the model\n",
    "def train(model, optimizer,criterion):\n",
    "    model.train()\n",
    "\n",
    "    for idx, data in enumerate(train_loader):  # Iterate in batches over the training dataset.\n",
    "        data.x = data.x.to(device)\n",
    "        data.y = data.y.to(device)\n",
    "        data.edge_index = data.edge_index.to(device)\n",
    "        data.batch = data.batch.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        print(f'output: {out}')\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        print(f'loss: {loss}')\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        \n",
    "        if idx % 500 == 0:\n",
    "            print(f'{idx}tr) loss: {loss}')\n",
    "    return\n",
    "\n",
    "# Test the model \n",
    "def test(loader, model):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        data.x = data.x.to(device)\n",
    "        data.y = data.y.to(device)\n",
    "        data.edge_index = data.edge_index.to(device)\n",
    "        data.batch = data.batch.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "# Training and Testing Pipeline \n",
    "def running_epochs(model,optimizer,criterion):\n",
    "    for epoch in range(1, 10):\n",
    "        train(model,optimizer,criterion)\n",
    "        train_acc = test(train_loader, model)\n",
    "        test_acc = test(test_loader, model)\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2114612/1196594096.py:3: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0tr) loss: 1.2389345169067383\n",
      "500tr) loss: 2.163346767425537\n",
      "Epoch: 001, Train Acc: 0.4736, Test Acc: 0.4627\n",
      "0tr) loss: 1.745064377784729\n",
      "500tr) loss: 2.2241251468658447\n",
      "Epoch: 002, Train Acc: 0.5139, Test Acc: 0.4886\n",
      "0tr) loss: 1.5111541748046875\n",
      "500tr) loss: 1.2976725101470947\n",
      "Epoch: 003, Train Acc: 0.5221, Test Acc: 0.4810\n",
      "0tr) loss: 1.2629868984222412\n",
      "500tr) loss: 1.319043755531311\n",
      "Epoch: 004, Train Acc: 0.5679, Test Acc: 0.5183\n",
      "0tr) loss: 1.5851362943649292\n",
      "500tr) loss: 1.4444528818130493\n",
      "Epoch: 005, Train Acc: 0.5198, Test Acc: 0.4817\n",
      "0tr) loss: 1.2885963916778564\n",
      "500tr) loss: 1.0512919425964355\n",
      "Epoch: 006, Train Acc: 0.5939, Test Acc: 0.5209\n",
      "0tr) loss: 2.2602334022521973\n",
      "500tr) loss: 1.5723509788513184\n",
      "Epoch: 007, Train Acc: 0.5821, Test Acc: 0.5065\n",
      "0tr) loss: 1.2063523530960083\n",
      "500tr) loss: 1.6647557020187378\n",
      "Epoch: 008, Train Acc: 0.5885, Test Acc: 0.4973\n",
      "0tr) loss: 0.8360915780067444\n",
      "500tr) loss: 1.4548461437225342\n",
      "Epoch: 009, Train Acc: 0.5831, Test Acc: 0.5068\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: GCN Baseline \n",
    "model, optimizer, criterion = set_model_parameters(model1, lr=0.001)\n",
    "# with torch.autograd.detect_anomaly(): # used for debugging\n",
    "running_epochs(model,optimizer,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2114612/497437324.py:3: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly(): # used for debugging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[ 0.1708, -0.4179, -0.6674,  0.2599, -0.2027,  0.7780,  0.3570, -0.2831,\n",
      "          0.0384,  0.1920,  0.1012],\n",
      "        [-0.3191, -0.0261,  0.5773, -0.0172, -0.1099, -0.2526, -0.1489,  0.3435,\n",
      "          0.6470, -0.3529, -0.3762],\n",
      "        [-0.0315,  0.7417,  0.2363,  0.0673,  0.1925, -1.5089, -1.1498,  0.3531,\n",
      "          0.6929,  0.0982, -0.4312],\n",
      "        [-0.8694, -0.1196,  0.0140, -0.1988, -0.2810,  0.1909,  0.1903,  0.1838,\n",
      "          0.4722, -0.5285, -0.2669],\n",
      "        [-0.7587,  0.3654,  0.3210,  0.7196,  0.0293,  0.1648,  0.1539, -0.7173,\n",
      "          0.8345,  0.0129,  0.2239],\n",
      "        [ 0.4746, -0.0060, -0.1980, -0.0292,  0.1588,  0.2983,  1.3400,  0.7774,\n",
      "          0.1064, -1.3597, -0.2671],\n",
      "        [ 0.7806, -0.1176,  0.3090, -0.3527,  0.4529,  0.5664,  0.7344,  1.3921,\n",
      "          1.2538, -1.0327,  0.3247],\n",
      "        [ 0.3681,  1.1594,  0.1511, -0.0604,  0.4586,  0.5777,  0.7259, -0.7239,\n",
      "          0.4492, -1.7603, -0.0696]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.630655527114868\n",
      "0tr) loss: 2.630655527114868\n",
      "output: tensor([[ 1.1864, -0.8879, -1.1718, -0.4805, -1.1088, -1.2909, -0.2356,  5.1971,\n",
      "          1.1964,  1.1478, -2.0329],\n",
      "        [-0.1393,  0.3681, -0.6674, -0.1038, -0.9306, -1.1600, -0.9459,  3.4378,\n",
      "          2.0192,  0.6422, -1.4339],\n",
      "        [-1.8785, -0.7894, -1.0749, -0.9465, -0.9747, -0.4489, -1.2324,  4.5099,\n",
      "          1.5670,  0.0613, -1.9036],\n",
      "        [ 1.4801, -0.8205, -0.3079, -1.1745, -1.6714, -1.8638, -0.7841,  3.3743,\n",
      "          0.4608,  1.8997, -1.9764],\n",
      "        [-0.1059,  0.4917, -1.2034, -0.8434, -0.3404, -0.0918, -0.4006,  3.3513,\n",
      "          0.2161,  0.2009, -1.1005],\n",
      "        [-1.1014, -2.1091, -3.0250, -0.9355, -4.7451, -2.8854, -2.6753,  7.0892,\n",
      "          4.9792, -2.2612, -0.7107],\n",
      "        [ 1.0435,  0.2696, -0.8607, -2.3049, -2.1067, -1.4346,  0.1844,  5.3693,\n",
      "          0.4722,  0.3293, -2.1410],\n",
      "        [ 0.5315,  1.5111, -2.0939, -3.2822, -2.5786, -0.5448, -0.6380,  6.1979,\n",
      "          0.9611,  0.7616, -2.0831]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.1933207511901855\n",
      "output: tensor([[-3.2722e-01,  3.8671e+00, -1.8372e+00, -3.5029e-01, -5.2376e-01,\n",
      "         -2.5683e+00, -1.4205e+00,  2.4730e+00,  1.3045e+00, -3.0609e-01,\n",
      "         -2.6005e+00],\n",
      "        [-9.2943e-01,  2.0420e+00, -3.2693e+00, -8.2515e-01, -1.6406e+00,\n",
      "         -1.7660e+00, -1.4712e+00,  3.2657e+00, -6.5217e-01,  2.1621e+00,\n",
      "         -2.6528e+00],\n",
      "        [-7.9498e-01,  3.2742e+00, -6.5792e-01, -2.2383e+00, -2.1838e+00,\n",
      "         -3.4675e+00, -1.6839e+00,  1.6375e+00, -2.4321e-01, -2.9366e-01,\n",
      "         -3.6916e+00],\n",
      "        [-3.4992e+00,  3.3843e+00, -7.1302e+00, -6.1350e+00, -8.4663e+00,\n",
      "         -4.6956e+00, -4.8150e+00,  1.1935e+01,  7.9913e+00,  1.9000e-01,\n",
      "         -2.7438e+00],\n",
      "        [-2.9921e-01, -1.2596e+00, -2.6001e+00, -2.1778e+00, -3.1408e+00,\n",
      "         -8.5542e-01, -2.5633e+00,  5.4649e+00,  1.5699e+00,  1.5186e+00,\n",
      "         -2.3864e+00],\n",
      "        [-1.6706e+00,  2.5939e+00, -1.4948e+00, -2.3417e+00, -3.0479e+00,\n",
      "         -1.5318e+00, -2.2511e+00,  6.0707e+00,  1.3335e+00,  1.3656e+00,\n",
      "         -2.3848e+00],\n",
      "        [-8.1412e-01, -1.0121e-01, -8.0671e-01, -1.5587e+00, -2.8221e+00,\n",
      "         -1.3556e+00, -2.2835e+00,  2.5928e+00,  1.9340e+00,  4.2640e-01,\n",
      "         -1.3533e+00],\n",
      "        [-1.1864e-02,  2.4807e+00, -1.4059e+00, -1.1843e+00, -1.0061e+00,\n",
      "         -1.7560e+00, -4.2017e-01,  1.3941e+00, -2.1693e-01,  5.8237e-01,\n",
      "         -1.9689e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 0.8269354104995728\n",
      "output: tensor([[-1.2201,  1.1220, -2.4228, -2.2193, -2.6656, -1.4780, -1.7742,  4.4733,\n",
      "          4.1555,  2.2452, -2.3503],\n",
      "        [-5.4242,  0.1693, -7.2619, -0.8666, -8.5871, -4.3932, -5.5921,  9.1747,\n",
      "         11.3136,  2.0983, -7.0911],\n",
      "        [-0.4156,  5.8436, -0.5565, -0.7969, -1.4843, -2.4459, -1.0304,  0.7780,\n",
      "          2.5261,  1.2821, -2.5461],\n",
      "        [-2.6006,  1.3154, -1.8610, -1.5569, -3.1576, -1.4207, -1.5397,  5.1111,\n",
      "          2.5195,  1.0218, -2.5174],\n",
      "        [ 0.2486,  4.7073, -1.0732,  0.3132, -2.8314, -2.3282, -2.2439, -0.8853,\n",
      "          3.8239,  0.1025, -2.1587],\n",
      "        [-3.5570,  2.5425, -3.9450, -2.3546, -4.8331, -3.3416, -4.4754,  5.8527,\n",
      "          4.2798,  1.7016, -4.1327],\n",
      "        [-2.4968,  2.2073, -3.3664, -1.5348, -4.1716, -2.3247, -4.0732,  6.5919,\n",
      "          5.4821,  1.3275, -3.5130],\n",
      "        [-1.2945,  6.4024, -2.9987, -3.0804, -2.2334, -1.4827, -1.5585,  0.7856,\n",
      "          3.4507,  0.6361, -3.1421]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.850243091583252\n",
      "output: tensor([[-4.0897,  3.6658, -3.9356, -4.2641, -5.4462, -4.0326, -3.8917,  5.3116,\n",
      "          2.5008,  2.8345, -2.0866],\n",
      "        [-3.0013,  4.8166, -3.0040, -2.2657, -4.1813, -3.7802, -3.9511, -1.2885,\n",
      "          8.0093,  3.6710, -1.2515],\n",
      "        [-1.2732,  2.5569, -1.4177, -1.5857, -2.7718, -1.3590, -3.2342, -2.2685,\n",
      "          4.7496,  1.5115, -1.1740],\n",
      "        [-4.1289,  2.3535, -3.2685, -3.4524, -3.8065, -2.6958, -4.1424,  3.5480,\n",
      "          5.1080,  1.5358, -1.9769],\n",
      "        [-3.9431,  1.2262, -4.9856, -5.5691, -5.6879, -4.9296, -6.2600,  5.7149,\n",
      "          8.4228, -1.2369, -1.9675],\n",
      "        [-2.2546,  1.1342, -1.4603, -0.9224, -2.4934, -1.4977, -1.5556,  1.5756,\n",
      "          5.9101,  3.0134,  1.0867],\n",
      "        [-1.5854,  1.2993, -1.4397, -1.2961, -2.7862, -2.3356, -2.2745, -1.1317,\n",
      "          4.4817,  1.6526, -1.2378],\n",
      "        [-1.1172,  1.1022, -0.8361, -0.3200, -2.4079, -1.6170, -2.2287,  0.2664,\n",
      "          5.9905,  1.4665,  1.1118]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.81780481338501\n",
      "output: tensor([[-2.5156,  0.3014, -1.4530, -0.6364, -2.2504, -1.5365, -1.6938, -1.9859,\n",
      "          3.0197,  3.5681,  0.4206],\n",
      "        [-0.9974,  2.0815, -0.7360, -0.0858, -2.7318, -1.3569, -4.3474,  7.6254,\n",
      "          5.9646,  4.0046, 11.0919],\n",
      "        [-1.5556, -0.6715, -1.1347, -0.7672, -3.1646, -2.7151, -2.5285, -0.8688,\n",
      "          4.3116,  2.9233,  1.1222],\n",
      "        [-6.2399, -1.2914, -2.8646, -3.2464, -7.5629, -5.0035, -5.5887,  4.4735,\n",
      "          5.2037,  6.7746,  6.9768],\n",
      "        [-2.9219, -0.3168, -1.4173, -1.3582, -2.4858, -1.6605, -2.1658, -2.0257,\n",
      "          3.7723,  3.9840,  0.6100],\n",
      "        [-2.0577,  0.3584, -2.1212, -1.4997, -2.7781, -2.1927, -2.7850,  0.0418,\n",
      "          3.6326,  0.7461,  0.6173],\n",
      "        [-3.9609,  0.7019,  0.5440, -1.6977, -5.2041, -1.9784, -1.9061,  7.6258,\n",
      "          2.0054,  6.0385,  5.7839],\n",
      "        [-4.1273, -4.8392, -1.6595, -1.5719, -3.6377, -2.5660, -2.0767,  0.6614,\n",
      "          3.8077,  3.4176, -0.3730]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.8530867099761963\n",
      "output: tensor([[-1.0721e+00, -8.3561e-01, -7.3938e-01,  9.9452e-02, -2.0486e+00,\n",
      "         -8.8678e-01, -7.1489e-01,  1.8811e-03,  1.5616e+00,  2.6390e+00,\n",
      "          1.1411e+00],\n",
      "        [-8.8190e-01, -2.5920e+00, -4.4695e+00, -3.6243e+00, -5.1905e+00,\n",
      "         -3.2494e+00, -3.1229e+00,  1.3759e-01,  3.1573e+00,  6.7310e+00,\n",
      "          2.6933e+00],\n",
      "        [-8.9202e+00,  1.0631e+00, -1.1660e+00, -2.2158e+00, -6.3278e+00,\n",
      "         -2.3330e+00, -5.7738e+00,  1.2176e+01, -5.5660e+00,  6.1940e+00,\n",
      "          3.7736e+00],\n",
      "        [-3.3488e+00,  5.9689e-01, -2.4070e+00, -4.5174e+00, -4.6111e+00,\n",
      "         -1.5052e+00, -4.2804e+00,  7.5708e+00, -1.6056e+00,  7.7955e+00,\n",
      "          2.0650e+00],\n",
      "        [-3.6384e+00,  1.3756e+00, -1.3773e+00, -1.5065e+00, -3.5610e+00,\n",
      "         -5.6006e-01, -2.4584e+00,  4.0034e+00, -1.0027e+00,  1.6083e+00,\n",
      "          1.5364e+00],\n",
      "        [-5.4151e+00,  1.2476e+00, -4.6942e+00, -4.9388e+00, -7.8918e+00,\n",
      "         -3.1849e+00, -4.9160e+00,  8.9260e+00, -1.6900e+00,  6.8287e-01,\n",
      "          1.6155e+00],\n",
      "        [-5.9373e+00, -5.3888e+00, -4.8356e+00, -1.8493e+00, -5.4728e+00,\n",
      "         -3.7380e+00, -7.3828e+00,  2.7808e+00,  1.6925e+00,  3.1205e+00,\n",
      "          2.6533e-01],\n",
      "        [-2.1875e+00,  1.0964e+00, -1.1218e+00,  4.0964e-01, -1.4519e+00,\n",
      "         -9.1646e-01, -5.4478e-01,  7.1227e-03,  1.0419e+00,  2.6984e+00,\n",
      "          1.8321e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 1.1936063766479492\n",
      "output: tensor([[-2.5333e+00,  2.5263e-01, -2.7184e+00, -2.2379e+00, -3.4258e+00,\n",
      "         -1.0946e+00, -1.4382e+00,  3.3109e+00,  5.5977e-01,  1.8071e+00,\n",
      "          4.2356e+00],\n",
      "        [-2.4949e+00, -1.5258e+00, -2.0877e+00, -1.3934e+00, -3.4008e+00,\n",
      "         -1.3166e+00, -3.1600e+00,  1.8796e+00, -2.4884e-02,  3.2378e+00,\n",
      "          3.9085e-03],\n",
      "        [-1.1365e+01,  3.4975e+00, -6.8885e+00, -6.4398e+00, -9.5928e+00,\n",
      "         -3.9033e+00, -5.3726e+00,  1.0257e+01, -7.2635e+00,  5.4997e+00,\n",
      "          2.8801e+00],\n",
      "        [-4.8342e+00, -1.6662e+00, -3.7545e+00, -2.2857e+00, -4.9157e+00,\n",
      "         -3.0528e+00, -4.3285e+00,  3.8782e+00, -1.5797e+00,  4.5507e+00,\n",
      "         -3.6137e-01],\n",
      "        [-4.2954e+00, -1.4539e-01, -2.2779e+00, -2.7290e+00, -2.6146e+00,\n",
      "         -1.0480e+00, -2.8316e+00,  3.7606e+00, -3.5395e-01,  1.1989e+00,\n",
      "          4.6704e-01],\n",
      "        [-1.5491e+01,  4.1910e+00, -1.1615e+01, -1.2376e+01, -1.2016e+01,\n",
      "         -5.9281e+00, -9.4687e+00,  1.0952e+01, -1.2775e+01,  1.1360e+01,\n",
      "          4.2448e+00],\n",
      "        [-6.3097e+00, -6.4825e+00, -4.9196e+00, -2.8974e+00, -6.1456e+00,\n",
      "         -5.5625e+00, -6.1651e+00,  4.9784e+00, -2.6509e-01,  6.1647e+00,\n",
      "          1.6690e-01],\n",
      "        [-2.1470e+00, -1.3297e+00, -2.1292e+00, -2.3789e+00, -4.2282e+00,\n",
      "         -2.9130e+00, -2.8374e+00,  1.2134e+00, -1.0764e+00,  1.1604e+00,\n",
      "          7.4550e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 4.595273971557617\n",
      "output: tensor([[-4.2180,  3.4956, -2.3216, -2.4478, -4.0996, -2.1262, -2.4016,  2.9766,\n",
      "         -2.6211,  2.3097,  2.2970],\n",
      "        [-1.5260, -3.1184, -0.1413, -0.4240, -1.4869, -0.1342, -2.9120,  2.5922,\n",
      "         -0.9810,  2.6757,  1.8290],\n",
      "        [-6.9960,  7.8723, -5.3460, -4.7586, -8.8471, -4.8852, -7.2317, 12.3863,\n",
      "         -8.6929,  3.5224,  5.1063],\n",
      "        [-9.0006,  4.8256, -1.9629, -6.7446, -6.0758, -5.4433, -6.0293, 10.6536,\n",
      "         -8.5995,  1.4781,  4.2797],\n",
      "        [-6.5423, -2.8688, -4.2301, -4.4279, -6.6662, -2.2632, -4.9319,  5.8922,\n",
      "         -2.2548,  4.0895,  1.4973],\n",
      "        [-3.0423, -1.9662, -1.5081, -1.7162, -2.4716, -2.0169, -1.4116,  2.5379,\n",
      "         -2.9035,  4.3196,  1.2062],\n",
      "        [-6.8401,  2.6886, -5.5535, -6.0876, -8.9073, -5.2752, -4.5371,  8.6299,\n",
      "         -9.5048,  8.7825,  3.8718],\n",
      "        [-3.0031,  1.5401, -2.8303, -2.6810, -2.9726, -0.2345, -2.6571,  2.3409,\n",
      "         -0.5956,  2.5560,  0.9254]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.536675214767456\n",
      "output: tensor([[ -4.9224,  -4.4768,  -4.6232,  -4.9351,  -6.3655,  -0.2267,  -5.3734,\n",
      "           5.3866,  -2.3536,   3.7786,  -0.9426],\n",
      "        [ -3.2544,   2.3450,  -2.9323,  -4.8283,  -6.7220,  -1.4252,  -5.0489,\n",
      "           4.5578,  -5.2505,   1.3060,   1.8436],\n",
      "        [ -2.3252,   3.8351,  -2.9746,  -2.2817,  -2.8718,   0.4725,  -0.9322,\n",
      "           2.4817,  -5.4548,   2.1717,   3.9460],\n",
      "        [-10.2199,   8.8269,  -6.0221,  -7.4545,  -6.0972,  -7.6818,  -7.8887,\n",
      "          10.9473,  -8.8276,  -4.1539,   4.9152],\n",
      "        [ -1.3024,  -2.5994,   1.0751,  -1.7140,  -0.7707,   1.1661,  -0.8563,\n",
      "           3.8151,  -3.9809,   1.4670,   0.9062],\n",
      "        [ -7.1506,   9.4373,  -8.7967, -11.7406, -13.7965,  -2.7154,  -7.6820,\n",
      "          12.9185,  -8.4319,  -0.9599,   6.7609],\n",
      "        [ -4.4809,  -0.5938,  -4.6919,  -3.6978,  -5.7111,  -3.0570,  -5.6966,\n",
      "           4.8917,  -3.3334,   2.3991,   1.6859],\n",
      "        [ -1.7852,  -2.1466,  -2.8817,  -2.8961,  -2.8080,   0.6837,  -3.1720,\n",
      "           1.0060,  -2.0564,   3.3885,   1.1312]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.627277135848999\n",
      "output: tensor([[-3.0586e+00,  4.4085e+00, -3.7879e+00, -3.9123e+00, -5.4093e+00,\n",
      "         -8.8605e-01, -4.7516e+00,  5.2103e+00, -2.3920e+00, -2.5001e+00,\n",
      "          3.7814e-01],\n",
      "        [-4.5395e+00,  6.8740e+00, -7.9948e+00, -8.7562e+00, -7.2399e+00,\n",
      "         -3.2062e+00, -5.0445e+00,  5.3437e+00, -6.1872e+00, -1.7198e+00,\n",
      "          5.2507e+00],\n",
      "        [-1.2624e+00, -2.4969e+00, -3.0202e+00, -3.6970e+00, -4.7118e+00,\n",
      "          2.9880e+00, -4.0949e+00, -3.1638e-01, -6.2953e+00,  5.7482e+00,\n",
      "         -5.8690e-01],\n",
      "        [-4.9831e+00,  3.8455e+00, -4.5826e+00, -7.3279e+00, -6.5799e+00,\n",
      "         -2.7891e+00, -5.9790e+00,  7.5801e+00, -2.3390e+00, -4.7701e+00,\n",
      "         -8.1958e-01],\n",
      "        [-2.4802e+00, -3.6193e+00, -2.5705e+00, -2.4510e+00, -4.4201e+00,\n",
      "          2.7080e+00, -8.0028e+00,  5.3108e+00, -5.0704e+00, -1.9168e+00,\n",
      "          2.9200e+00],\n",
      "        [-4.7337e+00,  1.0782e+01, -4.1061e+00, -6.6682e+00, -7.1886e+00,\n",
      "          4.5496e-03, -6.7564e+00,  5.2153e+00, -8.7267e+00,  1.6144e+00,\n",
      "          7.1714e+00],\n",
      "        [-6.8645e-01,  1.7826e+00, -2.2367e+00, -2.7219e+00, -2.5165e+00,\n",
      "         -1.1747e+00, -1.5895e+00,  1.8842e+00, -1.6318e+00,  5.1963e-02,\n",
      "          1.6898e+00],\n",
      "        [-1.8541e+00,  2.2620e+00, -1.1005e+00, -2.2957e+00, -2.3037e+00,\n",
      "         -1.0709e+00, -8.4912e-01,  1.8504e+00, -3.4804e+00,  2.0477e-01,\n",
      "          1.9669e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 4.563239097595215\n",
      "output: tensor([[  1.2517,   7.3936,  -1.3702,  -4.0257,  -4.2567,   3.4006,  -2.7298,\n",
      "           6.1703,  -7.9134,  -0.1771,   6.1509],\n",
      "        [  0.2840,   2.0762,  -2.1091,  -2.9820,  -2.3925,   0.1559,  -0.9363,\n",
      "           0.2550,  -1.8656,   1.3855,   1.8648],\n",
      "        [  0.1176,   0.4717,  -2.7052,  -0.0787,  -3.1289,   1.4475,  -3.7465,\n",
      "           4.7046,  -6.0649,  -2.5145,   3.8668],\n",
      "        [ -0.9482,  -1.7393,  -2.0419,  -3.6061,  -4.0812,   2.9186,  -2.3559,\n",
      "           2.4082,  -6.7180,   1.4909,   2.9507],\n",
      "        [  0.4299,  -2.3688,  -3.4558,  -5.0929,  -6.0933,   2.4191,  -4.2669,\n",
      "           5.9166,  -6.3352,  -0.9731,   1.1579],\n",
      "        [ -7.6470,   9.8912,  -7.8331,  -5.7249, -12.0586,  -0.0610,  -8.1728,\n",
      "          12.1553,  -5.0109,  -5.7547,   0.9528],\n",
      "        [ -4.6945,   6.3966,  -8.0730, -14.2786,  -9.6175,  -3.6312,  -7.2952,\n",
      "          10.3786,  -4.4645,  -4.0214,   0.4345],\n",
      "        [ -2.6929,   3.2484,  -4.5574,  -5.1865,  -6.5247,  -3.1843,  -3.9026,\n",
      "           4.8432,  -1.9354,  -0.9419,  -0.0360]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.660097122192383\n",
      "output: tensor([[  3.4164,  -2.5699,  -2.4041,  -3.6332,  -2.2398,   3.1335,  -2.7704,\n",
      "          -2.0694,  -6.7476,  -0.1341,   3.2834],\n",
      "        [ -2.3767,   2.8389,  -4.3166,  -3.6916,  -2.6161,   0.7872,  -1.8348,\n",
      "           1.8109,  -1.8130,  -1.6904,   1.3764],\n",
      "        [ -8.2823,   2.0036, -10.3079, -10.4970, -11.0943,  -5.3478, -12.3503,\n",
      "          12.2447,  -3.5151,  -8.9112,  -1.1442],\n",
      "        [  2.7992,  -8.6300,  -9.4322,  -7.8906,  -8.0749,   4.9765,  -8.6820,\n",
      "           7.7499,  -4.4946,  -7.3882,   2.8436],\n",
      "        [ -3.4591,   4.4021,  -4.0612,  -3.6500,  -5.0147,  -1.5964,  -5.8418,\n",
      "           3.4622,  -1.6367,  -2.4669,  -0.7021],\n",
      "        [ -5.5742,   1.9419,  -6.9384, -11.3673, -10.4311,  -4.6998,  -8.4319,\n",
      "           9.3635,  -2.4412,  -6.0687,   2.5775],\n",
      "        [  0.5822,  -1.4428,  -4.0576,  -2.9482,  -3.0898,  -0.3620,  -2.6871,\n",
      "           0.3370,  -0.7380,  -1.7830,   0.6534],\n",
      "        [  0.7829,   0.1281,  -2.2083,  -2.0225,  -2.5234,   1.3066,  -2.7242,\n",
      "          -0.0985,  -1.3133,   0.5853,  -0.3299]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.533653736114502\n",
      "output: tensor([[ 1.7285,  0.5722, -0.1805, -2.0257, -2.5274,  2.1162, -1.6317,  0.0592,\n",
      "         -0.9283, -1.3342,  1.8545],\n",
      "        [-0.4228,  1.4692, -4.0624, -3.5799, -5.3916,  0.6786, -4.0190,  2.6021,\n",
      "         -0.9824, -4.4001,  0.6613],\n",
      "        [-3.2783,  2.0245, -3.7324, -4.3753, -6.6939, -2.1146, -5.7077,  9.5582,\n",
      "         -1.1402, -7.4565,  0.8035],\n",
      "        [ 2.5137,  1.1029, -6.6601, -2.7290, -5.6000, -0.5490, -6.8470, -0.4286,\n",
      "         -3.0552,  0.1154, -1.2139],\n",
      "        [-2.7249,  0.0317, -2.6048, -5.0362, -4.4067, -2.7130, -5.5650,  5.9754,\n",
      "         -0.2880, -5.3854,  3.2290],\n",
      "        [ 0.8647,  4.2313, -2.9038, -4.6916, -5.8839,  2.2160, -8.4409,  6.0408,\n",
      "          0.2147, -3.4380, -0.8901],\n",
      "        [-2.2596,  0.6700, -5.5752, -6.6191, -8.7081, -0.8698, -8.3663,  9.8676,\n",
      "          3.2141, -7.3807,  1.5448],\n",
      "        [ 4.7866, -4.1745, -0.9869, -2.8544, -0.6447,  9.0092, -1.0326, -0.7911,\n",
      "         -6.6978, -1.9674,  3.6799]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 5.603572845458984\n",
      "output: tensor([[-2.2228,  0.5154, -4.0719, -2.8129, -3.6234, -0.4978, -2.3966, -0.5996,\n",
      "          0.8201, -1.5983,  1.4267],\n",
      "        [-1.0982,  0.1306, -3.1265, -1.3375, -1.5202, -0.2746, -2.1340, -0.8497,\n",
      "          1.7264, -1.5508,  0.8492],\n",
      "        [-1.9969, -5.5652, -6.5569, -7.6084, -8.6193,  1.7116, -8.9682,  9.4313,\n",
      "          3.9118, -7.6221,  4.0161],\n",
      "        [-2.1555,  0.6800, -4.7789, -3.7824, -4.1617, -1.3101, -5.3640,  2.1771,\n",
      "          1.8490, -1.1253,  1.3163],\n",
      "        [-0.7581, -1.4365, -8.5609, -8.5892, -7.1639, -1.4936, -8.8494,  3.7279,\n",
      "          0.2180, -5.2340, -1.6008],\n",
      "        [ 4.4855, -4.8186, -6.0579, -4.3298, -2.6049,  5.1708, -4.7934, -1.3845,\n",
      "         -5.8499, -0.5324,  1.9577],\n",
      "        [ 1.2257, -1.1388, -3.8310, -3.8657, -2.2663,  2.0727, -2.0758,  0.5143,\n",
      "         -2.4720, -1.5875,  1.5427],\n",
      "        [-1.0130,  1.0172, -4.5464, -3.6445, -3.3782, -0.4446, -3.4618,  0.7470,\n",
      "          2.8258, -0.5208,  0.1580]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.531862735748291\n",
      "output: tensor([[ 9.9979e-01, -9.8113e+00, -6.5543e+00, -5.4740e+00, -6.3907e+00,\n",
      "         -5.5790e-01, -7.6552e+00,  6.7967e+00,  4.9563e+00, -9.8151e+00,\n",
      "          3.4302e+00],\n",
      "        [-2.0333e+00, -3.1932e-01, -5.2321e+00, -5.8594e+00, -5.2264e+00,\n",
      "         -6.2228e-01, -5.7289e+00,  1.0427e+00,  3.6374e+00, -3.5993e+00,\n",
      "         -1.3380e+00],\n",
      "        [-1.2342e+00,  3.7798e+00, -3.4931e+00, -3.5194e+00, -2.6640e+00,\n",
      "         -3.3512e+00, -4.3971e+00,  2.4558e+00,  8.5748e+00, -3.9482e+00,\n",
      "         -2.9604e-01],\n",
      "        [-6.5841e-01, -1.2367e+00, -2.9233e+00, -2.4069e+00, -2.4799e+00,\n",
      "         -7.3187e-01, -2.2409e+00, -2.7533e-01,  1.3407e+00, -2.7074e-01,\n",
      "          5.7233e-01],\n",
      "        [-3.8760e+00, -1.7947e+00, -8.9102e+00, -3.6430e+00, -3.0018e+00,\n",
      "         -4.0563e+00, -3.3127e+00, -2.5133e+00, -2.9171e+00,  6.9911e+00,\n",
      "          4.1515e+00],\n",
      "        [-1.3918e+00, -6.4825e-01, -3.7226e+00, -5.9682e+00, -4.1764e+00,\n",
      "         -3.5036e+00, -4.8002e+00, -1.1047e+00, -8.6302e-01,  2.7857e+00,\n",
      "          1.1940e+00],\n",
      "        [-1.6327e+00,  1.9698e-01, -5.5735e+00, -4.0995e+00, -6.0934e+00,\n",
      "         -1.5864e+00, -6.2311e+00, -1.0633e+00,  1.2080e-01, -1.2421e-01,\n",
      "         -4.5864e-03],\n",
      "        [-3.9365e+00,  3.7025e+00, -6.7252e+00, -7.4518e+00, -7.8303e+00,\n",
      "         -2.0167e+00, -7.0592e+00,  6.6731e-01,  3.9564e+00, -3.9127e+00,\n",
      "         -2.2402e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 2.0125889778137207\n",
      "output: tensor([[ -2.3317,  -0.9072,  -7.9011,  -6.1253,  -5.2075,  -2.5493,  -5.7023,\n",
      "           2.1108,   6.4534,  -3.6335,  -0.5203],\n",
      "        [ -3.1388,   2.0651,  -8.3321,  -7.7976,  -6.9398,  -2.6031,  -7.1629,\n",
      "          -0.4131,   5.9059,  -2.3990,  -1.3789],\n",
      "        [ -0.2412,  -1.0621,  -3.9363,  -5.2068,  -2.9462,  -1.8765,  -3.6929,\n",
      "          -1.1288,   1.9954,   2.2988,  -0.2263],\n",
      "        [ -0.4205,  -1.3567,  -2.6396,  -3.6668,  -3.4322,  -1.4903,  -3.5727,\n",
      "           0.1477,   3.0416,   1.1421,  -0.4513],\n",
      "        [ -0.3269,  -1.3941,  -9.4496,  -6.6027,  -7.8269,  -2.2257,  -8.4403,\n",
      "           3.7837,   7.9883,  -7.3403,  -0.1111],\n",
      "        [ -4.1466,  -0.5685,  -9.1050,  -8.6254, -10.3298,  -2.7844,  -8.2255,\n",
      "           0.0644,   5.6494,  -2.9857,  -4.2320],\n",
      "        [ -4.0398,   1.7865,  -7.7034,  -8.0686,  -6.9236,  -2.1388,  -6.9727,\n",
      "           0.7688,   3.5257,  -2.6450,  -2.2266],\n",
      "        [-13.1557,   1.0706, -19.2586, -15.7093, -17.4183,  -9.0407, -12.3283,\n",
      "          -1.6046,   1.2876,   1.0619,   3.5513]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.322761535644531\n",
      "output: tensor([[ -1.0099,  -3.4728, -10.0068, -11.7405, -13.0107,  -4.4843, -12.9518,\n",
      "           3.9890,  10.1609,  -0.6666,  -4.0045],\n",
      "        [ -2.7581,  -2.9814,  -9.1832,  -8.2124,  -8.3662,  -2.0095,  -5.8685,\n",
      "           2.9673,   8.3339,  -1.5789,  -1.9946],\n",
      "        [ -5.6615,   1.4565,  -7.6077,  -5.9519,  -7.1610,  -6.5327,  -5.9000,\n",
      "          -0.1929,   1.7582,   0.3324,  -1.9704],\n",
      "        [ -9.4597,  -1.0033, -16.7322, -16.9736, -15.4248, -11.2623, -13.4459,\n",
      "          -0.7994,   3.3205,   3.8801,  -1.1541],\n",
      "        [ -4.1792,  -0.2289,  -9.4726,  -8.8282,  -8.7261,  -4.8850, -11.6182,\n",
      "          -0.2186,   5.5034,  -1.8358,  -4.1336],\n",
      "        [ -6.5200,   1.7316,  -9.5842,  -7.9912,  -9.2019,  -4.3720,  -8.2862,\n",
      "          -0.0394,   4.4555,  -1.2678,  -3.4200],\n",
      "        [ -2.4727,   0.4722,  -6.8757,  -6.4317,  -6.5290,  -4.7134,  -5.0869,\n",
      "          -1.0907,   2.1004,   1.2064,  -2.5455],\n",
      "        [ -4.1770,   0.2829,  -8.4505,  -7.3456,  -7.8458,  -1.2740,  -7.5218,\n",
      "           1.5708,   6.3487,  -3.3638,  -2.3428]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 5.397958278656006\n",
      "output: tensor([[  0.1488,  -2.4256,  -3.4137,  -2.8882,  -3.2895,  -1.0330,  -2.9034,\n",
      "           1.2175,   2.4934,  -1.8542,  -1.3463],\n",
      "        [ -0.8597,  -1.0005,  -8.2311,  -9.6141,  -9.9512,  -6.4508,  -7.3758,\n",
      "          -3.1948,   1.8168,   2.7895,  -4.4365],\n",
      "        [ -2.9768,  -0.7521,  -8.7753,  -5.5620,  -8.8127,  -2.4266,  -8.4106,\n",
      "           2.0123,   4.6800,  -0.0484,  -0.3747],\n",
      "        [ -8.0068,   8.3265, -20.2973, -20.2330, -21.9602, -11.9787, -19.2233,\n",
      "          -0.8651,   0.9114,  -2.1223,  -5.2907],\n",
      "        [ -1.4877,  -6.5897, -12.1785, -11.3449, -10.9566,  -4.2626, -10.5114,\n",
      "           6.1458,   6.9813,  -2.1581,  -1.9518],\n",
      "        [ -3.4272,   3.6364,  -7.9912,  -6.2906,  -7.5097,  -4.2853,  -8.6847,\n",
      "          -1.3136,   3.2627,  -1.4023,  -2.8571],\n",
      "        [ -2.7161,  -0.5021,  -6.4329,  -5.0642,  -5.5989,  -0.8760,  -5.4331,\n",
      "           1.0971,   4.0551,  -1.1686,  -1.8066],\n",
      "        [ -5.9558,   1.8798, -11.4181, -10.6071, -12.3895,  -6.3440, -11.2947,\n",
      "           0.7743,   3.5497,   1.9791,  -4.6460]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 5.298150539398193\n",
      "output: tensor([[ -1.5594,  -0.5868,  -5.5938,  -4.4335,  -5.7262,  -1.8280,  -5.7183,\n",
      "           0.6099,   1.8157,   1.9060,  -1.8314],\n",
      "        [ -5.5159,   2.7025, -12.1920, -12.4693, -11.1658,  -6.6597, -10.5904,\n",
      "           2.1245,   3.8483,  -2.5607,  -5.5946],\n",
      "        [ -2.4114,   2.0756,  -3.7033,  -3.9200,  -3.7359,  -2.2921,  -3.1898,\n",
      "           0.1203,   1.1953,   0.4714,  -1.7267],\n",
      "        [ -1.0741,  -0.6621,  -4.3268,  -3.2813,  -3.8810,  -1.6616,  -3.2057,\n",
      "           0.8375,   0.8570,  -0.1900,  -1.5739],\n",
      "        [ -7.0221,   2.5106, -27.3351, -33.1897, -26.6511, -22.5300, -27.2646,\n",
      "           5.7755,   0.9982,   7.6079, -12.5721],\n",
      "        [ -0.6817,   1.1861,  -3.4728,  -2.8189,  -4.3581,  -0.2322,  -2.4549,\n",
      "           0.6674,   1.2516,  -0.5307,  -1.3479],\n",
      "        [ -3.3810,  -1.1463,  -8.6150,  -5.5495,  -7.9617,  -6.2248,  -4.6339,\n",
      "           1.3507,   5.2143,   0.1834,  -2.0778],\n",
      "        [ -0.2144,   1.4196,  -9.3739,  -6.6520,  -9.3009,  -8.8547,  -8.0269,\n",
      "           1.9845,   2.8998,  -0.0679,  -3.9402]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.488680839538574\n",
      "output: tensor([[-3.8092e+00, -3.0193e+00, -1.0617e+01, -8.9106e+00, -1.3003e+01,\n",
      "         -7.0212e+00, -4.2469e+00,  4.1098e+00,  6.3208e-02, -1.7971e-01,\n",
      "         -2.7022e+00],\n",
      "        [-2.2996e+00,  1.3920e+00, -4.6404e+00, -3.9613e+00, -3.8892e+00,\n",
      "         -2.5748e+00, -2.8216e+00,  9.2880e-01,  4.4536e-01, -6.7426e-01,\n",
      "         -1.6750e+00],\n",
      "        [-1.3648e+01,  7.2485e+00, -2.2689e+01, -2.0312e+01, -2.5635e+01,\n",
      "         -1.9137e+01, -1.9752e+01,  1.4782e+01, -4.0332e-01, -5.6267e+00,\n",
      "         -6.7114e+00],\n",
      "        [-3.9270e+00, -4.2981e+00, -1.1914e+01, -8.6612e+00, -1.0678e+01,\n",
      "         -8.3696e+00, -6.5504e+00,  6.6566e+00, -3.3551e-01,  2.4344e+00,\n",
      "         -1.5290e+00],\n",
      "        [-4.4265e+00,  5.5545e+00, -1.1558e+01, -8.5957e+00, -1.2529e+01,\n",
      "         -5.1379e+00, -9.0773e+00,  3.6735e+00,  1.1622e+00, -9.0827e-01,\n",
      "         -4.5091e+00],\n",
      "        [-1.6398e+00, -4.4242e-02, -6.4424e+00, -5.6907e+00, -6.2057e+00,\n",
      "         -3.6590e+00, -5.0270e+00,  7.1206e-01,  1.3639e+00, -1.3394e-01,\n",
      "         -2.9474e+00],\n",
      "        [-4.8533e+00,  3.4143e+00, -9.9195e+00, -8.1023e+00, -7.8358e+00,\n",
      "         -5.6746e+00, -6.1925e+00,  1.2206e+00,  9.1903e-01, -7.7022e-02,\n",
      "         -3.5714e+00],\n",
      "        [-3.0327e+00,  4.0557e-01, -6.4544e+00, -3.9633e+00, -5.6505e+00,\n",
      "         -4.0736e+00, -3.5327e+00,  2.0036e+00,  1.0084e+00, -1.1490e-02,\n",
      "         -1.6399e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 3.826840400695801\n",
      "output: tensor([[ -4.1931,   0.4888,  -8.2786,  -7.2311,  -9.6382,  -3.8496,  -5.2779,\n",
      "           3.9869,  -2.4109,   0.6566,  -5.6954],\n",
      "        [ -5.6790,   3.4043, -10.1718, -10.1770, -10.4373,  -7.5668, -10.0835,\n",
      "           4.3215,  -0.5348,  -3.1106,  -7.2047],\n",
      "        [ -2.9516,  -1.0997,  -6.7518,  -7.1447,  -7.4747,  -3.0395,  -2.8398,\n",
      "           3.5775,  -0.3158,   0.9070,  -3.3986],\n",
      "        [ -0.9805,  -0.9577, -10.7675,  -8.2146, -10.1461,  -7.6673,  -4.7608,\n",
      "           1.2726,  -1.3697,   1.4243,  -3.9103],\n",
      "        [ -8.3225,  -0.1457, -13.6123, -14.2421, -14.8089,  -7.5465, -11.7691,\n",
      "           7.9710,  -1.6248,  -1.6409,  -9.4886],\n",
      "        [ -3.1638,   0.1911, -12.4087, -10.2263, -10.3884, -10.6032,  -7.7716,\n",
      "           2.7141,  -1.8157,   2.4859,  -3.8403],\n",
      "        [ -3.3368,  -3.1079, -11.8951, -10.0063, -12.2766,  -5.1554,  -1.5627,\n",
      "           3.7536,   2.4964,   3.4204,  -7.1340],\n",
      "        [ -3.8822,  -0.7689, -17.6388, -14.5251, -16.3804, -19.5609, -13.6796,\n",
      "           4.6682,   0.7099,  -1.2058,  -6.9815]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.351739883422852\n",
      "output: tensor([[ -2.1004,   3.2558, -10.4705,  -9.1315, -10.5806,  -6.3409,  -8.4608,\n",
      "           3.4949,  -0.4020,   0.1254,  -6.0717],\n",
      "        [ -3.5123,   1.2298,  -8.3708,  -6.6366,  -7.8528,  -2.4801,  -5.1983,\n",
      "           4.8442,  -1.8922,   0.2491,  -4.6024],\n",
      "        [ -3.2283,   0.2474,  -5.1977,  -4.5653,  -5.2292,  -1.0913,  -2.0664,\n",
      "           2.6051,  -1.2007,  -0.3817,  -2.7137],\n",
      "        [-12.2576,   1.2231, -32.8273, -30.7918, -33.7708, -30.1365, -25.9294,\n",
      "          12.2298,  -1.0263,  -4.2610, -12.9726],\n",
      "        [ -1.2803,   0.8919,  -6.7544,  -6.6458,  -6.6990,  -6.7476,  -3.9950,\n",
      "           2.9022,  -1.3223,   3.5597,  -5.0530],\n",
      "        [ -2.3970,  -0.0765,  -5.7365,  -5.5605,  -4.8399,  -3.0300,  -3.3799,\n",
      "           3.2806,  -1.8355,   0.5383,  -2.8977],\n",
      "        [ -2.9263,   1.6167,  -5.4513,  -4.9879,  -4.6483,  -3.1296,  -3.4978,\n",
      "           2.2622,  -1.5130,   0.9426,  -2.1367],\n",
      "        [ -7.0076,   0.8792, -17.5288, -12.8917, -14.9021,  -5.5067,  -6.8342,\n",
      "           5.1114,  -4.5666,   1.9309,  -1.7268]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.5801126956939697\n",
      "output: tensor([[ -4.1215,   0.9812,  -8.7489,  -8.4837,  -8.9189,  -4.1056,  -5.8314,\n",
      "           4.2866,  -0.4385,  -0.0872,  -2.9863],\n",
      "        [ -3.5524,   0.2807,  -7.9637,  -7.2952,  -7.5289,  -4.4379,  -5.7902,\n",
      "           3.3229,  -1.4540,  -0.2647,  -3.0928],\n",
      "        [ -4.7681,  -3.6035, -15.5248, -13.0114, -12.9338, -13.1748, -14.2790,\n",
      "           5.3312,  -3.6399,  -3.8958,  -5.8836],\n",
      "        [ -5.1466,   1.9525,  -7.1013,  -7.3903,  -7.6230,  -2.5594,  -5.6232,\n",
      "           3.4624,  -2.6398,   0.5912,  -4.8990],\n",
      "        [ -1.4349,   2.2910,  -4.0781,  -3.4725,  -3.6746,  -1.7946,  -1.3835,\n",
      "           0.7800,  -1.0487,   2.3998,  -0.7221],\n",
      "        [ -3.4625,   1.7505,  -6.6180,  -4.9201,  -6.1862,  -2.2886,  -2.8463,\n",
      "           2.5744,  -1.5569,   0.7511,  -2.5756],\n",
      "        [ -5.8290,   3.1969, -22.9541, -17.4971, -19.3475, -18.5015,  -4.1489,\n",
      "           6.9760, -13.3879,   2.9509, -11.9314],\n",
      "        [ -2.3782,   0.0481,  -5.1209,  -3.9164,  -4.3625,  -2.3731,  -3.0263,\n",
      "           2.5667,  -2.7556,   1.3118,  -1.2775]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.552053451538086\n",
      "output: tensor([[ -4.5872,  -1.1616,  -8.2619,  -5.3181,  -6.4486,  -3.3172,  -4.3713,\n",
      "           5.7572,  -2.7611,  -2.0819,  -2.7642],\n",
      "        [  1.6947,   0.2453, -24.0859, -18.9423, -23.3554, -23.2992, -10.4829,\n",
      "           9.6570,  -2.6876,  -3.2682,  -5.4112],\n",
      "        [ -1.4942,   2.3500,  -7.9715,  -7.3154,  -7.1461,  -4.6872,  -4.0906,\n",
      "           0.2125,  -1.9576,   2.7283,  -1.3740],\n",
      "        [ -3.3725,   0.8413, -11.4679,  -8.9382,  -9.7905,  -4.8432,  -2.2299,\n",
      "           7.3020,  -6.6065,   5.3842,   0.3787],\n",
      "        [ -2.1193,   0.8997,  -6.0212,  -5.3582,  -5.8319,  -3.6441,  -1.6394,\n",
      "           2.5242,  -4.2482,   2.1193,  -0.8286],\n",
      "        [ -4.7012,   3.0662,  -9.9902,  -8.6346, -10.6761,  -7.4587,  -7.7494,\n",
      "           2.1036,  -1.3851,   3.1500,  -6.7341],\n",
      "        [ -2.5906,   1.4614,  -8.2129,  -8.2348,  -8.3448,  -3.3957,  -5.6177,\n",
      "           3.5348,  -0.9955,   0.1189,  -3.3832],\n",
      "        [ -4.7319,   0.7874,  -8.3733,  -8.2938,  -6.3368,  -3.1253,  -3.0631,\n",
      "           4.6987,  -3.1970,   2.3583,  -0.5651]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 5.216226577758789\n",
      "output: tensor([[ -2.4692,   1.9944, -20.4512, -19.3610, -22.5312, -17.2449, -14.6182,\n",
      "           1.8292,  -1.6645,   1.3956,  -9.5861],\n",
      "        [ -5.3336,  -1.8471,  -9.4393,  -7.3618,  -8.1027,  -1.5813,  -6.0957,\n",
      "           3.6618,  -4.7851,   1.3901,  -2.3076],\n",
      "        [ -2.6041,   1.1597, -18.1806, -14.1630, -15.1628, -15.6956,  -8.5138,\n",
      "           1.0035,   1.0292,   0.5390,  -3.9561],\n",
      "        [ -2.2878,   0.7115,  -8.1973,  -6.7370,  -6.5813,  -4.5172,  -3.0915,\n",
      "           1.8514,  -5.0164,   6.1055,  -2.9748],\n",
      "        [ -2.6974,   1.2169, -10.6845,  -6.3687, -10.9126,  -4.1611,  -3.0096,\n",
      "           0.4904,  -5.5732,   6.6177,  -4.1521],\n",
      "        [ -4.2415,   0.1001,  -8.4263,  -6.2727,  -8.3779,  -3.0602,  -4.6471,\n",
      "           2.1601,  -0.9570,   3.4335,  -4.5516],\n",
      "        [ -4.1845,   3.4428, -11.0642, -11.7068, -13.1555,  -3.8728, -10.0276,\n",
      "           1.8362,  -2.8516,   3.8800,  -9.1711],\n",
      "        [ -1.2493,   2.7826,  -7.8303,  -5.5512,  -4.4797,  -3.7903,  -0.3803,\n",
      "           1.3178,  -6.2765,   6.0263,   0.6530]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 6.1672444343566895\n",
      "output: tensor([[ -2.5745,   1.5851,  -7.5216,  -6.3422,  -7.3067,  -3.5402,  -2.8373,\n",
      "           2.2682,  -1.3511,  -0.1832,  -1.7169],\n",
      "        [ -2.0469,  -2.0432,  -7.5092,  -7.1640,  -7.2777,  -4.1518,  -4.1188,\n",
      "           1.3270,  -5.9911,   5.2348,  -4.0921],\n",
      "        [ -0.9252,   4.4334, -14.3913,  -9.7894, -14.9593,  -7.4117,  -4.9452,\n",
      "          -6.5077,  -6.8263,   8.4198,  -2.0637],\n",
      "        [  4.4205,   2.6147, -14.2703,  -7.3176, -12.4853,  -8.5257,  -4.4797,\n",
      "          -3.4331,  -2.2366,   1.9195,  -1.6877],\n",
      "        [ -0.6391,   0.7780,  -6.1350,  -6.1373,  -9.4057,  -1.9689,  -4.8888,\n",
      "           2.1250,  -3.9557,   0.0666,  -6.3144],\n",
      "        [ -2.4910,  -0.0349,  -7.9543,  -7.3256,  -8.0320,  -1.9119,  -6.2696,\n",
      "           2.5937,  -1.4692,   1.7535,  -3.6385],\n",
      "        [ -2.0670,   3.6726, -10.9360,  -9.0574,  -9.1931,  -7.2033,  -6.2291,\n",
      "          -1.2706,  -0.8618,   3.1504,  -2.2286],\n",
      "        [ -0.6560,   3.3760, -12.2144,  -8.9927, -13.0836,  -8.2792,  -6.1808,\n",
      "          -2.8315,  -3.6427,   7.2156,  -5.0218]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.8588802814483643\n",
      "output: tensor([[-1.9112e-01,  1.0546e+00, -7.3236e+00, -2.4172e+00, -6.7309e+00,\n",
      "         -1.4125e-01, -6.4627e-01,  7.9858e-01, -7.2185e+00,  3.0070e+00,\n",
      "         -2.2304e-01],\n",
      "        [ 4.1522e+00,  2.1365e+00, -1.3800e+01, -9.5036e+00, -1.3166e+01,\n",
      "         -4.1393e+00, -7.8811e+00, -1.7333e+00, -1.2154e+00,  1.3612e+00,\n",
      "         -9.4026e-01],\n",
      "        [ 1.0764e-01,  3.3767e-01, -5.0534e+00, -4.5874e+00, -5.1343e+00,\n",
      "         -2.3890e-01, -3.7428e+00, -3.2708e-01, -1.6827e+00,  2.3928e+00,\n",
      "         -2.1609e+00],\n",
      "        [-1.8288e+00,  8.4611e+00, -1.6076e+01, -1.3099e+01, -1.6195e+01,\n",
      "         -1.0532e+01, -5.0008e+00, -4.1285e+00,  2.7006e-01,  2.1387e+00,\n",
      "          4.2003e+00],\n",
      "        [-1.8250e+00,  8.1765e-01, -7.3218e+00, -5.6979e+00, -8.2094e+00,\n",
      "         -5.4618e+00, -2.5882e+00,  1.1535e-02, -2.1870e+00,  3.9766e+00,\n",
      "          3.0357e-01],\n",
      "        [-1.9644e+00,  2.0479e-01, -6.7633e+00, -4.9330e+00, -7.8432e+00,\n",
      "         -1.0835e+00, -1.8684e+00,  9.4070e-01, -2.2542e+00,  8.8118e-01,\n",
      "         -2.5412e+00],\n",
      "        [-1.6536e+00,  8.0100e-02, -5.3650e+00, -4.4332e+00, -4.9073e+00,\n",
      "         -1.8193e+00, -2.9743e+00,  7.3357e-01, -2.5868e+00,  1.6858e+00,\n",
      "         -3.7449e+00],\n",
      "        [-1.6340e-02, -1.2149e+00, -7.3350e+00, -5.7922e+00, -6.9888e+00,\n",
      "         -3.2583e+00, -4.5968e+00,  1.0752e+00, -1.8529e+00,  8.7484e-01,\n",
      "         -3.8281e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 5.27177095413208\n",
      "output: tensor([[ -0.2553,   3.4578, -10.9851,  -6.3506,  -9.1903,  -6.2856,  -1.4840,\n",
      "          -3.9417,  -0.8197,   4.9589,   4.7538],\n",
      "        [  1.0757,   3.4888,  -5.5029,  -4.1033,  -6.0865,  -1.9788,  -1.3717,\n",
      "          -1.4478,  -3.2101,   3.7174,   0.4958],\n",
      "        [  2.0879,  -2.0634,  -4.5824,  -3.6464,  -7.3162,  -0.3523,  -3.7105,\n",
      "           0.2357,  -3.5832,  -0.2167,  -2.7402],\n",
      "        [ -1.6946,  -0.3738,  -2.9919,  -1.8971,  -4.2783,   0.7364,  -1.1590,\n",
      "           1.4328,  -2.7781,  -0.2023,   0.6059],\n",
      "        [ -1.0915,  -0.2359, -13.4366, -10.9117, -14.7288,  -2.2760,  -6.5541,\n",
      "          -4.7152,  -2.3129,   3.5991,  -3.4604],\n",
      "        [  0.6855,   3.2293,  -6.1250,  -6.5296,  -7.2974,  -3.7857,   3.0136,\n",
      "          -1.1027,  -5.5645,   6.9365,   1.6868],\n",
      "        [ -1.6504,   0.0532,  -8.4572,  -8.4677,  -8.9689,  -1.9298,  -8.5994,\n",
      "           0.0247,  -0.1462,  -1.3006,  -2.5925],\n",
      "        [  0.3062,   0.9705,  -2.2871,  -1.7455,  -1.7261,   0.3478,   1.0180,\n",
      "           0.1300,  -1.8804,   0.7123,  -0.5824]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.307300090789795\n",
      "output: tensor([[ -3.6712,   3.7556, -13.4206,  -9.2422, -12.3199,  -0.5753,  -3.9658,\n",
      "          -2.8250,  -3.3286,   0.0505,   2.9359],\n",
      "        [  2.2949,   0.4424,  -2.9789,  -3.0729,  -2.7684,  -0.3536,   3.6248,\n",
      "          -0.8734,  -3.6756,   1.1289,   3.7937],\n",
      "        [ -2.0174,   0.8876,  -6.2330,  -5.0748,  -6.3993,  -1.4218,   0.0465,\n",
      "          -0.0950,  -3.7723,   3.1035,   2.6336],\n",
      "        [  0.2087,  -0.7744,  -4.5330,  -7.2235,  -8.2716,  -0.1033,  -5.9121,\n",
      "          -1.4864,  -1.6363,  -0.2752,  -5.8437],\n",
      "        [ -3.2337,   0.3864, -12.7639,  -9.6862, -11.8585,  -1.7117,  -7.6661,\n",
      "          -4.6617,   1.1234,   2.0677,  -3.0498],\n",
      "        [  0.5234,   1.3976,  -4.2666,  -4.4288,  -8.0385,   3.3076,  -3.0365,\n",
      "           1.1036,  -1.7134,  -1.0770,  -3.8353],\n",
      "        [ -0.6663,  -0.8281,  -3.5119,  -3.4815,  -4.8524,   0.9939,  -2.0074,\n",
      "           0.3791,  -2.4123,  -1.3717,  -2.3129],\n",
      "        [  1.8068,   1.0695,  -4.2726,  -0.9116,  -4.7391,   1.5285,   0.6129,\n",
      "          -2.2095,  -1.9314,   0.1806,   0.4134]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.940491199493408\n",
      "output: tensor([[  0.2020,  -1.7407,  -2.1614,  -3.0579,  -4.0603,   1.7146,  -2.9972,\n",
      "           0.3129,  -1.8062,   0.3250,  -0.6619],\n",
      "        [  0.2225,  -0.4729,  -1.8542,  -0.6491,  -1.8624,   2.1206,   1.3779,\n",
      "           0.7336,  -1.4891,  -0.3426,   3.1374],\n",
      "        [  0.5027,  -0.5277,  -2.7564,  -1.7082,  -3.2357,   1.8359,  -3.6819,\n",
      "           0.7714,  -0.6660,  -0.9974,  -1.4100],\n",
      "        [  0.8854,   0.5970,  -3.0744,  -1.8839,  -2.5248,  -0.0727,   0.7148,\n",
      "          -0.5418,  -4.3087,  -2.5268,   4.4612],\n",
      "        [ -6.5690,  -4.0565, -17.7591, -16.2393, -19.6455,   1.2256,  -6.8613,\n",
      "           2.2045,   1.2191,  -3.3675,  -2.3645],\n",
      "        [ -5.0479,   0.5957,  -9.1615, -10.7955,  -9.7104,  -3.0207,  -3.0202,\n",
      "          -2.2658,   0.2316,  -0.3156,   2.8173],\n",
      "        [  0.7361,   0.1389,  -2.4934,  -3.2517,  -4.0309,   1.5345,  -2.4331,\n",
      "           2.0166,  -0.9471,  -1.9099,  -1.2652],\n",
      "        [ -2.0947,  -1.8642,  -5.7677,  -5.2497,  -8.2813,   3.7447,  -3.9391,\n",
      "           2.8818,  -0.6571,  -1.6838,  -1.4901]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.16062331199646\n",
      "output: tensor([[-4.3501e-01, -1.2240e-01, -2.1352e+00, -1.8449e+00, -8.3611e-01,\n",
      "          1.5039e+00,  8.0765e-01,  5.6719e-01, -1.0244e+00, -6.2601e-01,\n",
      "          1.6495e+00],\n",
      "        [-2.1792e+00, -2.2336e+00, -7.1682e+00, -6.8247e+00, -9.7563e+00,\n",
      "          5.5554e-01, -5.1012e+00,  4.8338e-01,  9.1935e-01, -2.3379e+00,\n",
      "         -2.1420e+00],\n",
      "        [-1.2032e+00, -2.6884e+00, -9.0281e-01, -1.6406e+00, -3.5286e+00,\n",
      "          2.8036e+00, -1.1852e+00,  1.0588e+00, -2.8308e+00, -1.6513e+00,\n",
      "         -2.7820e-01],\n",
      "        [ 2.6867e-01, -9.9771e-01, -2.2748e+00, -2.5710e+00, -2.0757e+00,\n",
      "          2.2450e+00, -1.9302e+00,  2.0435e+00, -2.4591e+00, -2.7637e+00,\n",
      "          1.2413e+00],\n",
      "        [-3.8043e+00, -2.5670e+00, -9.4436e+00, -8.8986e+00, -1.2311e+01,\n",
      "          2.6074e+00, -8.9144e+00, -3.8301e-01,  2.3856e+00, -6.5658e+00,\n",
      "         -6.8748e-01],\n",
      "        [-5.6581e-02, -1.9622e+00, -1.3485e+00, -2.0137e+00, -3.7291e+00,\n",
      "          1.6299e+00, -1.8442e+00,  1.5847e+00, -4.3560e-04, -1.6883e+00,\n",
      "         -2.5351e-01],\n",
      "        [-4.2629e+00, -1.9460e+00, -3.6172e+00, -5.1919e+00, -8.9317e+00,\n",
      "          2.2287e+00,  4.5943e-01, -2.8129e+00,  3.9482e+00, -1.9999e+00,\n",
      "          7.5895e+00],\n",
      "        [-4.9754e+00, -2.1272e+00, -5.8711e+00, -6.1620e+00, -6.1630e+00,\n",
      "          3.1567e+00, -5.4289e+00,  4.1450e+00, -6.1391e-01, -2.6580e+00,\n",
      "         -1.0942e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 3.2955968379974365\n",
      "output: tensor([[ -0.9363,  -0.2803,  -8.7814, -10.7719,  -9.1872,   0.1807,  -5.1295,\n",
      "          -0.5171,   4.0350,  -0.9722,  -4.1354],\n",
      "        [ -1.6310,  -3.4116,  -6.0745,  -6.0904,  -6.6467,   0.7611,  -7.0331,\n",
      "           2.3822,   0.9399,  -2.8209,  -1.7318],\n",
      "        [  2.0709,  -1.5210,   2.6001,  -3.1675,  -2.7049,   1.8836,   1.5291,\n",
      "           1.2016,  -0.3321,  -6.7592,   4.3779],\n",
      "        [ -0.2954,  -0.7043,  -1.5553,  -3.4775,  -3.0944,   1.3700,  -3.3776,\n",
      "           2.8380,  -1.4683,  -3.6375,  -1.4179],\n",
      "        [-13.3010,  -1.6670, -13.2310, -15.1052, -16.8311,  -4.0293, -14.0012,\n",
      "          -3.5038,   3.5723,  -5.5374,  -0.2066],\n",
      "        [  0.8964,  -0.3976,  -0.3261,  -1.0277,  -2.2752,   2.5433,  -2.3061,\n",
      "           0.9355,  -0.1112,  -1.8811,   0.7768],\n",
      "        [ -2.0616,  -2.6513,  -3.0203,  -5.2596,  -5.4667,   1.2888,  -4.3249,\n",
      "           3.8298,  -1.0560,  -3.9250,  -1.0651],\n",
      "        [ -1.3218,  -3.0713,  -3.2074,  -3.5741,  -4.0507,   0.8895,  -3.5458,\n",
      "           2.0490,   1.3546,  -2.5262,  -0.4449]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.825314521789551\n",
      "output: tensor([[-3.0905e-01, -5.4773e-01, -4.8001e+00, -7.3013e+00, -7.9807e+00,\n",
      "         -2.7831e+00, -6.4407e+00,  4.2736e+00, -1.4959e-01, -5.8468e+00,\n",
      "         -4.9749e-01],\n",
      "        [-8.4125e-01,  6.2593e-01, -1.8633e+00, -3.7544e+00, -3.7858e+00,\n",
      "         -1.7063e+00, -3.7469e+00, -2.1648e-01,  4.1012e+00, -3.9123e+00,\n",
      "          8.9859e-01],\n",
      "        [ 1.2333e+00, -2.1117e+00,  2.3947e+00, -1.0660e-01, -2.1910e+00,\n",
      "          4.8384e+00, -2.7252e-01,  6.9429e-01,  3.1856e+00, -3.4906e+00,\n",
      "          1.2265e-01],\n",
      "        [-5.3870e-01, -1.2436e+00, -1.2796e+00, -5.7760e+00, -4.8069e+00,\n",
      "         -7.2806e-01, -8.1008e+00,  5.3587e+00, -3.4380e-03, -4.7263e+00,\n",
      "         -1.5223e+00],\n",
      "        [ 3.9762e-01, -4.1619e+00, -1.9034e+00, -5.1106e+00, -6.6134e+00,\n",
      "          1.8259e+00, -6.5568e+00,  2.2328e+00, -5.8445e-02, -4.3393e+00,\n",
      "          3.8481e-01],\n",
      "        [-4.8667e+00, -3.2066e+00, -9.7739e+00, -7.5459e+00, -1.0993e+01,\n",
      "          1.4208e+00, -5.3843e+00,  7.5290e-01,  4.4496e+00, -6.4273e+00,\n",
      "          4.6489e+00],\n",
      "        [-5.3963e-01, -3.1132e-01, -3.1370e+00, -5.7247e+00, -5.8928e+00,\n",
      "          2.3018e+00, -5.6555e+00,  3.2933e+00,  1.8246e-01, -4.9888e+00,\n",
      "         -1.0661e+00],\n",
      "        [-1.0705e+01, -4.7384e+00, -7.3636e+00, -1.1112e+01, -1.2077e+01,\n",
      "         -8.2840e-02, -9.7186e+00, -7.7725e-02,  5.4183e+00, -1.8812e+00,\n",
      "          4.2051e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 5.377538681030273\n",
      "output: tensor([[ -0.4330,   1.3575,  -4.4380,  -5.6821,  -5.2636,  -2.8330,  -3.9967,\n",
      "           0.6377,   0.9208,  -1.2277,  -0.6266],\n",
      "        [ -1.8485,  -0.4451,  -1.4023,  -2.8913,  -4.1422,  -0.6743,  -4.1225,\n",
      "           3.7217,   1.9071,  -4.4610,  -1.2254],\n",
      "        [ -2.1758,  -0.4199,  -7.3508, -13.0074, -11.1146,  -1.9880,  -9.6616,\n",
      "           3.1815,   5.2703,  -6.3600,  -2.2728],\n",
      "        [ -1.1904,  -1.6927,  -4.0008,  -5.3491,  -6.3477,  -0.7879,  -6.7795,\n",
      "           1.6340,   1.1231,  -1.7891,  -1.1913],\n",
      "        [ -6.8905,  -4.1437, -18.6447, -21.3232, -24.7164,   4.0961, -18.5740,\n",
      "           2.2673,   8.3932,  -4.1907,  -2.0076],\n",
      "        [  0.3473,  -0.3076,  -0.7397,  -2.6117,  -4.7016,  -1.2683,  -3.4124,\n",
      "           1.6525,   0.8124,  -4.3534,  -2.7192],\n",
      "        [ -2.4273,  -0.8875,  -5.7723,  -6.4449,  -8.1450,  -2.7642,  -7.9802,\n",
      "           2.0612,   0.5907,  -1.8340,  -0.7256],\n",
      "        [  2.2569,   1.2690,  -1.1782,  -4.2022,  -3.1484,  -0.3846,  -2.0774,\n",
      "           0.3965,   0.8728,  -2.8086,  -0.8309]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.897860050201416\n",
      "output: tensor([[ -1.8598,   0.4523,  -5.1800,  -6.9187,  -7.2774,  -4.3484,  -7.5529,\n",
      "           1.3309,  -0.2100,  -1.3752,  -1.5549],\n",
      "        [  0.8498,  -1.3953,  -0.7601,  -3.0194,  -1.6474,  -1.8652,  -2.9126,\n",
      "          -0.3148,   2.9893,  -2.3171,  -0.6601],\n",
      "        [ -0.4313,   0.2782,  -2.4014,  -3.6757,  -3.5411,  -1.7941,  -3.1616,\n",
      "           1.3298,   0.8551,  -2.4975,  -0.5811],\n",
      "        [ -0.5784,   0.6271,  -1.6443,  -2.9299,  -1.8279,  -1.1207,  -3.5372,\n",
      "           0.2145,   0.6568,  -0.9300,  -0.9840],\n",
      "        [ -0.2106,   1.1744,  -1.2020,  -3.1240,  -2.5682,  -0.5671,  -2.2156,\n",
      "           0.4769,   0.5601,  -1.2882,  -0.7177],\n",
      "        [ -3.3620,  -3.7615,  -4.0376,  -6.2746,  -7.3730,  -1.6928,  -6.3774,\n",
      "           4.6424,   1.2752,  -4.6608,  -2.3398],\n",
      "        [-14.5050,  -4.1603, -19.5270, -23.3542, -31.0512,  -2.1852, -17.2976,\n",
      "           4.4356,   4.7863,  -3.4526,  -4.5553],\n",
      "        [ -1.0751,  -0.5922,  -0.8803,  -8.2571,  -6.7620,  -1.0626,  -7.0960,\n",
      "           4.1671,   4.9358,  -3.5021,  -4.3150]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 1.2734102010726929\n",
      "output: tensor([[-1.6954e+00,  2.0301e-01, -2.6731e+00, -4.7388e+00, -5.4498e+00,\n",
      "         -4.4432e+00, -5.5801e+00,  1.1507e+00,  2.2763e+00, -2.4740e+00,\n",
      "         -1.1130e+00],\n",
      "        [-2.0683e+00,  1.2221e+00, -6.2630e-01, -8.2015e+00, -5.3465e+00,\n",
      "         -1.7667e+00, -1.1826e+01,  6.7927e+00,  2.8136e+00, -5.2138e+00,\n",
      "         -2.9747e+00],\n",
      "        [-3.0352e-02,  2.3700e-02,  1.0138e+00, -4.1353e+00, -3.6362e+00,\n",
      "         -2.9871e+00, -4.4130e+00,  3.0007e+00,  2.2767e+00, -2.7517e+00,\n",
      "         -1.7468e+00],\n",
      "        [-6.9450e+00,  2.2970e-01, -1.6149e+01, -2.8669e+01, -2.5880e+01,\n",
      "          2.4474e+00, -2.4712e+01,  6.4671e+00, -1.5005e+00, -5.6469e+00,\n",
      "         -4.4166e+00],\n",
      "        [-1.4426e+00, -2.2637e+00, -4.3285e+00, -7.0393e+00, -7.3531e+00,\n",
      "         -3.9926e+00, -6.8277e+00,  3.4503e+00,  1.1269e+00, -2.3317e+00,\n",
      "         -1.4780e+00],\n",
      "        [-9.2684e-02, -6.2571e-01, -6.7714e-01, -4.7925e+00, -4.7613e+00,\n",
      "         -4.1337e+00, -5.6618e+00,  3.2325e+00,  2.7899e+00, -3.7019e+00,\n",
      "         -2.1583e+00],\n",
      "        [-9.2946e-01,  1.1666e+00, -4.9972e+00, -5.0587e+00, -5.8098e+00,\n",
      "         -3.5256e+00, -3.8509e+00,  2.2459e-02,  8.4446e-01, -1.4276e+00,\n",
      "         -1.1340e+00],\n",
      "        [-1.2303e+00, -1.4666e-01, -3.0773e+00, -5.1209e+00, -5.3817e+00,\n",
      "         -2.9696e+00, -4.9315e+00,  1.1879e+00,  7.1862e-01, -1.8009e+00,\n",
      "         -8.5184e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 2.3836710453033447\n",
      "output: tensor([[ -2.4379,  -0.1821,  -4.2826,  -7.5407,  -9.6265,  -6.8439,  -7.6063,\n",
      "           3.4745,   1.9526,  -3.0706,  -4.0531],\n",
      "        [ -8.8363,  -0.7792,  -1.3119, -12.8940, -11.4118,   1.0329,  -9.6887,\n",
      "           3.6048,   0.5118,   0.1095,  -2.7502],\n",
      "        [ -2.5237,  -0.3493,  -1.2935,  -5.7090,  -6.5028,  -6.7121,  -6.6423,\n",
      "           2.7359,   1.0930,  -3.6346,  -1.7850],\n",
      "        [ -2.1370,   0.5856,  -4.3635, -10.1011,  -8.6392,  -4.8980,  -9.2316,\n",
      "           3.7407,   2.1220,  -4.4343,  -0.5562],\n",
      "        [ -5.8987,  -0.3173,  -3.6958,  -9.1375, -10.3648,  -2.4471, -11.4171,\n",
      "           5.5764,   3.8802,  -8.0444,  -6.5005],\n",
      "        [ -5.2451,  -1.8960,  -6.2234, -12.9562, -13.8164,   0.5321,  -9.2161,\n",
      "           2.6609,   0.1727,  -1.0811,   0.0891],\n",
      "        [ -2.1797,   1.3537,  -4.7647,  -5.8161,  -5.8379,  -1.3977,  -4.2838,\n",
      "           2.4756,   0.5706,  -1.5669,  -0.9222],\n",
      "        [ -2.3572,  -2.5134,  -1.9840,  -5.2384,  -6.7117,  -1.3176,  -7.3974,\n",
      "           5.2603,   1.4428,  -3.8073,  -3.0485]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.390674114227295\n",
      "output: tensor([[-2.4435e+00,  1.3639e+00, -3.9809e+00, -1.2889e+01, -1.2091e+01,\n",
      "         -1.5096e+00, -9.7247e+00,  3.8278e+00,  1.8701e+00,  2.2449e-01,\n",
      "         -8.5568e+00],\n",
      "        [-7.4901e+00,  1.9042e+00, -1.0897e+01, -1.6875e+01, -1.7625e+01,\n",
      "         -1.2581e+01, -1.5558e+01,  4.1198e+00,  1.3210e+00, -3.5187e+00,\n",
      "         -2.4089e+00],\n",
      "        [-2.4730e+00,  7.3635e-01, -1.0482e+00, -7.7255e+00, -8.6671e+00,\n",
      "         -7.1781e+00, -7.9995e+00,  2.5958e+00,  1.0408e+00, -3.1704e+00,\n",
      "         -3.3706e+00],\n",
      "        [-4.4418e+00,  1.3910e+00,  2.3847e+00, -6.6263e+00, -6.3059e+00,\n",
      "         -3.8450e+00, -7.5373e+00,  2.1403e+00, -8.1899e-02, -3.4445e+00,\n",
      "         -1.1591e+00],\n",
      "        [-5.8548e+00, -1.6214e+00, -3.6583e+00, -1.0552e+01, -9.8119e+00,\n",
      "         -3.5241e+00, -1.0035e+01,  2.8447e+00,  1.6684e+00, -1.6420e+00,\n",
      "         -2.4129e-01],\n",
      "        [-2.6551e+00,  1.7882e-01, -8.2171e-02, -3.4921e+00, -4.2336e+00,\n",
      "         -1.6761e+00, -3.6216e+00,  3.7192e+00,  7.7122e-01, -1.7066e+00,\n",
      "         -2.9735e+00],\n",
      "        [-2.7822e+00,  2.1339e+00, -2.3709e+00, -5.2104e+00, -6.1634e+00,\n",
      "         -2.5538e+00, -4.5492e+00,  1.5381e+00,  8.7435e-03, -1.5942e+00,\n",
      "         -2.1903e+00],\n",
      "        [-1.0488e+00,  2.0360e+00, -3.0462e+00, -5.7975e+00, -5.0506e+00,\n",
      "         -3.1027e+00, -5.8632e+00,  1.0339e+00,  2.7349e-01, -2.1216e+00,\n",
      "         -1.7178e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 2.0626418590545654\n",
      "output: tensor([[ -4.3809,   0.9983,  -6.0845, -10.1013, -10.4656,  -8.6461, -10.3050,\n",
      "           1.2876,   1.6822,  -1.1427,  -1.0156],\n",
      "        [ -4.2832,   1.1091,  -0.0444,  -4.9228,  -3.6934,  -3.6599,  -6.7261,\n",
      "           4.9005,  -0.0803,  -2.0535,  -4.9579],\n",
      "        [ -2.1801,   2.0033,  -3.9979,  -7.2755,  -8.9958,  -6.4755,  -6.2850,\n",
      "           0.7062,   1.3035,  -1.2634,  -1.8094],\n",
      "        [ -1.8184,   0.5921,  -3.0332,  -7.2566,  -8.1497,  -4.4148,  -7.8102,\n",
      "           4.1721,   0.4368,  -3.7317,  -5.2607],\n",
      "        [ -1.6769,   1.9035,   0.4658,  -7.0183,  -5.9773,  -3.2924,  -7.7309,\n",
      "           2.9930,   1.7139,  -2.8035,  -4.5345],\n",
      "        [ -6.9518,   3.2711,  -0.7519,  -7.4791,  -9.2820,   0.6653,  -8.4484,\n",
      "           3.7774,  -2.0468,   3.5487,  -4.8180],\n",
      "        [ -5.4127,   2.8534, -13.9742, -20.3178, -18.4834,  -4.4023, -21.6230,\n",
      "           1.1330,   0.5538,   2.8634,  -5.6826],\n",
      "        [ -3.9598,   3.0445,  -0.5909,  -8.0819,  -6.2122,  -5.3128,  -7.8845,\n",
      "          -0.1338,   1.5083,  -1.8048,  -1.7940]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.217262029647827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: Error detected in CudnnRnnBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/traitlets/config/application.py\", line 976, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2936, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3135, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2114612/497437324.py\", line 4, in <cell line: 3>\n",
      "    running_epochs(model,optimizer,criterion)\n",
      "  File \"/tmp/ipykernel_2114612/2446513367.py\", line 47, in running_epochs\n",
      "    train(model,optimizer,criterion)\n",
      "  File \"/tmp/ipykernel_2114612/2446513367.py\", line 17, in train\n",
      "    out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tmp/ipykernel_2114612/2564363434.py\", line 119, in forward\n",
      "    x = self.conv1(x, edge_index)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/dauin_user/lcolomba/projects/logml22-latent-graph-learning/models/gcn.py\", line 135, in forward\n",
      "    _, (x, _) = self.lin(x)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/torch/nn/modules/rnn.py\", line 769, in forward\n",
      "    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " (Triggered internally at  ../torch/csrc/autograd/python_anomaly_mode.cpp:102.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'CudnnRnnBackward0' returned nan values in its 1th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model, optimizer, criterion \u001b[38;5;241m=\u001b[39m set_model_parameters(model4, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mdetect_anomaly(): \u001b[38;5;66;03m# used for debugging\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mrunning_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mrunning_epochs\u001b[0;34m(model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrunning_epochs\u001b[39m(model,optimizer,criterion):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 47\u001b[0m         \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m         train_acc \u001b[38;5;241m=\u001b[39m test(train_loader, model)\n\u001b[1;32m     49\u001b[0m         test_acc \u001b[38;5;241m=\u001b[39m test(test_loader, model)\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)  \u001b[38;5;66;03m# Compute the loss.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Derive gradients.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update parameters based on gradients.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pyg2/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pyg2/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'CudnnRnnBackward0' returned nan values in its 1th output."
     ]
    }
   ],
   "source": [
    "# Experiment 4: GCN Baseline \n",
    "model, optimizer, criterion = set_model_parameters(model4, lr=0.001)\n",
    "with torch.autograd.detect_anomaly(): # used for debugging\n",
    "    running_epochs(model,optimizer,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLSTM(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GATLSTM, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConvLSTM(1, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, len(le.classes_))\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = GATLSTM(hidden_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2247591/923925910.py:3: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly(): # used for debugging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[-0.3059, -0.9168, -1.0832,  0.9137,  0.1868,  0.6608, -0.8938, -0.1464,\n",
      "         -1.0865, -0.7613,  0.9666],\n",
      "        [-0.0204,  0.0915, -0.2076,  0.0834, -0.8381, -0.2178,  0.1938, -0.4756,\n",
      "         -0.6720, -0.1423, -0.1076],\n",
      "        [ 1.0716,  0.0405, -0.5549,  0.4571,  0.1828,  0.9457, -0.0021,  0.0553,\n",
      "         -0.5997, -1.3675, -0.6173],\n",
      "        [ 0.0825,  0.3694, -0.2130,  0.1629, -0.6340, -0.3819, -0.0327, -0.2073,\n",
      "          0.4921, -0.1563, -0.6814]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.9380886554718018\n",
      "0tr) loss: 2.9380886554718018\n",
      "output: tensor([[-1.1442, -2.7622, -2.9753, -2.5535, -2.1439,  1.0067, -3.0903,  5.1159,\n",
      "          2.1677,  2.2098, -0.1411],\n",
      "        [-2.2889, -1.4060, -1.7546, -1.5181, -1.5843, -0.1571, -1.7316,  2.5619,\n",
      "          1.8949,  1.3041,  0.6121],\n",
      "        [-1.7447, -3.2308, -1.1994, -1.2180, -1.3713,  0.1350, -1.7294,  3.1666,\n",
      "          4.6408,  0.3419, -1.2955],\n",
      "        [-2.0169, -0.9090, -2.3761, -2.2190, -1.8485, -0.9052, -2.4023,  4.8069,\n",
      "          3.0772, -0.6524,  0.5945]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.025184631347656\n",
      "output: tensor([[-5.1420, -2.8620, -7.5808, -3.9558, -4.7488, -2.4588, -5.5314,  7.2234,\n",
      "          1.7160,  2.6563, -2.0858],\n",
      "        [-4.0359, -1.5117, -2.8133, -1.9364, -4.4909, -1.3622, -1.9520,  6.0657,\n",
      "          2.8693,  2.4678, -0.6601],\n",
      "        [-1.9503, -0.5567, -1.4221, -2.2279, -1.7433,  0.6712, -2.0800,  3.3476,\n",
      "          3.1878,  1.7368, -1.2856],\n",
      "        [-3.4690, -2.5532, -3.2042, -3.2590, -3.0263, -1.0470, -2.5401,  6.4392,\n",
      "          2.9594,  3.9203, -0.5757]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.3337464332580566\n",
      "output: tensor([[-0.4684, -0.7825, -6.0065, -3.9325, -4.4980, -2.9531, -6.4603,  9.4149,\n",
      "          1.3998,  3.8353, -1.9893],\n",
      "        [-1.5021,  0.2547, -2.6713, -1.7498, -1.9698, -2.1925, -1.6149,  3.9702,\n",
      "          0.9842,  1.2702, -1.2554],\n",
      "        [-2.2426, -0.0762, -5.1715, -3.3738, -2.3994, -0.7906, -4.0926,  6.7165,\n",
      "          1.8652,  7.8403, -4.1450],\n",
      "        [-5.1595, -0.5810, -4.1454, -2.9781, -3.4525,  1.3097, -3.6556,  4.4763,\n",
      "          0.6556,  3.2397, -1.7714]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.3565213680267334\n",
      "output: tensor([[-1.4047e+00, -9.2330e-01, -2.0321e+00, -2.0648e+00, -2.7661e+00,\n",
      "         -1.3033e+00, -2.5744e+00,  2.6591e+00, -4.6022e-01,  2.4900e+00,\n",
      "         -1.7832e+00],\n",
      "        [-9.6468e-01,  2.2886e+00, -3.4280e+00, -3.1223e+00, -3.2657e+00,\n",
      "          7.1444e-01, -3.9951e+00,  5.8147e+00,  2.0497e+00,  8.1900e-01,\n",
      "         -2.6976e+00],\n",
      "        [-2.7279e+00,  4.5650e-03, -4.2650e+00, -5.2678e+00, -5.7962e+00,\n",
      "         -2.7237e+00, -5.1780e+00,  1.2539e+01,  8.0795e-01,  4.3782e+00,\n",
      "         -4.7046e+00],\n",
      "        [ 2.2872e+00,  1.1567e+00, -3.1898e+00, -4.2250e+00, -2.3114e+00,\n",
      "         -2.1812e+00, -5.3535e+00,  1.9537e+00,  1.0532e-02,  3.5367e+00,\n",
      "         -2.6209e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 6.242293357849121\n",
      "output: tensor([[-6.6259, -0.6837, -6.2421, -7.0143, -3.0474, -6.4992, -5.8781,  3.3091,\n",
      "          3.3171,  1.9867, -3.2678],\n",
      "        [-2.0143, -3.3890, -4.0536, -4.1139, -3.6162, -2.7388, -6.3288,  5.1377,\n",
      "          0.7450,  4.5215, -0.2270],\n",
      "        [-1.5337, -0.0233, -5.0738, -3.0149, -2.1977,  0.0993, -3.1371,  3.1787,\n",
      "          0.5195,  3.1293, -1.8417],\n",
      "        [-0.8485, -1.7458, -5.3772, -3.7575, -3.0532, -1.1341, -4.4851,  4.0437,\n",
      "          2.0053,  1.5367, -0.8041]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.4222702980041504\n",
      "output: tensor([[ 0.3539, -1.7691, -4.3388, -1.6047, -3.2645,  1.2654, -2.4641,  1.7933,\n",
      "          2.0603,  0.2263,  1.0883],\n",
      "        [-1.3960, -2.8316, -6.7348, -4.7435, -2.8017, -1.5683, -6.3661,  0.3682,\n",
      "          7.9818,  0.3511, -2.1350],\n",
      "        [-2.5886, -0.5607, -6.9995, -6.8677, -3.8545, -3.1889, -6.5306, -0.8490,\n",
      "         10.7368, -1.0201, -1.8826],\n",
      "        [-3.0753, -3.3218, -7.2444, -4.2116, -2.0508,  0.2762, -5.0998,  0.2557,\n",
      "          4.4146, -0.8295, -0.3375]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.172621726989746\n",
      "output: tensor([[ 0.4110, -1.4423, -3.2400, -4.6715, -2.8567,  1.7797, -3.9974, -0.2646,\n",
      "          4.3196, -2.2227, -0.7828],\n",
      "        [-3.7122, -1.0591, -8.7527, -6.4791, -6.0231,  0.6354, -6.5976, -2.5239,\n",
      "          5.1111, -3.3176, -1.6009],\n",
      "        [ 0.1144, -3.0974, -2.9708, -2.1009, -0.3746, -2.4681, -3.5377, -1.2068,\n",
      "          7.5955, -3.6654, -2.3527],\n",
      "        [ 0.4600,  0.0857, -3.5672, -2.9465, -0.8512, -0.0364, -5.3780, -4.6582,\n",
      "          9.4685,  1.1911, -3.8828]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.7216122150421143\n",
      "output: tensor([[ -2.7490,  -1.6643,  -1.4549,  -1.3641,   0.1874,   3.3955,  -1.8783,\n",
      "          -0.3115,   2.2990,  -0.9820,   0.9527],\n",
      "        [ -3.2796,  -0.5949,  -8.5559,  -6.5928,  -4.5619,   3.5307,  -7.6107,\n",
      "           0.5229,   3.6239,  -3.4511,  -3.1917],\n",
      "        [ -1.1086,  -0.4556,  -5.5335,  -5.9617,  -2.5482,   1.5524,  -6.5740,\n",
      "          -5.8839,   5.1954,   0.8257,  -2.1354],\n",
      "        [ -1.7318,  -0.5151,  -5.8689, -12.1346,  -6.1085,  -3.5289,  -5.1008,\n",
      "          -7.4825,  17.1739,  -9.4162,  -9.2075]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 9.181248664855957\n",
      "output: tensor([[-0.7116,  2.4234, -3.3900, -3.2197, -1.7512,  1.9719, -3.6751, -5.3228,\n",
      "          1.8029, -1.7336, -0.7752],\n",
      "        [ 0.3478,  2.2037, -3.2246, -2.3359, -1.8646,  0.6618, -2.9894, -4.0869,\n",
      "          5.0660, -5.5174, -0.8924],\n",
      "        [-2.3784, -0.1684, -8.1323, -6.0675, -2.7496,  5.4473, -9.1805, -1.0501,\n",
      "          0.8040,  0.1734, -6.8921],\n",
      "        [-0.0443,  2.6838, -0.4446, -2.4681, -1.6096,  0.8908, -3.5178, -5.9107,\n",
      "          6.0408, -3.7873, -0.6936]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.3005571365356445\n",
      "output: tensor([[-2.2610,  4.5851, -2.0912, -3.0958, -2.3884,  6.5874, -7.8057, -3.0673,\n",
      "         -8.1193,  3.3564,  2.6600],\n",
      "        [ 0.6289,  4.4085, -8.0488, -3.8328, -6.8730,  1.1674, -9.5871,  2.6670,\n",
      "         -2.6184, -3.1085, -1.8121],\n",
      "        [-2.4738,  4.7770, -4.1984, -4.1120, -1.9367, -3.7407, -5.1790, -3.7025,\n",
      "          5.7336, -3.5901, -3.3376],\n",
      "        [-2.5196,  2.3813, -6.6475, -4.5597, -4.9349, -1.1022, -6.3065, -3.4832,\n",
      "          4.7076, -3.6705, -3.0527]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 7.228883743286133\n",
      "output: tensor([[ -2.2222,   6.0848,  -7.7170,  -2.4268,  -5.0746,  -3.5139,  -7.7942,\n",
      "           6.0308,  -3.6772,  -6.9011,  -2.8489],\n",
      "        [ -1.7988,   8.0663,  -1.9576,  -1.1980,   2.2273,  -0.6154,  -0.4122,\n",
      "          -3.1486,   4.9577,  -3.8725,  -3.6844],\n",
      "        [ -1.4506,   2.9842,  -0.9153,   0.0790,  -0.3685,  -0.1873,  -1.1655,\n",
      "          -2.1802,   0.0584,  -1.4200,  -0.0383],\n",
      "        [ -2.8167,   2.4347,  -8.6038,  -7.1497,  -3.7191,   1.5092, -10.4817,\n",
      "           0.2498,  -8.3415,   3.5291,  -0.4322]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.6917147636413574\n",
      "output: tensor([[-3.3835,  8.1126, -0.6261, -0.3909,  0.7370, -4.3921, -1.7636, -0.6109,\n",
      "          1.1957, -7.5960, -5.8486],\n",
      "        [-4.0145,  8.1539, -7.6858, -5.7613, -6.4476, -0.6594, -8.8199,  4.5893,\n",
      "         -8.1660, -0.6235,  1.2573],\n",
      "        [-2.5311,  2.4254, -4.0343, -3.0565, -1.8523,  0.8240, -4.0020,  0.3089,\n",
      "         -0.9300, -1.6394,  0.9648],\n",
      "        [-4.8013,  2.7209, -4.2070, -3.7083, -2.1395, -1.5245, -4.0941,  6.8217,\n",
      "         -3.2437, -3.1048, -1.2096]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.42377233505249\n",
      "output: tensor([[ -0.3706,   9.4547,   2.5652,  -0.1442,   0.7423,  -2.2129,   0.3196,\n",
      "          -4.2106,   0.5097,  -5.8008,  -2.5226],\n",
      "        [ -2.5495,   3.4494,  -7.0544,  -4.7661,  -4.3646,  -3.7013,  -6.0434,\n",
      "           5.4804,  -4.5984,  -1.7069,  -0.8684],\n",
      "        [ -1.0594,   4.0200,  -4.1632,  -2.3929,  -2.6794,  -2.5888,  -4.3123,\n",
      "          -6.4695,   0.7094,   3.6446,   0.5751],\n",
      "        [-10.6069,   0.5316, -14.5389,  -7.8183,  -8.6411, -13.1443, -11.9728,\n",
      "          14.0896,  -2.8024,  -5.2409,  -2.8916]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 9.346529960632324\n",
      "output: tensor([[ -1.0195,  -0.5354,  -2.0784,  -1.3706,  -2.8845,  -2.7349,  -3.6072,\n",
      "           0.5939,   0.0456,  -2.0398,   1.6560],\n",
      "        [  0.0856,   9.9148,  -2.7273,  -4.6379,  -0.7739, -11.2929,  -3.4641,\n",
      "           1.9970,  -1.6574,  -8.5484,  -4.0426],\n",
      "        [ -4.8408,  -0.2085,  -1.8627,  -0.7557,  -1.6194,  -3.7744,  -4.0144,\n",
      "           8.4376,  -6.4632,  -3.5408,  -0.1273],\n",
      "        [ -6.0805,   1.6809,  -4.1068,  -4.2908,  -2.2068,  -4.5226,  -4.5159,\n",
      "           2.5359,  -5.4774,  -0.5662,  -2.2152]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 7.421311378479004\n",
      "output: tensor([[ -3.7724,   0.7726,  -3.5711,  -2.2837,  -2.5466,  -6.1034,  -3.8102,\n",
      "           3.6456,  -3.3074,   1.4437,  -2.5348],\n",
      "        [ -3.5323,  -1.6832,  -4.6655,  -1.2898,  -4.4562,  -7.7602,  -5.7586,\n",
      "           8.7992,  -4.2339,  -2.1363,  -0.4604],\n",
      "        [ -4.9354,  -2.2473,  -4.7533,  -4.0278,  -4.9367,  -8.3402,  -7.4862,\n",
      "           7.4442,  -0.9517,   0.3742,  -0.1522],\n",
      "        [ -9.8879,  -0.2221,  -7.6591,  -8.6990, -10.6768,  -4.1076,  -9.2542,\n",
      "           4.3112,   0.6537,  -3.2794,  -7.3961]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.66623592376709\n",
      "output: tensor([[ -6.9269,  -0.2257,  -0.0557,  -3.8871,  -5.1785, -12.8711,  -8.5324,\n",
      "          10.5381,  -5.0045,  -4.1466,   1.7832],\n",
      "        [ -7.9721,  -4.4116,  -6.6319,  -5.9893,  -8.6282,  -4.9306,  -7.9004,\n",
      "           1.4711,   2.8333,   1.2017,  -0.6277],\n",
      "        [ -9.1140,  -3.8793,  -4.9588,  -5.6680,  -8.0149, -11.5840,  -8.9223,\n",
      "          13.4909,  -5.1226,   1.9567,  -6.9670],\n",
      "        [ -3.6323,  -4.9300,  -3.7823,  -2.8699,  -4.2721,  -4.6031,  -5.2850,\n",
      "           3.9790,   0.6384,   2.1373,   0.8484]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 1.4783425331115723\n",
      "output: tensor([[ -3.4941,  -2.9703,  -2.4712,  -3.5624,  -5.4129,  -4.4881,  -4.3891,\n",
      "           5.2637,  -1.3082,  -0.3152,   1.3687],\n",
      "        [ -2.9288,  -2.3451,   1.7067,   0.2652,   1.4844,  -6.7656,  -1.5739,\n",
      "           2.1281,  -0.4997,   2.9793,   0.0743],\n",
      "        [ -1.1906,  -4.2506,  -9.2111,  -9.3680, -12.2384, -11.1777,  -5.2170,\n",
      "          -1.7363,  -1.0771,  10.6084,  -7.3623],\n",
      "        [ -8.0874,  -3.1709,  -8.3148,  -7.5636,  -8.2097, -11.9381, -12.5057,\n",
      "          14.6775,  -6.4316,   0.8604, -10.2351]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 5.929129123687744\n",
      "output: tensor([[ -5.1220,  -3.8801,  -7.0288,  -8.1199,  -8.6375,  -7.8302,  -6.5721,\n",
      "           3.2428,  -0.9351,   3.2243,  -1.8058],\n",
      "        [ -2.5662,  -3.8017,  -2.0112,  -2.9480,  -3.6744,  -2.9008,  -3.8235,\n",
      "           3.1331,  -0.2584,   0.9830,  -0.1585],\n",
      "        [-13.2006,  -8.6181,  -7.0753,  -6.3417, -10.3507, -12.2009, -13.7786,\n",
      "          12.4676,  -6.8694,   4.4550,  -3.5650],\n",
      "        [ -2.7419,  -5.9178,  -1.9005,  -4.6037,  -9.0881, -15.0198,  -8.4637,\n",
      "           5.1758,   1.2100,  -0.1434,   0.1987]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.070774555206299\n",
      "output: tensor([[ -3.5856,  -4.5065,  -4.1514,  -3.5780,  -6.0656,  -3.7578,  -3.2440,\n",
      "           2.9277,  -1.1989,   3.0807,  -0.6785],\n",
      "        [-11.9312,  -6.7769,  -9.8685,  -8.8153,  -9.3713, -16.5467, -12.8326,\n",
      "          19.2994,  -8.3950,   1.3461,  -7.1638],\n",
      "        [ -3.5676, -12.0212,  -4.8733,  -3.5162,  -5.1849,  -4.5892,  -4.0411,\n",
      "          -2.0968,   0.6343,   8.2229,   4.3023],\n",
      "        [ -7.5115,  -4.3108,  -1.2173,  -2.3189,  -6.0760, -13.8366,  -5.3622,\n",
      "           5.6803,   0.3956,   1.8334,  -1.7616]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.567687034606934\n",
      "output: tensor([[ -6.4432, -11.9495, -10.5943,  -3.8798,  -7.0447,  -8.2177,  -6.5912,\n",
      "           2.0117,   3.6733,   7.1937,  -1.9955],\n",
      "        [ -4.8319,  -6.2289,  -5.1493,  -3.8857,  -7.1057,  -8.7583,  -7.7115,\n",
      "           4.5157,  -3.4818,   8.4758,  -2.3287],\n",
      "        [ -0.6737,   0.9282,   6.3311,   0.4336,  -0.4594,  -9.9268,   0.0186,\n",
      "          -0.0242,  -0.1757,   0.0819,   2.2676],\n",
      "        [ -7.9045,  -4.6451,  -1.7443,  -8.5557,  -8.8738, -14.2593,  -6.8583,\n",
      "           8.7294,  -7.3287,   3.4138,  -3.4135]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 8.697072982788086\n",
      "output: tensor([[-9.4333e+00, -8.4257e+00, -1.3497e+01, -1.1509e+01, -1.3139e+01,\n",
      "         -1.2676e+01, -1.6005e+01,  7.8437e+00, -3.2729e+00,  6.4641e+00,\n",
      "          1.5725e+00],\n",
      "        [-3.2686e+00, -3.8442e+00,  1.0752e+00, -6.1294e-01, -3.9459e+00,\n",
      "         -7.7524e+00, -1.7450e+00, -2.1429e+00,  4.6737e+00, -4.6887e-01,\n",
      "         -3.1738e+00],\n",
      "        [-6.9253e+00, -3.2929e+00, -1.2964e+00, -4.2483e+00, -1.0399e+01,\n",
      "         -1.7092e+01, -5.2340e+00,  1.0694e+01, -3.3438e+00,  2.7872e+00,\n",
      "         -4.2747e+00],\n",
      "        [-9.5313e-02,  1.0943e-01, -3.6475e-01, -2.1915e+00, -3.6267e+00,\n",
      "         -6.8066e+00, -8.2917e-01, -2.0570e+00,  4.4793e+00,  4.4544e-03,\n",
      "         -1.7272e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 6.914485454559326\n",
      "output: tensor([[ -2.3405,  -1.7736,  -0.7322,  -1.3865,  -4.0915,  -8.6902,  -4.4660,\n",
      "          -0.8042,   2.4536,   1.4018,  -2.1714],\n",
      "        [ -0.8565,   0.5063,   1.9802,  -0.3175,  -0.9313,  -2.5071,   0.2146,\n",
      "          -0.3659,   1.8774,   0.3057,  -2.5648],\n",
      "        [ -4.4204, -16.4747,  -4.8036,  -7.5505,  -5.9671, -11.1506,  -1.8656,\n",
      "           0.5821,   3.5239,   3.9753,   3.7678],\n",
      "        [-13.0190,  -6.6112,  -9.7547, -17.5603, -15.1920, -28.4440, -14.9471,\n",
      "          15.9582,  -4.0627,   9.4552,  -5.8093]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 10.362051010131836\n",
      "output: tensor([[ -0.5917,  -3.3241,  -0.9896,  -2.8938,  -4.4637,  -3.7238,  -2.8269,\n",
      "           1.0052,   2.2344,  -3.3391,  -1.6654],\n",
      "        [ -5.0580,  -0.1179,  -3.9597,  -3.2789,  -5.9243,  -9.3807,  -4.1165,\n",
      "           3.4419,  -0.7921,   3.2196,   2.8001],\n",
      "        [ -1.0637, -10.6182, -11.2673,  -6.6000, -10.6182,  -8.0477,  -7.5909,\n",
      "           3.0193,   4.1657,   1.3565,   3.6737],\n",
      "        [ -9.4085,  -7.6021,  -8.0193, -11.0549,  -8.6631, -22.3850, -14.1496,\n",
      "           6.6747,   3.1756,   5.6988,  -1.8418]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.597890615463257\n",
      "output: tensor([[ -4.8089,  -1.5947,  -2.9274, -13.2605, -14.4831, -20.8787,  -8.1845,\n",
      "           3.4275,   3.3524,  -0.4369,  -4.8055],\n",
      "        [ -2.0835,   6.7703,  -2.1213,  -3.3801,  -3.8905,  -9.2334,  -4.7831,\n",
      "          -1.6072,   2.4829,   1.2214,  -1.7296],\n",
      "        [ -3.2780,  -4.5138,  -6.0963,  -3.9685,  -6.3347,  -3.5470,  -5.3230,\n",
      "           0.8530,   2.0004,  -0.2570,  -0.3159],\n",
      "        [ -8.2782, -11.9619, -14.4531, -11.7984, -14.9359,  -8.6051, -11.3046,\n",
      "           0.8706,   4.3404,   1.2474,   1.3360]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 13.021728515625\n",
      "output: tensor([[  1.4698,  -4.5054,  -5.9394,  -4.7112,  -6.2850,  -5.0203,  -6.7458,\n",
      "           1.8926,   2.6065,  -3.7809,   0.1947],\n",
      "        [ -4.1019,  -8.6050,  -6.3240,  -5.1697, -11.4595,  -7.9260,  -6.9156,\n",
      "           1.7687,  -2.5815,   2.5791,  -3.1958],\n",
      "        [ -4.3426,   5.2779,  -9.7679, -11.8714, -11.6041,  -9.9429, -12.5826,\n",
      "          -1.9546,   1.2020,   1.7670,  -6.2410],\n",
      "        [ -1.3631,  -2.3442,  -1.4174,  -3.1960,  -3.1152,  -0.9644,  -2.9651,\n",
      "           0.2257,  -0.8823,  -2.3830,  -1.0539]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 1.6413793563842773\n",
      "output: tensor([[ -0.6838,   0.3656,  -5.2119,  -4.6970,  -3.7925,  -3.0208,  -6.5933,\n",
      "          -1.1153,   1.8632,  -2.0420,  -2.1162],\n",
      "        [  2.1915,  -7.7756, -10.7812,  -7.2752,  -7.6418,  -4.9670, -11.0782,\n",
      "           5.4482,  -1.1580,  -4.6184,  -0.7950],\n",
      "        [ -0.6827,   2.7382,  -0.4206,  -2.1264,  -3.4551,  -3.2672,  -3.3143,\n",
      "          -1.3780,  -0.2715,  -2.6253,   1.1785],\n",
      "        [  1.5828,  -4.2548,  -9.5558,  -8.1241,  -9.5153, -10.4797, -10.8123,\n",
      "          -0.5026,   4.0698,   1.7942,  -3.2413]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.013683795928955\n",
      "output: tensor([[  1.3111,  -6.5299,  -6.0121,  -6.2643,  -9.8640,  -5.8222,  -5.2577,\n",
      "           1.5996,  -0.2828,  -0.8790,  -2.7499],\n",
      "        [ -1.7282,   2.5308,  -9.0093, -12.8770, -10.4678,  -9.5045, -11.9263,\n",
      "          -0.1266,   0.9904,  -3.9083,  -4.2752],\n",
      "        [  9.4946,  -5.8489,  -9.3578,  -6.5410,  -9.5285,  -3.5782,  -8.6016,\n",
      "           1.7470,  -0.8747, -10.7468,   1.1263],\n",
      "        [ -3.2770,   1.6000,  -8.9226,  -8.1505,  -8.9965,  -2.6423,  -7.9974,\n",
      "          -3.0692,   2.9590,  -4.5146,   3.1176]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.061298370361328\n",
      "output: tensor([[ -3.9348,   7.5045,  -3.7901, -12.2309,  -8.7475,   0.7147, -10.1910,\n",
      "          -1.7468,  -0.1279,  -8.0248,  -6.6775],\n",
      "        [  0.7610,   0.2977,  -6.4854,  -5.1856,  -6.0669,  -5.6682,  -6.9598,\n",
      "          -3.2288,   3.8797,  -1.3709,  -0.7488],\n",
      "        [ -1.3831,  -3.5411, -10.8898,  -7.0156,  -8.2917,  -0.4394,  -9.6039,\n",
      "           1.5498,  -0.3804,  -4.8072,   3.3576],\n",
      "        [  8.9425,  -9.9213, -10.0893,  -6.3449,  -8.3946,  -0.7052,  -7.6003,\n",
      "           3.4683,   1.0640,  -4.8495,   3.5767]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 1.9817616939544678\n",
      "output: tensor([[  1.6926,  -2.8358,  -7.1105,  -3.3482,  -6.6482,  -0.3973,  -4.5782,\n",
      "           1.3196,   1.2061,  -7.0186,   2.6942],\n",
      "        [ -0.6509,  -0.4339,  -2.8665,  -4.7722,  -4.3908,  -1.8032,  -6.5029,\n",
      "           2.6162,   0.7271,  -3.1646,  -3.6171],\n",
      "        [  6.7302, -14.9189,  -9.6332,  -5.3842,  -7.9528,   2.2956,  -6.7402,\n",
      "           3.1242,  -0.3698,  -5.8316,   6.0661],\n",
      "        [ -1.1436,  -1.1467,  -9.5710, -10.4430, -12.2814,  -1.7473, -13.8526,\n",
      "          -5.1940,  -0.0607,  -6.8959,   2.5086]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 5.510861396789551\n",
      "output: tensor([[  7.1332,  -9.0815,  -8.1096,  -4.0611,  -7.2943,   1.3573,  -7.6540,\n",
      "           2.1405,  -0.2631,  -5.5270,   4.3495],\n",
      "        [  0.2699,  -7.3983, -11.3332,  -4.6683,  -7.4634,   2.4800,  -8.4906,\n",
      "           3.3593,   3.5099,  -7.5396,   3.2279],\n",
      "        [ -2.2354,  -0.1437,  -4.9826,  -6.5185,  -4.2601,   0.9287,  -8.9877,\n",
      "           0.4441,   1.9328,  -3.8638,  -3.3961],\n",
      "        [  2.0901,   0.7901, -11.8562,  -7.3021, -10.7353,   0.4608, -13.2339,\n",
      "          -0.4616,   3.1849,  -9.7069,  -0.7857]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.600191593170166\n",
      "output: tensor([[ -2.2500,  -4.4244,  -5.7579,  -5.4275,  -7.5537,  -0.1704,  -8.2829,\n",
      "           1.1611,  -0.7770,  -4.1053,  -2.3189],\n",
      "        [  4.7653, -11.5253, -13.3146, -12.6082, -14.8724,   4.9089, -11.0905,\n",
      "           4.3201,  -2.5742,  -8.5973,   4.7608],\n",
      "        [ -2.9894,   0.8337,  -4.8923,  -5.4180,  -5.9587,   1.4734,  -6.5849,\n",
      "          -1.8916,  -0.2362,  -3.9390,  -0.1469],\n",
      "        [ -3.3107,  -1.7786, -12.4325, -10.7392, -10.1995,  -1.7402, -11.6262,\n",
      "          -0.1219,   4.1906,  -5.7692,  -4.5127]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.5078635215759277\n",
      "output: tensor([[ -3.9183,  -8.0595, -14.2021, -15.9850, -18.4907,   7.2990, -17.6656,\n",
      "           0.5964,   2.3187, -11.3943,   1.1262],\n",
      "        [ -1.4731,  -3.0487,  -8.6664,  -6.6274,  -7.9273,  -0.2274,  -8.5551,\n",
      "           2.3548,   1.0891,  -6.0337,  -1.9719],\n",
      "        [ -0.1699,  -2.1713,  -6.5109,  -2.9636,  -4.1439,   1.4420,  -5.1675,\n",
      "           0.8300,   0.5640,  -4.1049,   2.3978],\n",
      "        [  3.3523,  -6.9811, -12.1805,  -3.2435,  -6.2776,   2.7027,  -9.7805,\n",
      "           2.7117,   0.6501, -12.7174,  11.9079]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 9.353324890136719\n",
      "output: tensor([[ -6.7033,  -3.4031, -11.0779,  -7.7837,  -8.9351,  -1.4451, -15.2132,\n",
      "           7.8425,   4.3008,  -8.0875,  -7.9527],\n",
      "        [ -1.7148,  -4.4377, -13.3109,  -8.6626, -11.5972,  -0.1336, -14.9140,\n",
      "           5.2154,  -1.4404,  -8.1334,   4.6773],\n",
      "        [ -0.9204,  -6.8433, -10.6255,  -9.0104, -10.4845,  -0.2367, -10.6643,\n",
      "           1.4843,   3.0573,  -9.2746,   1.2828],\n",
      "        [ -3.9670,  -1.3153,  -5.4610,  -3.5404,  -5.0270,   1.3914,  -6.9336,\n",
      "           2.4973,   0.6642,  -3.5705,   1.2136]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 5.998846530914307\n",
      "output: tensor([[ -2.3558,  -0.1931,  -4.5820,  -4.3638,  -3.5711,  -0.3614,  -4.6929,\n",
      "           1.2674,   0.1951,  -2.7397,  -0.2329],\n",
      "        [ -2.0464,  -2.6002, -10.0828,  -7.8296,  -8.2926,   0.0408,  -9.4895,\n",
      "           1.4749,   0.1520,  -4.8531,   2.4377],\n",
      "        [ -6.5553, -14.2078, -20.6198,  -9.1369, -12.6380,   4.6862, -14.8915,\n",
      "           1.4137,   3.3104, -11.4820,   8.4369],\n",
      "        [ -6.2457,  -3.6391,  -8.8069, -13.3093, -10.2864,   3.3983, -13.2426,\n",
      "           1.3675,   2.7784,  -8.3816,  -2.0469]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.420397758483887\n",
      "output: tensor([[-11.4803,  -2.3834, -12.0275,  -7.9952, -10.9231,  -1.5977, -15.5329,\n",
      "           6.1146,   2.3798, -10.3818,  -5.9085],\n",
      "        [ -6.4655,  -7.3281, -12.2651,  -8.7105,  -7.2770,  -0.9483,  -9.0325,\n",
      "           1.7786,   3.8106, -11.1362,   9.0862],\n",
      "        [ -5.1438,  -0.2051,  -5.4150,  -5.9165,  -4.6333,  -1.3586,  -8.7555,\n",
      "           2.0093,  -0.7578,  -2.8711,   0.8446],\n",
      "        [ -3.1995,   0.1480,  -5.4592,  -6.0389,  -5.5791,  -1.5916,  -6.9993,\n",
      "           0.9321,   0.7256,  -2.8576,   0.9443]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 6.976693153381348\n",
      "output: tensor([[ -4.2481,   0.3669, -17.0798, -12.6159, -12.6116,  -3.3476, -12.3479,\n",
      "           2.2726,  -0.2607,  -2.8801,  -1.8623],\n",
      "        [ -6.2926,   1.1583,  -3.7675,  -3.8138,  -5.2454,   0.3077,  -6.2203,\n",
      "           2.1425,  -1.1429,  -5.2538,   0.2205],\n",
      "        [ -6.9739,  -6.8964, -11.8201,  -7.6917, -11.6076,   2.2203,  -9.1582,\n",
      "          -0.0420,   3.5604,  -6.7984,   4.6280],\n",
      "        [ -4.0909,   2.4039, -10.7491,  -2.8946,  -4.1155,  -4.1782,  -7.8054,\n",
      "           1.7226,   5.8083,  -3.3796,  -3.8180]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.4647040367126465\n",
      "output: tensor([[ -3.0762,   1.3690,  -3.3293,  -2.7253,  -2.9998,   0.3437,  -3.7873,\n",
      "          -1.5935,   1.5099,  -0.9260,  -0.3987],\n",
      "        [ -7.3540,   1.9372,  -8.4125,  -9.1851,  -8.3266,  -3.4013,  -9.9132,\n",
      "          -1.4748,   1.8937,  -1.9181,   1.3649],\n",
      "        [ -9.1644,  -0.3451,  -9.8283,  -8.4284,  -9.1091,  -2.9168, -11.1406,\n",
      "           3.3441,   2.2365,  -5.3099,  -5.1331],\n",
      "        [ -6.3367,  -4.2832, -17.1828, -10.6288, -10.7431,   0.7093, -18.8014,\n",
      "           1.2747,   4.0869, -11.1556,  -2.4576]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.468149185180664\n",
      "output: tensor([[-13.1895,  -4.5870, -17.6994,  -8.2353, -12.0895,   1.1426, -13.4335,\n",
      "           3.5156,   6.4562,  -9.5067,  -7.2844],\n",
      "        [ -5.2482,   0.2107,  -3.2783,  -4.1590,  -4.5378,  -0.5393,  -4.9357,\n",
      "          -0.8417,   0.8710,  -0.2737,  -0.9270],\n",
      "        [ -7.4551,   1.8883, -10.5107,  -9.2847,  -9.2405,  -2.1091, -10.7928,\n",
      "           1.1132,   0.7069,  -0.0793,  -1.7665],\n",
      "        [ -8.5880,   4.3447,  -6.1225,  -6.8443, -10.3775,  -6.0990,  -9.2757,\n",
      "           2.1367,  -0.9043,  -1.7479,  -2.0954]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 5.249145030975342\n",
      "output: tensor([[-10.1528,   2.1220, -15.3125, -13.7652, -13.7675,  -5.0146, -16.2576,\n",
      "           0.4834,   1.8821,   1.8575,  -5.8679],\n",
      "        [ -6.7742,  -3.5372, -11.0703,  -6.0823,  -6.3134,   1.9013,  -9.3576,\n",
      "           3.2268,   3.5671, -10.9633,  -6.4612],\n",
      "        [ -4.7540,   0.9790,  -3.6583,  -5.1709,  -3.3328,  -2.7588,  -5.7612,\n",
      "           1.9235,   0.3245,  -0.4793,  -2.4917],\n",
      "        [-12.9904,   7.8808,  -8.1414, -10.3481,  -8.8902,  -6.6845, -11.6933,\n",
      "           2.7943,  -3.2935,  -1.8665,  -8.8013]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.074274063110352\n",
      "output: tensor([[-12.6330,   4.1415,  -9.3939, -12.4424,  -8.3457,  -6.3667,  -8.8073,\n",
      "           1.9816,  -0.9985,   0.1413,  -3.2674],\n",
      "        [ -3.1453,   3.6038,  -3.6226,  -4.4614,  -5.0722,  -1.7406,  -4.9927,\n",
      "          -0.6050,   0.3138,   1.5374,  -3.8571],\n",
      "        [-13.5086,  -2.5008, -12.6716,  -4.8320,  -6.6316,   0.7700,  -8.1580,\n",
      "           3.5421,   4.2748,  -3.0065,  -8.1859],\n",
      "        [ -9.0950,   2.2145, -10.2042,  -8.6912,  -9.2074,  -3.4350,  -9.5085,\n",
      "           4.6551,  -1.2407,  -3.5471, -11.3052]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.3301796913146973\n",
      "output: tensor([[-7.0939e+00,  1.4550e+00, -8.5159e+00, -6.6666e+00, -7.7655e+00,\n",
      "         -1.6916e+00, -1.0138e+01,  2.1959e+00,  3.2956e-01,  2.2850e-03,\n",
      "         -6.2023e+00],\n",
      "        [-6.5278e+00, -2.6211e-02, -6.4066e+00, -3.9182e+00, -6.6319e+00,\n",
      "         -2.2265e+00, -6.7167e+00,  2.1322e+00,  1.5804e+00, -8.0635e-01,\n",
      "         -4.5637e+00],\n",
      "        [-6.2328e+00,  6.4460e+00, -9.9285e+00, -1.4759e+01, -1.2506e+01,\n",
      "         -1.0880e+01, -1.5515e+01,  2.5521e+00, -7.2561e+00,  2.9772e+00,\n",
      "         -4.4806e+00],\n",
      "        [-1.2528e+01, -7.3897e-01, -6.7978e+00, -6.8965e+00, -7.1202e+00,\n",
      "         -3.6845e+00, -8.3741e+00,  3.3871e+00,  5.1037e+00, -1.0356e+00,\n",
      "         -8.0639e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 5.887486457824707\n",
      "output: tensor([[-3.9852e+00,  1.2021e-01, -2.4257e+00, -2.7261e+00, -3.1909e+00,\n",
      "         -3.4503e-01, -3.0873e+00,  1.7023e+00,  9.8109e-01, -3.5417e-01,\n",
      "         -5.3062e+00],\n",
      "        [-1.4927e+01,  5.0560e-01, -2.0471e+01, -1.6495e+01, -1.4491e+01,\n",
      "         -1.5462e+01, -2.7096e+01,  3.0306e+00, -4.6970e+00,  1.1911e+01,\n",
      "         -7.5102e+00],\n",
      "        [-5.9958e+00, -2.0424e+00, -3.8668e+00, -4.6385e+00, -4.1518e+00,\n",
      "          2.6415e-03, -5.8799e+00,  2.7116e+00,  6.7327e-01, -1.0334e-01,\n",
      "         -4.5614e+00],\n",
      "        [-4.7507e+00,  1.1635e+00, -4.4017e+00, -2.3227e+00, -3.9374e+00,\n",
      "         -7.6432e-01, -3.7507e+00,  2.7835e+00,  6.1309e-01, -4.7003e-01,\n",
      "         -5.5055e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 2.13312029838562\n",
      "output: tensor([[ -8.6907,   4.7999, -10.1240, -17.4975, -15.4740,  -9.7211, -19.1375,\n",
      "           0.8262,  -2.7967,   8.6133,  -7.3141],\n",
      "        [ -8.3341,   2.5195,   1.7405,  -5.3891,  -3.4092,  -3.1143,  -6.5345,\n",
      "           2.9955,  -3.0788,   5.6145,  -5.7108],\n",
      "        [-10.1831,  -0.7733,  -5.5519,  -3.2098,  -5.6289,   2.0732,  -8.2413,\n",
      "           4.2263,   1.6651,   0.5172, -10.5090],\n",
      "        [-10.3639,   1.8850,  -7.4596,  -7.0743,  -8.8205,  -1.2101,  -9.8356,\n",
      "           1.3259,  -0.3394,   2.2130,  -7.5527]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.32004714012146\n",
      "output: tensor([[-16.9903,   1.9054,  -7.9994, -20.1038, -18.7951,  -4.7177, -21.1030,\n",
      "           0.9370,  -3.5004,  10.8541,  -7.8528],\n",
      "        [ -9.6361,  -3.2678,  -5.5853,  -5.4216,  -6.0031,   0.4445,  -6.8260,\n",
      "           5.8311,  -1.9552,  -1.1535,  -8.7981],\n",
      "        [ -5.0589,   1.9858,  -2.4483,  -4.6101,  -4.9442,  -2.0312,  -5.9997,\n",
      "           0.7342,  -0.8404,   2.9202,  -3.6851],\n",
      "        [ -9.3468,   1.1006,  -1.3046,  -2.9530,  -2.2364,   0.1932,  -3.5111,\n",
      "           2.5423,   1.7231,  -0.9870,  -8.0643]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.966754913330078\n",
      "output: tensor([[ -8.0442,  -0.2382,  -4.0742,  -8.0268,  -9.5953,  -1.4540,  -9.3027,\n",
      "           0.8466,  -1.3914,   4.9335,  -8.3818],\n",
      "        [ -7.8888,  -3.3589,  -2.5426,  -6.0923,  -6.4977,  -6.6449,  -5.2660,\n",
      "          -0.5881,  -0.2871,   7.7267,  -4.2030],\n",
      "        [-10.3692,   0.6213,   0.8617,  -6.8043,  -6.9178,  -4.3375,  -9.3730,\n",
      "           3.5284,  -0.5077,   4.0659,  -9.6373],\n",
      "        [-12.4807,  -2.2847,  -4.4169,  -4.2631,  -5.6646,   0.1881,  -6.1764,\n",
      "           5.1996,   0.3389,  -2.2126, -10.7624]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.8114800453186035\n",
      "output: tensor([[-10.1691,   1.4940,  -3.5199,  -7.4170,  -6.8094,  -0.6986,  -7.6095,\n",
      "           3.8946,   1.0644,   1.3235, -15.9258],\n",
      "        [-12.1589,  -6.6950,  -2.9726, -17.2447, -12.4164, -10.1837, -17.5081,\n",
      "          -1.3049,   2.3309,  10.5907, -11.5519],\n",
      "        [ -9.6778,  -0.8301,  -0.8770,  -3.6095,  -5.5161,   3.5186,  -7.0210,\n",
      "           1.9852,   0.1674,   0.4377,  -7.7655],\n",
      "        [ -4.0819,   0.1775,  -4.1447,  -7.8352,  -7.0717,  -2.8413,  -7.9011,\n",
      "          -0.8862,  -0.5390,   6.2394,  -4.9791]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 7.052092552185059\n",
      "output: tensor([[-9.7725, -1.3382,  3.1583, -3.4138, -1.2443, -0.2099, -4.4935,  5.2323,\n",
      "         -2.0601, -1.0934, -9.2199],\n",
      "        [-7.4352,  1.3596, -0.5534, -5.9643, -5.9252,  1.5797, -7.0335, -0.3529,\n",
      "         -0.7910,  4.9589, -8.1842],\n",
      "        [-6.2863, -1.3721, -1.2881, -0.8626, -1.8010,  2.5134, -3.2593,  3.1564,\n",
      "          0.3377, -1.4002, -6.9364],\n",
      "        [-8.8063, -1.9925, -1.8940, -7.5091, -5.2311, -4.0253, -8.5579,  2.0309,\n",
      "          1.7754,  4.7646, -9.5268]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.5958447456359863\n",
      "output: tensor([[ -6.3960,  -0.2556,  -3.8785,  -4.8473,  -5.3851,   1.4796,  -7.1111,\n",
      "           2.0772,  -0.9798,  -0.3154,  -8.1051],\n",
      "        [ -6.0430,  -0.0840,  -1.8140, -12.0358, -11.6619,  -7.3447, -10.7161,\n",
      "          -3.9520,   1.3091,   8.1533,  -9.2806],\n",
      "        [-13.7023,   1.3846,  -1.5361,  -5.2972,  -4.3276,   3.4039,  -4.4952,\n",
      "           5.4229,   1.8644,  -3.1504, -14.6200],\n",
      "        [ -3.5030,   0.5720,   1.1659,  -3.9877,  -1.9845,  -1.6987,  -3.2529,\n",
      "          -0.1904,   3.5289,   1.1160,  -4.9324]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.013050079345703\n",
      "output: tensor([[ -3.0485,   2.8229,  -4.9476,  -9.1496,  -7.6489,  -3.9371,  -7.6768,\n",
      "           1.9522,  -2.5617,   2.4965,  -6.1833],\n",
      "        [-13.9896,   2.1561,   1.6265,  -6.5920,  -7.0495,   3.4466,  -9.2007,\n",
      "           1.5844,   1.0363,   1.9510, -11.2827],\n",
      "        [ -3.8988,   2.0837,  -2.1749,  -2.7218,  -4.1204,   1.3636,  -3.5039,\n",
      "           1.2076,  -0.7101,  -0.9333,  -7.5945],\n",
      "        [-10.7024,  -0.2425,  -2.5711, -12.1091,  -7.8439,  -5.4299, -13.7930,\n",
      "           2.4714,   1.1058,   1.5344, -11.6764]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 1.4331092834472656\n",
      "output: tensor([[ -4.9995,   2.8385,  -5.9024,  -5.9940,  -4.0038, -10.8680,  -4.7840,\n",
      "           1.9048,   3.4889,   2.5481, -10.6493],\n",
      "        [ -5.1584,   2.2197,   0.0546,  -4.0102,  -4.8652,  -0.0491,  -3.7289,\n",
      "           2.8728,  -0.6745,  -1.5065, -10.2099],\n",
      "        [ -9.9304,   0.1426,  -2.0615,  -2.8739,  -4.4421,   0.4901,  -4.8904,\n",
      "           1.6553,   2.5155,  -0.9960,  -7.7320],\n",
      "        [ -9.2030,  -0.3478,  -5.3116, -12.1426, -11.4681,   0.3149,  -9.8255,\n",
      "           5.7632,  -1.1088,   0.0280, -14.0159]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 5.569368362426758\n",
      "output: tensor([[-16.7038,   1.6437,   2.5094, -15.8583, -11.1834,  -5.9332, -14.7414,\n",
      "           5.8842,   3.0643,  -4.4985, -18.1660],\n",
      "        [ -4.0601,   0.0454,  -0.1269,  -1.7975,  -1.0192,   3.8600,  -3.3678,\n",
      "           3.8581,  -1.0620,  -3.2360,  -4.1081],\n",
      "        [ -5.6595,   2.2049,  -1.0256,  -0.9001,  -2.4693,   0.7285,  -3.8804,\n",
      "           0.6788,   0.1634,   0.1269,  -3.6124],\n",
      "        [ -9.9012,  -2.1365,   0.6889, -12.3368,  -6.4517,  -5.7867, -13.4270,\n",
      "           2.0752,   1.4299,  -1.2358,  -5.5547]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.7864770889282227\n",
      "output: tensor([[-1.1497e+01,  1.5793e+00, -2.4488e+00, -8.9343e+00, -5.5938e+00,\n",
      "         -4.3583e+00, -6.6290e+00,  3.0077e+00,  3.1814e+00,  1.5239e+00,\n",
      "         -2.0853e+01],\n",
      "        [-1.0462e+01, -1.6523e+00, -2.8914e+00, -6.4722e+00, -6.2632e+00,\n",
      "          2.1083e+00, -8.2088e+00,  1.8639e+00,  1.6440e+00, -1.9826e-03,\n",
      "         -8.7592e+00],\n",
      "        [-4.5976e+00,  2.3331e+00, -1.6850e+00, -6.9378e+00, -5.5635e+00,\n",
      "         -9.1913e-01, -6.1246e+00,  2.2618e+00, -8.3571e-01,  2.0627e-01,\n",
      "         -3.7242e+00],\n",
      "        [-1.9593e+00,  9.3440e-01, -6.2539e-01, -1.9868e+00, -1.0255e+00,\n",
      "          1.1307e+00, -2.6461e+00,  2.8998e-01, -5.1688e-01, -4.7267e-01,\n",
      "         -4.6343e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 1.4304225444793701\n",
      "output: tensor([[ -5.4097,   2.8292,  -0.8355,  -4.1144,  -4.9664,   2.6245,  -3.5490,\n",
      "           1.1097,   0.8257,  -0.0644,  -8.0397],\n",
      "        [ -2.4480,   1.2291,  -0.7108,  -1.3904,  -1.7155,  -0.2872,  -1.1970,\n",
      "           0.3215,   0.1798,   0.2312,  -1.7048],\n",
      "        [-11.0761,   6.7233,   2.6460,  -2.5211,   0.2693,  -7.3339,  -5.7432,\n",
      "           3.9796,   3.4857,  -4.7981,  -7.1726],\n",
      "        [-10.7078,   2.6523,   4.2343,   0.2442,  -1.1561,   0.9731,  -0.5778,\n",
      "           5.4361,   0.1738,  -2.4014, -11.5433]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 1.481194257736206\n",
      "output: tensor([[ -8.5396,   2.2916,   1.6162,  -5.6546,  -3.7247,  -8.6318,  -2.6595,\n",
      "           4.0455,   0.4719,  -5.9321,  -8.4713],\n",
      "        [-10.3953,  -1.2209,  -0.2519,  -5.0290,  -5.0654,  -4.2149,  -6.9050,\n",
      "           4.0719,   1.4899,  -0.1981,  -7.8649],\n",
      "        [ -2.8758,   0.5431,  -1.2272,  -1.2973,  -1.7410,  -1.2020,  -2.0164,\n",
      "           0.3987,   0.4404,  -0.0571,  -0.6411],\n",
      "        [ -3.1650,   0.9839,  -4.1124,  -4.8990,  -3.6228,   1.7741,  -4.2304,\n",
      "           2.3802,  -0.2092,   0.6232,  -9.0552]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 4.167431831359863\n",
      "output: tensor([[ -4.1950,   1.4235,  -2.2345,  -4.6500,  -3.9243,  -0.5298,  -4.3538,\n",
      "           1.9800,  -0.3465,  -1.0255,  -6.4702],\n",
      "        [ -5.3783,   0.6082,  -1.0491,  -4.7124,  -3.2720,   2.0168,  -4.7961,\n",
      "           2.0954,  -0.6356,  -0.3919,  -6.0867],\n",
      "        [-13.1816,   1.9419,   1.5034,  -3.0620,  -3.6017,  -2.4590,  -5.2577,\n",
      "           3.2422,   1.0790,  -0.6550,  -9.8033],\n",
      "        [ -9.3134,   7.7096,  -7.3481, -11.9267,  -9.4404,  -6.5263, -10.9363,\n",
      "           0.6083,  -0.2162,  -2.8907,  -5.8122]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.2554075717926025\n",
      "output: tensor([[-6.2326e+00,  2.4295e+00, -3.0054e+00, -1.2279e+01, -8.0955e+00,\n",
      "         -1.0331e+01, -9.5540e+00,  4.7486e+00, -1.8333e+00,  7.3542e-01,\n",
      "         -9.1279e+00],\n",
      "        [-5.2121e+00,  1.1455e+00, -2.8609e+00, -5.8085e+00, -6.1463e+00,\n",
      "         -1.4967e+00, -5.7717e+00,  4.6168e+00, -2.5501e-01, -1.4233e+00,\n",
      "         -8.4483e+00],\n",
      "        [-2.0190e+00,  1.0430e+00,  2.2455e-01, -2.3081e+00, -1.5959e+00,\n",
      "         -2.3232e-01, -2.0768e+00,  5.3886e-01, -5.3639e-03,  2.0557e-01,\n",
      "         -2.0398e+00],\n",
      "        [-8.2404e+00,  1.8073e+00,  3.6163e-01, -6.1258e+00, -5.5815e+00,\n",
      "         -3.7228e-02, -6.8157e+00,  1.7716e+00,  1.1613e-03,  1.3337e-01,\n",
      "         -3.7269e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "loss: 1.9101289510726929\n",
      "output: tensor([[ -6.9197,   4.8006,   0.1749,  -3.4970,  -2.4582,  -4.0143,  -3.9664,\n",
      "           4.0568,   1.0650,  -2.8585, -12.5975],\n",
      "        [ -6.8485,  -0.7712,  -1.5068,  -4.2552,  -3.5174,  -1.2461,  -5.5977,\n",
      "           3.2134,   1.2353,   0.0771,  -3.0327],\n",
      "        [ -7.1726,   1.8296,  -6.6107, -11.1821,  -9.5042,  -4.6172, -11.7871,\n",
      "           2.3767,  -0.4299,  -0.6610,  -5.5027],\n",
      "        [ -5.4824,   0.4732,  -0.6465,  -7.8632,  -7.6753,  -2.1819,  -7.8180,\n",
      "           3.7504,  -1.8863,  -0.4173,  -6.0882]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 1.9954630136489868\n",
      "output: tensor([[ -1.7484,   0.0329,  -1.3587,  -2.7737,  -2.9079,  -0.9667,  -3.0126,\n",
      "           1.3383,   0.8586,  -0.4931,  -2.9802],\n",
      "        [ -8.5587,  -0.6281,   3.0490,  -3.7351,  -4.8326,  -0.8381,  -5.0388,\n",
      "           5.4760,   0.4937,   0.7359,  -3.9659],\n",
      "        [ -0.3302,   4.4396,  -0.7003,  -4.6490,  -3.0887,  -4.0066,  -4.7531,\n",
      "           5.8513,  -2.2949,  -5.7727,  -8.8996],\n",
      "        [ -8.4717,   0.6847, -10.3357, -17.1011, -13.7714,  -7.9197, -14.1677,\n",
      "           1.8325,   2.4143,   3.2947,  -7.1002]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.15541410446167\n",
      "output: tensor([[ -8.3692,  -0.6253,  -7.6500, -16.2449, -10.2415, -10.1722, -14.2863,\n",
      "           3.4845,   0.2777,  -1.6022,  -5.5874],\n",
      "        [-10.5870,  -1.2019,  -4.8590,  -9.8489,  -9.5271,  -7.9515, -11.6894,\n",
      "           8.5798,   1.5614,   0.4663,  -9.7254],\n",
      "        [ -4.9101,   3.6518,  -2.7730,  -5.8354,  -5.7865,  -2.3463,  -3.5657,\n",
      "           4.5266,  -2.3330,  -1.9770,  -7.8750],\n",
      "        [ -3.3665,   2.0158,  -1.0444,  -2.7875,  -2.0620,  -2.4623,  -1.9167,\n",
      "           1.1700,   2.2433,  -1.4920,  -5.8364]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 5.536312580108643\n",
      "output: tensor([[-1.2399, -2.7773, -2.3831, -2.9135, -2.1540, -3.8676, -3.1906,  0.8010,\n",
      "          0.7374,  0.5943, -1.3448],\n",
      "        [-2.2137,  2.9980, -6.4167, -5.5831, -5.2788, -7.0404, -6.5281,  3.7540,\n",
      "          2.3457, -3.3511, -5.5235],\n",
      "        [-3.8651,  0.8403, -2.2876, -7.0149, -4.0642, -4.6004, -5.3735,  4.5487,\n",
      "          0.7122, -0.8269, -8.1044],\n",
      "        [-4.1041,  0.2081, -1.0002, -8.4938, -5.9079,  1.9128, -7.7579,  3.9829,\n",
      "         -3.4460,  0.4409, -3.8087]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.7063965797424316\n",
      "output: tensor([[ -2.7550,  -0.8235,   1.3666,  -6.7805,  -4.7812,  -7.3549,  -3.8935,\n",
      "           3.0348,   6.1295,  -6.4897, -10.0542],\n",
      "        [ -6.4099,   0.7855,  -1.1090,  -1.4815,  -2.8272,  -4.9489,  -3.9915,\n",
      "           3.2152,  -0.5370,   0.6616,  -1.8229],\n",
      "        [ -8.6195,  -4.7192,  -3.4550, -10.8216,  -8.4099,  -1.9238,  -8.2351,\n",
      "           3.4074,   0.5529,   2.4342,  -5.2996],\n",
      "        [ -0.8491,  -2.2009,  -3.9459,  -3.0938,  -4.3896,  -2.9719,  -4.6561,\n",
      "           2.4303,   1.0778,  -1.3911,  -1.2232]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.656128168106079\n",
      "output: tensor([[ -0.7008,   0.2721,  -0.6127,  -2.8741,  -2.4020,  -2.4330,  -2.6183,\n",
      "           0.7681,   0.2146,   0.0234,  -0.7506],\n",
      "        [ -1.9702,   0.3753,  -6.4131, -13.1985, -12.3526,  -7.6383, -11.7015,\n",
      "           4.5717,   0.0573,  -4.2387,  -5.1132],\n",
      "        [ -6.7057,   1.9520,  -2.5850,  -8.7365,  -7.1159,  -7.5767,  -6.6353,\n",
      "           4.0077,  -1.2102,  -0.2754,  -2.4464],\n",
      "        [ -4.6767,  -3.3764,  -7.7336, -10.9731, -13.4580,  -5.7042, -11.9413,\n",
      "           0.9408,   0.6815,   0.6452,   0.9762]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 5.259699821472168\n",
      "output: tensor([[  0.4265,  -2.4543,  -1.9213,  -4.3730,  -4.0934,  -5.7324,  -5.4606,\n",
      "           2.1096,  -1.9411,   2.2795,   0.1193],\n",
      "        [ -7.3982,  -1.0582, -11.3456, -15.6850, -15.4361,  -8.2121, -15.2062,\n",
      "           3.1350,   1.5741,   2.2360,  -6.1449],\n",
      "        [ -0.9158,   1.2692,  -6.9287, -10.2180,  -9.6818,  -3.1295,  -9.8145,\n",
      "           3.1216,   0.4822,  -5.9177,  -1.5710],\n",
      "        [ -4.6692,  -0.7445,   0.7717,  -4.0642,  -3.9744,  -0.8082,  -2.7267,\n",
      "          -0.0532,   0.2795,   1.8450,  -0.5863]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 1.5767567157745361\n",
      "output: tensor([[  2.3302,  -2.9001,  -2.2201,  -9.5835,  -8.4440,  -5.8993,  -8.1160,\n",
      "           1.7136,  -1.1963,   1.7070,  -1.5512],\n",
      "        [ -0.1070,   1.4152,  -5.6478,  -6.4212,  -5.0223,  -3.9358,  -5.6882,\n",
      "           0.4544,   1.0366,  -0.9322,  -2.2168],\n",
      "        [ -1.2358,  -0.5750,  -3.6516,  -7.5384,  -6.8294,  -5.9960,  -6.7247,\n",
      "           1.0909,   1.4743,   2.3815,  -3.1884],\n",
      "        [-10.6999,  -2.4239,  -3.5156, -12.3817,  -9.8660,  -4.0745,  -9.5722,\n",
      "          -0.5618,   1.6194,   2.8260,  -1.8185]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.289188861846924\n",
      "output: tensor([[  2.3680,  -3.8881,  -8.5750, -16.0719, -12.6792, -15.2151, -11.1707,\n",
      "           3.6392,  -4.1418,   2.8368,  -5.1601],\n",
      "        [ -1.7076,   1.7578,  -5.8287, -11.6545, -10.1930,  -7.0076, -10.0401,\n",
      "          -2.0637,   0.4321,   1.3019,   0.8033],\n",
      "        [ -2.6122,  -0.8246,   0.2135,  -4.8661,  -4.0301,  -4.0428,  -4.1701,\n",
      "          -0.5789,   2.0527,   1.5911,   0.1237],\n",
      "        [ -0.4511,   0.0203,  -0.0445,  -1.3514,  -1.2095,  -0.4660,  -1.3925,\n",
      "          -0.0551,  -0.1889,   0.4507,   0.0295]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.049346685409546\n",
      "output: tensor([[ -4.2550,  -0.9554,  -3.1849,  -9.1511,  -7.5187,  -7.2906,  -8.4926,\n",
      "          -1.2610,   2.8930,   1.8105,  -1.6379],\n",
      "        [  2.3594,  -6.6221,  -8.5246, -17.7659, -15.7780, -12.0564, -17.1619,\n",
      "           4.0987,   0.6478,   2.6244,  -1.0127],\n",
      "        [ -1.4233,  -1.7952,  -2.7766,  -4.0256,  -4.0116,  -2.8246,  -3.4438,\n",
      "          -0.0318,   0.8509,   1.6389,  -0.7791],\n",
      "        [ -2.3568,   0.3267,   0.3816,  -2.9539,  -3.0325,   0.4351,  -2.9145,\n",
      "          -0.6439,   0.3343,   0.5172,   0.9253]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.8421878814697266\n",
      "output: tensor([[ -4.3550,   1.9314,  -0.9804,  -7.8781,  -6.6950,  -7.9815,  -6.2108,\n",
      "          -1.2817,   4.1109,   1.4181,  -3.6511],\n",
      "        [  6.1045,  -2.7423,  -2.2396, -10.9779,  -7.5186,  -6.4865,  -8.0280,\n",
      "           0.8298,   0.1813,  -1.2273,   1.5166],\n",
      "        [ -4.5799,  -2.5413,  -2.6600,  -5.9372,  -5.7481,  -4.9572,  -5.6390,\n",
      "           0.0787,   1.3158,   2.9953,  -0.2914],\n",
      "        [ -0.3666,   0.6966,  -3.3383,  -6.4129,  -5.9000,  -3.5819,  -6.7175,\n",
      "          -0.0352,   0.8872,   0.7637,  -0.2608]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 3.9747421741485596\n",
      "output: tensor([[ -1.0660,   0.4580,  -2.1951,  -2.7853,  -2.7381,  -2.2935,  -3.0480,\n",
      "          -0.1705,   1.4506,   0.1058,  -0.2512],\n",
      "        [ -0.4443,  -1.0398,  -3.6193,  -6.7503,  -5.7746,  -4.4842,  -6.5087,\n",
      "           1.0668,   0.5664,  -0.1253,  -0.6694],\n",
      "        [ -0.5753,  -3.4006,  -5.4277, -12.7237,  -8.9631,  -7.9768,  -9.7407,\n",
      "          -0.1358,   7.0050,   0.9762,  -7.2531],\n",
      "        [ -4.9097,   1.1002,  -2.9865,  -9.0015,  -8.6664,  -6.4188,  -7.7509,\n",
      "          -2.9833,   0.3316,   1.5074,  -2.4696]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "loss: 2.487760543823242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: Error detected in CudnnRnnBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/traitlets/config/application.py\", line 976, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2936, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3135, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2247591/923925910.py\", line 4, in <cell line: 3>\n",
      "    running_epochs(model,optimizer,criterion)\n",
      "  File \"/tmp/ipykernel_2247591/2446513367.py\", line 47, in running_epochs\n",
      "    train(model,optimizer,criterion)\n",
      "  File \"/tmp/ipykernel_2247591/2446513367.py\", line 17, in train\n",
      "    out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tmp/ipykernel_2247591/57322910.py\", line 14, in forward\n",
      "    x = self.conv1(x, edge_index)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tmp/ipykernel_2247591/2825018721.py\", line 103, in forward\n",
      "    x_src = x_dst = self.lin_src(x)[-1][0].mean(dim=0).view(-1, H, C)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/dauin_user/lcolomba/.conda/envs/pyg2/lib/python3.9/site-packages/torch/nn/modules/rnn.py\", line 769, in forward\n",
      "    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " (Triggered internally at  ../torch/csrc/autograd/python_anomaly_mode.cpp:102.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'CudnnRnnBackward0' returned nan values in its 1th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model, optimizer, criterion \u001b[38;5;241m=\u001b[39m set_model_parameters(model5, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mdetect_anomaly(): \u001b[38;5;66;03m# used for debugging\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mrunning_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mrunning_epochs\u001b[0;34m(model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrunning_epochs\u001b[39m(model,optimizer,criterion):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 47\u001b[0m         \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m         train_acc \u001b[38;5;241m=\u001b[39m test(train_loader, model)\n\u001b[1;32m     49\u001b[0m         test_acc \u001b[38;5;241m=\u001b[39m test(test_loader, model)\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)  \u001b[38;5;66;03m# Compute the loss.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Derive gradients.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update parameters based on gradients.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pyg2/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pyg2/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'CudnnRnnBackward0' returned nan values in its 1th output."
     ]
    }
   ],
   "source": [
    "# Experiment 5: GAT + LSTM Baseline \n",
    "model, optimizer, criterion = set_model_parameters(model5, lr=0.001)\n",
    "with torch.autograd.detect_anomaly(): # used for debugging\n",
    "    running_epochs(model,optimizer,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchGeometric",
   "language": "python",
   "name": "pyg2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8cb2cf009b56e4f45028220f93cf800117e77713dd4472a875537ab4fe70ac3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
