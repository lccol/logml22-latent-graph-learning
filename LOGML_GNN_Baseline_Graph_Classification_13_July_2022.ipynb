{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LOGML_GNN_Baseline_Graph_Classification_13_July_2022.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "sWcOM_roJ1RB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn5U4EE6K86v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5053267-7d91-4663-c601-54ee83a379cd"
      },
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.0+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html"
      ],
      "metadata": {
        "id": "viSOl2Ej-fgN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-FO5xL3mw98"
      },
      "source": [
        "### Graph Classification with Graph Neural Networks\n",
        "\n",
        "Graph classification refers to the problem of classifiying entire graphs (in contrast to nodes), given a **dataset of graphs**, based on some structural graph properties. Here, we want to embed entire graphs, and we want to embed those graphs in such a way so that they are linearly separable given a task at hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5FBQ9gXpL-W"
      },
      "source": [
        "Our task for graph classification is **heart arrhythmia prediction**, in which sensors are represented as nodes and ECG records are represented as graphs, and the task is to infer whether a an ECG record has some kind of heart arrhythmia. \n",
        "\n",
        "Dataset:\n",
        "- PhysioNet 2020 (P20-ECG)\n",
        "- 6877 patients\n",
        "- 80K MTS samples\n",
        "- 12-lead ECG\n",
        "- Each TS has 2500 timestamps\n",
        "- 9 classes of heart arrhythmia\n",
        "\n",
        "\n",
        "Graph Structure:\n",
        "- Each patient's ECG is a graph \n",
        "- 12 leads means 12 nodes per graph\n",
        "- 6877 patients means 6877 ECG graphs for each patient\n",
        "- 80K MTS samples **( What does this mean?)**\n",
        "- Each TS has 2500 time steps mean for each node we will have two properties (a) node features for the patient when the ECG is recorded (b) time series data which reflect the heart's electrical signal over 2500 time steps. **(Are the node features just as blood pressure dynamically recorded at the same time resolution, no right? These are only measured once and stay static for the 2500 time steps?)**\n",
        "- We are going with fully connected graph with self loops which means each node is connected to every other node in the graph. So the node degree for each node is 11+1 (where 11 are edges to other nodes and 1 is the self loop)\n",
        "- 9 classes of heart issues exist **(It would be nice if someone could check if the classes are balanced or not, just good to know before modelling)**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysing Network Characteristics of the Dummy Dataset "
      ],
      "metadata": {
        "id": "wRi38CrqGnws"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHSP6-RBOqCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f400bf2-2332-458c-8dae-ffdbecaddfcd"
      },
      "source": [
        "# DUMMY DATASET - We will replace it with our own \n",
        "# The TU Dortmund University has collected a wide range of different graph classification datasets, known as the [**TUDatasets**](https://chrsmrrs.github.io/datasets/), which are also accessible via [`torch_geometric.datasets.TUDataset`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.TUDataset) in PyTorch Geometric.\n",
        "# Let's load and inspect one of the smaller ones, the **MUTAG dataset**:\n",
        "\n",
        "import torch\n",
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('=============================================================')\n",
        "\n",
        "# statistics about the first graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
            "Extracting data/TUDataset/MUTAG/MUTAG.zip\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: MUTAG(188):\n",
            "====================\n",
            "Number of graphs: 188\n",
            "Number of features: 7\n",
            "Number of classes: 2\n",
            "\n",
            "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])\n",
            "=============================================================\n",
            "Number of nodes: 17\n",
            "Number of edges: 38\n",
            "Average node degree: 2.24\n",
            "Has isolated nodes: False\n",
            "Has self-loops: False\n",
            "Is undirected: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DgLTbTcrbQO"
      },
      "source": [
        "Note: We will have to change the y in the Data object accordingly **because we have 9 classes and this dataset has 2 classes** - so this is a binary graph classification task. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the graphs into training and test datasets"
      ],
      "metadata": {
        "id": "ncVIAqquGzIN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j11WiUr-PRH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43f1567c-1548-4cb0-c81d-d0681c90a94e"
      },
      "source": [
        "# This dataset provides **188 different graphs**, and the task is to classify each graph into **one out of two classes**.\n",
        "\n",
        "# By inspecting the first graph object of the dataset, we can see that it comes with **17 nodes (with 7-dimensional feature vectors)** and **38 edges** (leading to an average node degree of 2.24).\n",
        "# It also comes with exactly **one graph label** (`y=[1]`), and, in addition to previous datasets, provides addtional **4-dimensional edge features** (`edge_attr=[38, 4]`).\n",
        "# However, for the sake of simplicity, we will not make use of those.\n",
        "\n",
        "# PyTorch Geometric provides some useful utilities for working with graph datasets, *e.g.*, we can shuffle the dataset and use the first 150 graphs as training graphs, while using the remaining ones for testing:\n",
        "\n",
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:150]\n",
        "test_dataset = dataset[150:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training graphs: 150\n",
            "Number of test graphs: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini Batching of Graphs "
      ],
      "metadata": {
        "id": "u58fKxNDEZlZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gZ-l0npPIca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7e0fe3-83ed-4dd3-8c41-8289c7003a9b"
      },
      "source": [
        "# Mini batching of graphs\n",
        "# Adjacency matrices are stacked in a diagonal fashion (creating a giant graph that holds multiple isolated subgraphs),\n",
        "# and node and target features are simply concatenated in the node dimension:\n",
        "# PyTorch Geometric automatically takes care of batching multiple graphs into a single giant graph with the help of the torch_geometric.data.DataLoader class: \n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "for step, data in enumerate(train_loader):\n",
        "    print(f'Step {step + 1}:')\n",
        "    print('=======')\n",
        "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
        "    print(data)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 2636], x=[1188, 7], edge_attr=[2636, 4], y=[64], batch=[1188], ptr=[65])\n",
            "\n",
            "Step 2:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 2506], x=[1139, 7], edge_attr=[2506, 4], y=[64], batch=[1139], ptr=[65])\n",
            "\n",
            "Step 3:\n",
            "=======\n",
            "Number of graphs in the current batch: 22\n",
            "DataBatch(edge_index=[2, 852], x=[387, 7], edge_attr=[852, 4], y=[22], batch=[387], ptr=[23])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GNN Model definitions for Graph Classification "
      ],
      "metadata": {
        "id": "YYohJnAoEnun"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN3sRVuaQ88l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c31f098-3a23-4f60-d303-ef6495eed28a"
      },
      "source": [
        "# Training Graph Neural Networks for Graph Classification\n",
        "# Embed each node by performing multiple rounds of message passing\n",
        "# Aggregate node embeddings into a unified graph embedding (readout layer)\n",
        "# Train a final classifier on the graph embedding\n",
        " \n",
        "# There exists multiple readout layers in literature, but the most common one is to simply take the average of node embeddings\n",
        "\n",
        "# For ex: In GCNConv we use the  ReLU(ùë•)=max(ùë•,0)  activation for obtaining localized node embeddings,\n",
        "# before we apply our final classifier on top of a graph readout layer.\n",
        "\n",
        "# PyTorch Geometric provides this functionality via torch_geometric.nn.global_mean_pool, \n",
        "# which takes in the node embeddings of all nodes in the mini-batch and the assignment vector batch \n",
        "# to compute a graph embedding of size [batch_size, hidden_channels] for each graph in the batch.\n",
        "\n",
        "# The final architecture for applying GNNs to the task of graph classification then looks as follows and allows for complete end-to-end training:\n",
        "\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.nn import GraphConv\n",
        "\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GNN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GraphConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GAT, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GATConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        \n",
        "        return x\n",
        "       \n",
        "# Define the models\n",
        "model1 = GCN(hidden_channels=64)\n",
        "model2= GAT(hidden_channels=64)\n",
        "model3 = GNN(hidden_channels=64)\n",
        "\n",
        "# Print them \n",
        "print(model1)\n",
        "print(model2)\n",
        "print(model3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(7, 64)\n",
            "  (conv2): GCNConv(64, 64)\n",
            "  (conv3): GCNConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "GAT(\n",
            "  (conv1): GATConv(7, 64, heads=1)\n",
            "  (conv2): GATConv(64, 64, heads=1)\n",
            "  (conv3): GATConv(64, 64, heads=1)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "GNN(\n",
            "  (conv1): GraphConv(7, 64)\n",
            "  (conv2): GraphConv(64, 64)\n",
            "  (conv3): GraphConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvhgQoO8Svw4"
      },
      "source": [
        "# Set model paramters and model type\n",
        "def set_model_parameters(model_type, lr=0.01):\n",
        "    model = model_type\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    return model, optimizer, criterion\n",
        "\n",
        "# Train the model\n",
        "def train(model, optimizer,criterion):\n",
        "    model.train()\n",
        "\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "         loss = criterion(out, data.y)  # Compute the loss.\n",
        "         loss.backward()  # Derive gradients.\n",
        "         optimizer.step()  # Update parameters based on gradients.\n",
        "         optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "# Test the model \n",
        "def test(loader, model):\n",
        "     model.eval()\n",
        "\n",
        "     correct = 0\n",
        "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "         out = model(data.x, data.edge_index, data.batch)  \n",
        "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n",
        "# Training and Testing Pipeline \n",
        "def running_epochs(model,optimizer,criterion):\n",
        "    for epoch in range(1, 171):\n",
        "        train(model,optimizer,criterion )\n",
        "        train_acc = test(train_loader, model)\n",
        "        test_acc = test(test_loader, model)\n",
        "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1: GCN Baseline \n",
        "model, optimizer, criterion = set_model_parameters(model1, lr=0.01)\n",
        "running_epochs(model,optimizer,criterion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2hNwdSGE3mi",
        "outputId": "3b80ae64-0262-4915-a6de-1840c5c16265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 007, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 008, Train Acc: 0.7200, Test Acc: 0.7632\n",
            "Epoch: 009, Train Acc: 0.7333, Test Acc: 0.7632\n",
            "Epoch: 010, Train Acc: 0.7467, Test Acc: 0.7632\n",
            "Epoch: 011, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 012, Train Acc: 0.7200, Test Acc: 0.8158\n",
            "Epoch: 013, Train Acc: 0.7333, Test Acc: 0.7632\n",
            "Epoch: 014, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 015, Train Acc: 0.7067, Test Acc: 0.8158\n",
            "Epoch: 016, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 017, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 018, Train Acc: 0.7267, Test Acc: 0.8158\n",
            "Epoch: 019, Train Acc: 0.7467, Test Acc: 0.7632\n",
            "Epoch: 020, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 021, Train Acc: 0.7200, Test Acc: 0.6842\n",
            "Epoch: 022, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 023, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 024, Train Acc: 0.7400, Test Acc: 0.7632\n",
            "Epoch: 025, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 026, Train Acc: 0.7533, Test Acc: 0.8158\n",
            "Epoch: 027, Train Acc: 0.7333, Test Acc: 0.8421\n",
            "Epoch: 028, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 029, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 030, Train Acc: 0.7533, Test Acc: 0.8421\n",
            "Epoch: 031, Train Acc: 0.7200, Test Acc: 0.8421\n",
            "Epoch: 032, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 033, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 034, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 035, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 036, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 037, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 038, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 039, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 040, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 041, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 042, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 043, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 044, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 045, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 046, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 047, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 048, Train Acc: 0.7333, Test Acc: 0.7368\n",
            "Epoch: 049, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 050, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 051, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 052, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 053, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 054, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 055, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 056, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 057, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 058, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 059, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 060, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 061, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 062, Train Acc: 0.7467, Test Acc: 0.7632\n",
            "Epoch: 063, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 064, Train Acc: 0.7800, Test Acc: 0.8158\n",
            "Epoch: 065, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 066, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 067, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 068, Train Acc: 0.7800, Test Acc: 0.7632\n",
            "Epoch: 069, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 070, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 071, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 072, Train Acc: 0.7800, Test Acc: 0.8158\n",
            "Epoch: 073, Train Acc: 0.7800, Test Acc: 0.8158\n",
            "Epoch: 074, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 075, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 076, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 077, Train Acc: 0.7800, Test Acc: 0.8158\n",
            "Epoch: 078, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 079, Train Acc: 0.7800, Test Acc: 0.8158\n",
            "Epoch: 080, Train Acc: 0.7800, Test Acc: 0.7632\n",
            "Epoch: 081, Train Acc: 0.7800, Test Acc: 0.7632\n",
            "Epoch: 082, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 083, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 084, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 085, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 086, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 087, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 088, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 089, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 090, Train Acc: 0.8000, Test Acc: 0.8158\n",
            "Epoch: 091, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 092, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 093, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 094, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 095, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 096, Train Acc: 0.7800, Test Acc: 0.7368\n",
            "Epoch: 097, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 098, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 099, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 100, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 101, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 102, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 103, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 104, Train Acc: 0.7867, Test Acc: 0.7368\n",
            "Epoch: 105, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 106, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 107, Train Acc: 0.7867, Test Acc: 0.7895\n",
            "Epoch: 108, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 109, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 110, Train Acc: 0.7667, Test Acc: 0.7368\n",
            "Epoch: 111, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 112, Train Acc: 0.7800, Test Acc: 0.7368\n",
            "Epoch: 113, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 114, Train Acc: 0.7867, Test Acc: 0.7368\n",
            "Epoch: 115, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 116, Train Acc: 0.8067, Test Acc: 0.6579\n",
            "Epoch: 117, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 118, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 119, Train Acc: 0.7867, Test Acc: 0.7368\n",
            "Epoch: 120, Train Acc: 0.8000, Test Acc: 0.7632\n",
            "Epoch: 121, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 122, Train Acc: 0.7867, Test Acc: 0.7368\n",
            "Epoch: 123, Train Acc: 0.8000, Test Acc: 0.7895\n",
            "Epoch: 124, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 125, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 126, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 127, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 128, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 129, Train Acc: 0.7867, Test Acc: 0.7895\n",
            "Epoch: 130, Train Acc: 0.7867, Test Acc: 0.7895\n",
            "Epoch: 131, Train Acc: 0.7867, Test Acc: 0.7895\n",
            "Epoch: 132, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 133, Train Acc: 0.7867, Test Acc: 0.7895\n",
            "Epoch: 134, Train Acc: 0.7867, Test Acc: 0.7895\n",
            "Epoch: 135, Train Acc: 0.7867, Test Acc: 0.7895\n",
            "Epoch: 136, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 137, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 138, Train Acc: 0.8000, Test Acc: 0.7895\n",
            "Epoch: 139, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 140, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 141, Train Acc: 0.8333, Test Acc: 0.7632\n",
            "Epoch: 142, Train Acc: 0.8267, Test Acc: 0.6842\n",
            "Epoch: 143, Train Acc: 0.8000, Test Acc: 0.7632\n",
            "Epoch: 144, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 145, Train Acc: 0.8400, Test Acc: 0.7368\n",
            "Epoch: 146, Train Acc: 0.8400, Test Acc: 0.7632\n",
            "Epoch: 147, Train Acc: 0.8067, Test Acc: 0.7368\n",
            "Epoch: 148, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 149, Train Acc: 0.8067, Test Acc: 0.7105\n",
            "Epoch: 150, Train Acc: 0.8000, Test Acc: 0.7105\n",
            "Epoch: 151, Train Acc: 0.8067, Test Acc: 0.7368\n",
            "Epoch: 152, Train Acc: 0.8000, Test Acc: 0.7368\n",
            "Epoch: 153, Train Acc: 0.8200, Test Acc: 0.7368\n",
            "Epoch: 154, Train Acc: 0.8067, Test Acc: 0.7632\n",
            "Epoch: 155, Train Acc: 0.8133, Test Acc: 0.7368\n",
            "Epoch: 156, Train Acc: 0.8200, Test Acc: 0.7368\n",
            "Epoch: 157, Train Acc: 0.8333, Test Acc: 0.7105\n",
            "Epoch: 158, Train Acc: 0.8400, Test Acc: 0.7105\n",
            "Epoch: 159, Train Acc: 0.8067, Test Acc: 0.7632\n",
            "Epoch: 160, Train Acc: 0.8267, Test Acc: 0.7632\n",
            "Epoch: 161, Train Acc: 0.8400, Test Acc: 0.7368\n",
            "Epoch: 162, Train Acc: 0.8400, Test Acc: 0.6842\n",
            "Epoch: 163, Train Acc: 0.8333, Test Acc: 0.7368\n",
            "Epoch: 164, Train Acc: 0.8467, Test Acc: 0.6842\n",
            "Epoch: 165, Train Acc: 0.8200, Test Acc: 0.7105\n",
            "Epoch: 166, Train Acc: 0.8267, Test Acc: 0.7105\n",
            "Epoch: 167, Train Acc: 0.8400, Test Acc: 0.6842\n",
            "Epoch: 168, Train Acc: 0.8400, Test Acc: 0.6842\n",
            "Epoch: 169, Train Acc: 0.8533, Test Acc: 0.6842\n",
            "Epoch: 170, Train Acc: 0.8267, Test Acc: 0.7368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2: GAT Baseline \n",
        "model, optimizer, criterion = set_model_parameters(model2, lr=0.01)\n",
        "running_epochs(model,optimizer,criterion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCIq4QNeFlAA",
        "outputId": "a8a9acf4-bacc-4d7e-fc51-aebd3fb3c3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 003, Train Acc: 0.6800, Test Acc: 0.7368\n",
            "Epoch: 004, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 005, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 008, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 009, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 010, Train Acc: 0.7533, Test Acc: 0.8421\n",
            "Epoch: 011, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 012, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 013, Train Acc: 0.7467, Test Acc: 0.8158\n",
            "Epoch: 014, Train Acc: 0.7400, Test Acc: 0.8158\n",
            "Epoch: 015, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 016, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 017, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 018, Train Acc: 0.7333, Test Acc: 0.8158\n",
            "Epoch: 019, Train Acc: 0.7600, Test Acc: 0.8421\n",
            "Epoch: 020, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 021, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 022, Train Acc: 0.7600, Test Acc: 0.8421\n",
            "Epoch: 023, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 024, Train Acc: 0.7533, Test Acc: 0.8158\n",
            "Epoch: 025, Train Acc: 0.7600, Test Acc: 0.8421\n",
            "Epoch: 026, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 027, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 028, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 029, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 030, Train Acc: 0.7667, Test Acc: 0.8421\n",
            "Epoch: 031, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 032, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 033, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 034, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 035, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 036, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 037, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 038, Train Acc: 0.7467, Test Acc: 0.8421\n",
            "Epoch: 039, Train Acc: 0.7533, Test Acc: 0.8421\n",
            "Epoch: 040, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 041, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 042, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 043, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 044, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 045, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 046, Train Acc: 0.7600, Test Acc: 0.8421\n",
            "Epoch: 047, Train Acc: 0.7600, Test Acc: 0.8421\n",
            "Epoch: 048, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 049, Train Acc: 0.7733, Test Acc: 0.8421\n",
            "Epoch: 050, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 051, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 052, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 053, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 054, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 055, Train Acc: 0.7600, Test Acc: 0.8421\n",
            "Epoch: 056, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 057, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 058, Train Acc: 0.7600, Test Acc: 0.8421\n",
            "Epoch: 059, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 060, Train Acc: 0.7467, Test Acc: 0.8158\n",
            "Epoch: 061, Train Acc: 0.7600, Test Acc: 0.8421\n",
            "Epoch: 062, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 063, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 064, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 065, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 066, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 067, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 068, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 069, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 070, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 071, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 072, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 073, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 074, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 075, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 076, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 077, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 078, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 079, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 080, Train Acc: 0.7600, Test Acc: 0.8421\n",
            "Epoch: 081, Train Acc: 0.7533, Test Acc: 0.8158\n",
            "Epoch: 082, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 083, Train Acc: 0.7533, Test Acc: 0.8158\n",
            "Epoch: 084, Train Acc: 0.7533, Test Acc: 0.8421\n",
            "Epoch: 085, Train Acc: 0.7600, Test Acc: 0.8421\n",
            "Epoch: 086, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 087, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 088, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 089, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 090, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 091, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 092, Train Acc: 0.7667, Test Acc: 0.8421\n",
            "Epoch: 093, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 094, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 095, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 096, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 097, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 098, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 099, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 100, Train Acc: 0.7533, Test Acc: 0.8158\n",
            "Epoch: 101, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 102, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 103, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 104, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 105, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 106, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 107, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 108, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 109, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 110, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 111, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 112, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 113, Train Acc: 0.7533, Test Acc: 0.8158\n",
            "Epoch: 114, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 115, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 116, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 117, Train Acc: 0.7533, Test Acc: 0.8158\n",
            "Epoch: 118, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 119, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 120, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 121, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 122, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 123, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 124, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 125, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 126, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 127, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 128, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 129, Train Acc: 0.7533, Test Acc: 0.8158\n",
            "Epoch: 130, Train Acc: 0.7533, Test Acc: 0.8158\n",
            "Epoch: 131, Train Acc: 0.7533, Test Acc: 0.8158\n",
            "Epoch: 132, Train Acc: 0.7667, Test Acc: 0.8421\n",
            "Epoch: 133, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 134, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 135, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 136, Train Acc: 0.7667, Test Acc: 0.8421\n",
            "Epoch: 137, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 138, Train Acc: 0.7733, Test Acc: 0.8421\n",
            "Epoch: 139, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 140, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 141, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 142, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 143, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 144, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 145, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 146, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 147, Train Acc: 0.7467, Test Acc: 0.7632\n",
            "Epoch: 148, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 149, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 150, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 151, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 152, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 153, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 154, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 155, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 156, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 157, Train Acc: 0.7467, Test Acc: 0.7632\n",
            "Epoch: 158, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 159, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 160, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 161, Train Acc: 0.7800, Test Acc: 0.8158\n",
            "Epoch: 162, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 163, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 164, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 165, Train Acc: 0.7800, Test Acc: 0.7632\n",
            "Epoch: 166, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 167, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 168, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 169, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 170, Train Acc: 0.7800, Test Acc: 0.7632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 3: GraphConv Baseline - suggested online in docs \n",
        "model, optimizer, criterion = set_model_parameters(model3, lr=0.01)\n",
        "running_epochs(model,optimizer,criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIZxNXW8Fuj2",
        "outputId": "e90d42d1-32d3-4e15-b3a5-914203d233c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 007, Train Acc: 0.6533, Test Acc: 0.7368\n",
            "Epoch: 008, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 009, Train Acc: 0.7333, Test Acc: 0.8158\n",
            "Epoch: 010, Train Acc: 0.7733, Test Acc: 0.8684\n",
            "Epoch: 011, Train Acc: 0.7867, Test Acc: 0.8421\n",
            "Epoch: 012, Train Acc: 0.7667, Test Acc: 0.8421\n",
            "Epoch: 013, Train Acc: 0.7867, Test Acc: 0.8421\n",
            "Epoch: 014, Train Acc: 0.8000, Test Acc: 0.8684\n",
            "Epoch: 015, Train Acc: 0.7933, Test Acc: 0.8421\n",
            "Epoch: 016, Train Acc: 0.8067, Test Acc: 0.8684\n",
            "Epoch: 017, Train Acc: 0.7933, Test Acc: 0.8421\n",
            "Epoch: 018, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 019, Train Acc: 0.8133, Test Acc: 0.8421\n",
            "Epoch: 020, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 021, Train Acc: 0.8000, Test Acc: 0.8158\n",
            "Epoch: 022, Train Acc: 0.7933, Test Acc: 0.8158\n",
            "Epoch: 023, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 024, Train Acc: 0.8200, Test Acc: 0.8158\n",
            "Epoch: 025, Train Acc: 0.8267, Test Acc: 0.7368\n",
            "Epoch: 026, Train Acc: 0.7800, Test Acc: 0.7632\n",
            "Epoch: 027, Train Acc: 0.8600, Test Acc: 0.7632\n",
            "Epoch: 028, Train Acc: 0.8533, Test Acc: 0.7632\n",
            "Epoch: 029, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 030, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 031, Train Acc: 0.7600, Test Acc: 0.5789\n",
            "Epoch: 032, Train Acc: 0.8133, Test Acc: 0.8158\n",
            "Epoch: 033, Train Acc: 0.8000, Test Acc: 0.8158\n",
            "Epoch: 034, Train Acc: 0.8000, Test Acc: 0.7368\n",
            "Epoch: 035, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 036, Train Acc: 0.8267, Test Acc: 0.8158\n",
            "Epoch: 037, Train Acc: 0.8667, Test Acc: 0.7895\n",
            "Epoch: 038, Train Acc: 0.8400, Test Acc: 0.7895\n",
            "Epoch: 039, Train Acc: 0.8267, Test Acc: 0.8421\n",
            "Epoch: 040, Train Acc: 0.8400, Test Acc: 0.7632\n",
            "Epoch: 041, Train Acc: 0.8467, Test Acc: 0.7895\n",
            "Epoch: 042, Train Acc: 0.8467, Test Acc: 0.7632\n",
            "Epoch: 043, Train Acc: 0.8333, Test Acc: 0.8421\n",
            "Epoch: 044, Train Acc: 0.8933, Test Acc: 0.8421\n",
            "Epoch: 045, Train Acc: 0.8733, Test Acc: 0.7895\n",
            "Epoch: 046, Train Acc: 0.8800, Test Acc: 0.8158\n",
            "Epoch: 047, Train Acc: 0.9133, Test Acc: 0.8421\n",
            "Epoch: 048, Train Acc: 0.8800, Test Acc: 0.8158\n",
            "Epoch: 049, Train Acc: 0.9133, Test Acc: 0.8158\n",
            "Epoch: 050, Train Acc: 0.9067, Test Acc: 0.8421\n",
            "Epoch: 051, Train Acc: 0.8867, Test Acc: 0.8158\n",
            "Epoch: 052, Train Acc: 0.9133, Test Acc: 0.8158\n",
            "Epoch: 053, Train Acc: 0.8467, Test Acc: 0.8421\n",
            "Epoch: 054, Train Acc: 0.8800, Test Acc: 0.7895\n",
            "Epoch: 055, Train Acc: 0.8800, Test Acc: 0.8158\n",
            "Epoch: 056, Train Acc: 0.9067, Test Acc: 0.8158\n",
            "Epoch: 057, Train Acc: 0.9267, Test Acc: 0.7895\n",
            "Epoch: 058, Train Acc: 0.9067, Test Acc: 0.8421\n",
            "Epoch: 059, Train Acc: 0.9000, Test Acc: 0.8421\n",
            "Epoch: 060, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 061, Train Acc: 0.9000, Test Acc: 0.8158\n",
            "Epoch: 062, Train Acc: 0.9067, Test Acc: 0.8158\n",
            "Epoch: 063, Train Acc: 0.7933, Test Acc: 0.8684\n",
            "Epoch: 064, Train Acc: 0.8933, Test Acc: 0.7895\n",
            "Epoch: 065, Train Acc: 0.8467, Test Acc: 0.7368\n",
            "Epoch: 066, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 067, Train Acc: 0.8600, Test Acc: 0.7895\n",
            "Epoch: 068, Train Acc: 0.9200, Test Acc: 0.7895\n",
            "Epoch: 069, Train Acc: 0.8533, Test Acc: 0.8421\n",
            "Epoch: 070, Train Acc: 0.9000, Test Acc: 0.8158\n",
            "Epoch: 071, Train Acc: 0.8867, Test Acc: 0.7368\n",
            "Epoch: 072, Train Acc: 0.9067, Test Acc: 0.8158\n",
            "Epoch: 073, Train Acc: 0.9333, Test Acc: 0.8421\n",
            "Epoch: 074, Train Acc: 0.9067, Test Acc: 0.8158\n",
            "Epoch: 075, Train Acc: 0.9067, Test Acc: 0.8158\n",
            "Epoch: 076, Train Acc: 0.9200, Test Acc: 0.7895\n",
            "Epoch: 077, Train Acc: 0.9200, Test Acc: 0.8158\n",
            "Epoch: 078, Train Acc: 0.9267, Test Acc: 0.7895\n",
            "Epoch: 079, Train Acc: 0.9267, Test Acc: 0.7895\n",
            "Epoch: 080, Train Acc: 0.9133, Test Acc: 0.8158\n",
            "Epoch: 081, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 082, Train Acc: 0.8867, Test Acc: 0.7895\n",
            "Epoch: 083, Train Acc: 0.8933, Test Acc: 0.7632\n",
            "Epoch: 084, Train Acc: 0.9200, Test Acc: 0.7632\n",
            "Epoch: 085, Train Acc: 0.9200, Test Acc: 0.7895\n",
            "Epoch: 086, Train Acc: 0.8867, Test Acc: 0.7895\n",
            "Epoch: 087, Train Acc: 0.9267, Test Acc: 0.7632\n",
            "Epoch: 088, Train Acc: 0.9267, Test Acc: 0.7632\n",
            "Epoch: 089, Train Acc: 0.9133, Test Acc: 0.8158\n",
            "Epoch: 090, Train Acc: 0.9267, Test Acc: 0.7632\n",
            "Epoch: 091, Train Acc: 0.9200, Test Acc: 0.7368\n",
            "Epoch: 092, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 093, Train Acc: 0.9267, Test Acc: 0.8158\n",
            "Epoch: 094, Train Acc: 0.9133, Test Acc: 0.8158\n",
            "Epoch: 095, Train Acc: 0.9267, Test Acc: 0.8158\n",
            "Epoch: 096, Train Acc: 0.9333, Test Acc: 0.8158\n",
            "Epoch: 097, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 098, Train Acc: 0.9467, Test Acc: 0.8158\n",
            "Epoch: 099, Train Acc: 0.9267, Test Acc: 0.8158\n",
            "Epoch: 100, Train Acc: 0.9400, Test Acc: 0.8158\n",
            "Epoch: 101, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 102, Train Acc: 0.9267, Test Acc: 0.8158\n",
            "Epoch: 103, Train Acc: 0.9267, Test Acc: 0.8158\n",
            "Epoch: 104, Train Acc: 0.9267, Test Acc: 0.8158\n",
            "Epoch: 105, Train Acc: 0.9467, Test Acc: 0.8158\n",
            "Epoch: 106, Train Acc: 0.8933, Test Acc: 0.8158\n",
            "Epoch: 107, Train Acc: 0.8533, Test Acc: 0.7368\n",
            "Epoch: 108, Train Acc: 0.8933, Test Acc: 0.7895\n",
            "Epoch: 109, Train Acc: 0.9267, Test Acc: 0.7895\n",
            "Epoch: 110, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 111, Train Acc: 0.9200, Test Acc: 0.7895\n",
            "Epoch: 112, Train Acc: 0.9267, Test Acc: 0.7895\n",
            "Epoch: 113, Train Acc: 0.9333, Test Acc: 0.8158\n",
            "Epoch: 114, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 115, Train Acc: 0.9267, Test Acc: 0.7895\n",
            "Epoch: 116, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 117, Train Acc: 0.9267, Test Acc: 0.8158\n",
            "Epoch: 118, Train Acc: 0.8200, Test Acc: 0.8421\n",
            "Epoch: 119, Train Acc: 0.9267, Test Acc: 0.7368\n",
            "Epoch: 120, Train Acc: 0.9333, Test Acc: 0.7632\n",
            "Epoch: 121, Train Acc: 0.8800, Test Acc: 0.8421\n",
            "Epoch: 122, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 123, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 124, Train Acc: 0.9133, Test Acc: 0.7632\n",
            "Epoch: 125, Train Acc: 0.9400, Test Acc: 0.8158\n",
            "Epoch: 126, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 127, Train Acc: 0.9000, Test Acc: 0.7895\n",
            "Epoch: 128, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 129, Train Acc: 0.9400, Test Acc: 0.7895\n",
            "Epoch: 130, Train Acc: 0.9133, Test Acc: 0.8158\n",
            "Epoch: 131, Train Acc: 0.9400, Test Acc: 0.7895\n",
            "Epoch: 132, Train Acc: 0.9400, Test Acc: 0.7895\n",
            "Epoch: 133, Train Acc: 0.9200, Test Acc: 0.8158\n",
            "Epoch: 134, Train Acc: 0.9400, Test Acc: 0.8158\n",
            "Epoch: 135, Train Acc: 0.9333, Test Acc: 0.8158\n",
            "Epoch: 136, Train Acc: 0.8800, Test Acc: 0.8158\n",
            "Epoch: 137, Train Acc: 0.9400, Test Acc: 0.8158\n",
            "Epoch: 138, Train Acc: 0.9400, Test Acc: 0.7895\n",
            "Epoch: 139, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 140, Train Acc: 0.9400, Test Acc: 0.8158\n",
            "Epoch: 141, Train Acc: 0.9333, Test Acc: 0.8158\n",
            "Epoch: 142, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 143, Train Acc: 0.9400, Test Acc: 0.7895\n",
            "Epoch: 144, Train Acc: 0.9400, Test Acc: 0.7895\n",
            "Epoch: 145, Train Acc: 0.9400, Test Acc: 0.7895\n",
            "Epoch: 146, Train Acc: 0.9400, Test Acc: 0.7895\n",
            "Epoch: 147, Train Acc: 0.9400, Test Acc: 0.8158\n",
            "Epoch: 148, Train Acc: 0.9467, Test Acc: 0.8158\n",
            "Epoch: 149, Train Acc: 0.8533, Test Acc: 0.6579\n",
            "Epoch: 150, Train Acc: 0.8867, Test Acc: 0.7895\n",
            "Epoch: 151, Train Acc: 0.9133, Test Acc: 0.8421\n",
            "Epoch: 152, Train Acc: 0.8600, Test Acc: 0.7632\n",
            "Epoch: 153, Train Acc: 0.9200, Test Acc: 0.7368\n",
            "Epoch: 154, Train Acc: 0.8600, Test Acc: 0.8421\n",
            "Epoch: 155, Train Acc: 0.9133, Test Acc: 0.7895\n",
            "Epoch: 156, Train Acc: 0.9133, Test Acc: 0.7105\n",
            "Epoch: 157, Train Acc: 0.9400, Test Acc: 0.7895\n",
            "Epoch: 158, Train Acc: 0.8867, Test Acc: 0.8158\n",
            "Epoch: 159, Train Acc: 0.9400, Test Acc: 0.7895\n",
            "Epoch: 160, Train Acc: 0.9400, Test Acc: 0.7632\n",
            "Epoch: 161, Train Acc: 0.9333, Test Acc: 0.8158\n",
            "Epoch: 162, Train Acc: 0.9400, Test Acc: 0.8158\n",
            "Epoch: 163, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 164, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 165, Train Acc: 0.9400, Test Acc: 0.7895\n",
            "Epoch: 166, Train Acc: 0.9400, Test Acc: 0.8158\n",
            "Epoch: 167, Train Acc: 0.9400, Test Acc: 0.7895\n",
            "Epoch: 168, Train Acc: 0.9400, Test Acc: 0.8158\n",
            "Epoch: 169, Train Acc: 0.9333, Test Acc: 0.7895\n",
            "Epoch: 170, Train Acc: 0.9067, Test Acc: 0.8158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Things to do :\n",
        "- Plot the training and loss curves \n",
        "- Test on the real data for 100 or 200 graphs \n",
        "- Compare the resulsts for these 3 GNN only experiments \n",
        "- Since there are 9 classes of heart issues that exist- it would be nice if someone could check if the classes are balanced or not, just good to know before modelling\n",
        "\n",
        "\n",
        "Questions to ask:\n",
        "- Are the node features just as blood pressure dynamically recorded at the same time resolution, no right? These are only measured once and stay static for the 2500 time steps?\n",
        "- 80K MTS samples ( What does MTS mean? Multivariate Time Series, seems like it?) I am confused because if there are about 6000 patients (6000 graphs) with 12 leads ( so each graph has 12 nodes) and each patient has 2500 time steps of heart activity recorded ( so at each node there is a 2500 time step of electrical activity?). But where does 80K MTS fit into the picture?  "
      ],
      "metadata": {
        "id": "ShM8vbXvH7Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5pDQMG5aCGsA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}